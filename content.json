{"meta":{"title":"理想国","subtitle":"","description":"","author":"进击的学渣","url":"https://haosutopia.github.io","root":"/"},"pages":[{"title":"关于博客","date":"2018-01-05T13:03:39.000Z","updated":"2019-03-18T21:35:06.000Z","comments":true,"path":"about/index.html","permalink":"https://haosutopia.github.io/about/","excerpt":"","text":"&emsp;&emsp;这是我的个人博客，本人本科同济毕业，目前在TUM学习Elektrotechnik und Informationstechnik， 涉猎领域主要有控制理论、机器人、机器学习等。我会将目前正在学习的知识通过博客做总结，供大家在将来学习中参考，本博客文章均为原创，禁止转载。若有疑问，请留言或联系邮箱： &#97;&#x6c;&#x6c;&#101;&#x6e;&#x77;&#99;&#x68;&#x40;&#x68;&#x6f;&#116;&#109;&#97;&#105;&#108;&#x2e;&#99;&#111;&#x6d; 05.01.2018"},{"title":"Chenghao Wang","date":"2020-09-28T13:53:39.000Z","updated":"2020-09-29T01:58:11.719Z","comments":true,"path":"home/index.html","permalink":"https://haosutopia.github.io/home/","excerpt":"","text":"About MeMy name is Chenghao Wang. I hold my Bachlor of Engineer at Tongji University in China. During that time, I minored in mathematics and studied basic theories of electricity and control. Currently, I’m a master student at Eletrical and Computer Engineering Department, Technical University of Munich(TUM). Throughout my master education, I studied robotics, machine learning, control theory, and completed various projects with different kinds of robots. Research Interest Robotics: Whole-body Control, Motion Planning, Trajectory Planning Machine Learning: Reinforcement Learning, Classification Work Experience Tutor for course “Robocup@Home” TUM, 10/2019-01/2020 Assistance to Robocup@Home project Tutor for course “Multidimentional Signal Processing” (MDSP) TUM, 11/2018-02/2019 Assistance to MATLAB homework Projects Whole-body Hierarchical task execution for mobile service robots (master thesis)This project proposed a torque-based general framework ros_task to realize whole-body hierarchical tasks execution. This framework is based on ROS and suitable for various ROS-based robots. Some basic tasks designed for mobile service robots are applied in this framework. They can achieve some specific behaviors for the robot, such as picking up objects while moving. your browser does not support the video tag Reproducing of handwriting trajectory with robotic armThis project focused on motion planning and control of the robot. Model predictive control (MPC) is used to generate feasible trajectories from the handwriting and an adaptive torque controller based on dynamic model of the robot is designed for control. *TODO**: Add videos Gait planning of quadruped robot based on reinforcement learningThis project applied reinforcement learning to do gait planning for a quadruped robot. SARSA algorithm is used to adjust gait parameters based on the sensor data, so that the robot can climb up different obstacles. *TODO**: Add videos Papers Whole-body Hierarchical task execution for mobile service robots (in progress)"}],"posts":[{"title":"计算机视觉（一）：图像表示与梯度","slug":"Computer-Vision-01","date":"2018-08-08T19:13:45.000Z","updated":"2018-08-08T19:45:58.000Z","comments":true,"path":"2018/08/Computer-Vision-01/","link":"","permalink":"https://haosutopia.github.io/2018/08/Computer-Vision-01/","excerpt":"","text":"&emsp;&emsp;计算机视觉，顾名思义就是使计算机能够像人眼一样“看”东西的科学。一张图片，人可以将它抽象并快速提取其中的有效信息。而对于计算机来说，图片就是一个特殊的矩阵，需要通过一系列算法来得到相关信息。本次学习材料主要是TUM Computer Vision课堂内容以及一些相关论文。 图像表示&emsp;&emsp;首先介绍一下图像的基本知识。图像一般可分为彩色图和灰度图：彩色图每个点的颜色可以通过RGB三种颜色混合而成，因此彩色图通常有RGB三个通道；而灰度图每个点只有一个灰度值。 &emsp;&emsp;彩色图可以将三个通道合并，转化为灰度图，其公式如下：$$I=0.299R+0.587G+0.114B$$ &emsp;&emsp;在现实中，人眼看到的东西都是连续的，但在计算机中我们需要将其离散化，并赋予每个点坐标与数值。其示意图如下：&emsp;&emsp;图像在计算机中的坐标原点为图像左上角，其$x,y$轴与通常坐标系不同，在将来的操作中需要格外注意。这里我们将图像的矩阵表示为$I$，而图中某点的数值表示为$I(x,y)$，其取值范围通常为$[0,1]$或$[0,255]$。在之后的讨论中，我们默认对灰度图进行操作。 梯度计算&emsp;&emsp;梯度在连续函数中可以通过求导计算，但在离散函数中只能通过一些方法来近似。其中最直接的方法就是将离散点与相邻点的差值作为该点梯度：$$\\nabla I(x,y)=\\begin{bmatrix}{ {\\Bbb d} \\over {\\Bbb d}x}I(x,y) \\ { {\\Bbb d} \\over {\\Bbb d}y}I(x,y) \\end{bmatrix} = \\begin{bmatrix}I(x+1,y)-I(x,y) \\ I(x,y+1)-I(x,y)\\end{bmatrix}$$ &emsp;&emsp;这种方法虽然简单，但误差较大。在实际中，我们计算梯度的思路是：先将离散函数通过辅助函数连续化，再对连续化后的函数进行求导得到梯度。&emsp;&emsp;其中连续化是将连续的辅助函数与离散函数卷积，得到一个重构的连续函数。在这里我们以一维数据为例，其中连续函数用$f(x)$表示，离散函数用$f[x]$表示：$$f(x)\\approx\\sum_{k=-\\infty}^{\\infty}f[k]h(x-k)=f[x]\\ast h(x)$$ &emsp;&emsp;对$f(x)$求导便可以得到函数的梯度：$$f’(x)\\approx{ {\\Bbb d} \\over {\\Bbb d}x}(f[x]\\ast h(x))=\\sum_{k=-\\infty}^{\\infty}f[k]h’(x-k)=f[x]\\ast h’(x)$$ &emsp;&emsp;则图像上每一点的梯度为：$$f’[x]=f[x]\\ast h’[x]=\\sum_{k=-\\infty}^{\\infty}f[x-k]h’[k]$$ &emsp;&emsp;实际中我们无法做无穷次的累加，因此采用近似的方式得到函数及其导数：$$\\begin{split}f[x]&amp;=\\sum_{k=-n}^{n}f[x-k]h[k]\\f’[x]&amp;=\\sum_{k=-n}^{n}f[x-k]h’[k]\\end{split}$$ &emsp;&emsp;其中辅助函数$h(x)$我们可以选择高斯函数或Sinc函数。 高斯函数&emsp;&emsp;高斯函数一般表达式如下：$$g(x)=Ce^{-x^2 \\over 2\\sigma^2}$$ &emsp;&emsp;其图像及导数图像如下： &emsp;&emsp;图中红线表示相邻采样点的取值。可以发现，高斯函数会对相邻采样点取值有影响，从而产生一些偏差。 Sinc函数&emsp;&emsp;Sinc函数表达式如下：$$h(x)={\\rm sinc}(x)={\\sin(\\pi x) \\over \\pi x}, {\\rm sinc}(0)=1$$ &emsp;&emsp;其图像及其导数图像如下： &emsp;&emsp;从图中可以发现，Sinc函数在相邻采样点取值均为0，克服了高斯函数的缺陷。但其导数图像衰弱程度比高斯函数慢很多。&emsp;&emsp;因此通常我们在连续化原函数时使用Sinc函数，在计算梯度时使用高斯函数。 Sobel滤波器&emsp;&emsp;将上述一维的数据拓展为二维，使用高斯函数作为辅助函数，我们得到：$$\\begin{split}&amp;I(x,y)\\approx I[x,y]\\ast h(x,y)=\\sum_{k=-\\infty}^{\\infty}\\sum_{l=-\\infty}^{\\infty}I[x.y]g(x-k)g(y-l)\\&amp;{ {\\rm d} \\over {\\rm d}x}I(x,y) \\approx I[x,y]\\ast { {\\rm d} \\over {\\rm d}x}h(x,y)=\\sum_{k=-\\infty}^{\\infty}\\sum_{l=-\\infty}^{\\infty}I[x.y]g’(x-k)g(y-l)\\&amp;{ {\\rm d} \\over {\\rm d}y}I(x,y) \\approx I[x,y]\\ast { {\\rm d} \\over {\\rm d}y}h(x,y)=\\sum_{k=-\\infty}^{\\infty}\\sum_{l=-\\infty}^{\\infty}I[x.y]g(x-k)g’(y-l)\\end{split}$$ &emsp;&emsp;则图像上每一点的梯度为:$$\\begin{split}&amp;{ {\\rm d} \\over {\\rm d}x}I[x,y]=\\sum_{k=-\\infty}^{\\infty}\\sum_{l=-\\infty}^{\\infty}I[x.y]g’[x-k]g[y-l]=\\sum_{k=-\\infty}^{\\infty}\\sum_{l=-\\infty}^{\\infty}I[x-k.y-l]g’[k]g[l]\\&amp;{ {\\rm d} \\over {\\rm d}y}I[x,y]=\\sum_{k=-\\infty}^{\\infty}\\sum_{l=-\\infty}^{\\infty}I[x.y]g[x-k]g’[y-l]=\\sum_{k=-\\infty}^{\\infty}\\sum_{l=-\\infty}^{\\infty}I[x-k.y-l]g[k]g’[l]\\end{split}$$ &emsp;&emsp;近似后得到：$$\\begin{split}{ {\\rm d} \\over {\\rm d}x}I[x,y]=\\sum_{k=-1}^{1}\\sum_{l=-1}^{1}I[x-k.y-l]g’[k]g[l]\\{ {\\rm d} \\over {\\rm d}y}I[x,y]=\\sum_{k=-1}^{1}\\sum_{l=-1}^{1}I[x-k.y-l]g[k]g’[l]\\end{split}$$ &emsp;&emsp;上式可以发现，图像中点的梯度就相当于其相邻像素点取值分别乘以相应权重再相加，因此可以将这些权重表示为一个矩阵。为了使权重相加为1，我们取：$$C=(1+2e^{-{1 \\over 2\\sigma^2 } })^{-1}$$ &emsp;&emsp;当$\\sigma=\\sqrt{1 \\over 2\\log2}$时：$$\\begin{split}&amp;g’[k]g[l]\\Rightarrow {1 \\over 8}\\log2\\begin{bmatrix}+1 &amp; \\ \\ 0\\ &amp; -1\\+2 &amp; \\ \\ 0\\ &amp; -2\\+1 &amp; \\ \\ 0\\ &amp; -1\\end{bmatrix}\\&amp;g[k]g’[l]\\Rightarrow {1 \\over 8}\\log2\\begin{bmatrix}+1 &amp; +2 &amp; +1\\0 &amp; 0 &amp; 0\\-1 &amp; -2 &amp; -1\\end{bmatrix}\\end{split}$$ &emsp;&emsp;为了方便计算，我们将上式整数化，得到横向与纵向Sobel滤波器分别为：$$\\begin{split}横向{\\rm Sobel}滤波器：\\begin{bmatrix}+1 &amp; \\ \\ 0\\ &amp; -1\\+2 &amp; \\ \\ 0\\ &amp; -2\\+1 &amp; \\ \\ 0\\ &amp; -1\\end{bmatrix}\\纵向{\\rm Sobel}滤波器：\\begin{bmatrix}+1 &amp; +2 &amp; +1\\0 &amp; 0 &amp; 0\\-1 &amp; -2 &amp; -1\\end{bmatrix}\\end{split}$$ &emsp;&emsp;在实际应用中我们只需要将图像矩阵与Sobel滤波器卷积就可以得到图像的梯度矩阵了。 小结 图像在计算机中表示为矩阵形式。 图像梯度通过Sobel滤波器估计：横向滤波器计算$x$轴方向导数；纵向滤波器计算$y$轴方向导数。","categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://haosutopia.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"}],"tags":[]},{"title":"决策树","slug":"Decision-Tree-01","date":"2018-05-31T20:56:03.000Z","updated":"2018-06-01T15:43:24.000Z","comments":true,"path":"2018/05/Decision-Tree-01/","link":"","permalink":"https://haosutopia.github.io/2018/05/Decision-Tree-01/","excerpt":"","text":"&emsp;&emsp;机器学习中使用最多的算法莫过于神经网络与树形算法了，而树形算法的基础就是决策树（Decision Tree）。决策树是一种基本的分类与回归方法，相对于神经网络这种黑箱，决策树容易理解，并且运行速度快。但由于其结构较为简单，故预测能力有限，无法与强监督学习模型相提并论，需要进一步的优化。 概述&emsp;&emsp;决策树的思想与我们在日常生活中进行判断或决策的过程类似。下面以判断“一个人对电脑游戏的喜爱程度”为例：&emsp;&emsp;我们知道，一个人是否喜欢电脑游戏或对电脑游戏的喜爱程度可以通过他/她的年龄、性别、职业等特征来粗略估计。而这些特征的“重要性”是不同的，有些特征对我们判断的帮助更大，因此它们也就更加“重要”（在此例中，若我们发现年龄大的人不喜欢电脑游戏，不论男女和职业，那么年龄对于判断该问题更加“重要”）。在判断中我们会先考虑最“重要”的特征（年龄），再考虑第二“重要”的特征（性别），如此下去，最终得到输出（喜爱程度）。这就是一个最简单的决策树。&emsp;&emsp;上面的例子中我们看出，决策树是一个倒置的树形结构。其中条件判断部分称为内部节点（internal node），决策输出部分称为叶节点（leaf）。虽然在实际中，数据集常常拥有很多特征，决策树也会变得非常庞大，但不可否认该算法使用时的简洁性。根据数据集的输出特性，决策树可以分为两类：分类决策树和回归决策树。分类决策树处理离散输出，而回归决策树处理连续输出。&emsp;&emsp;不难发现，决策树最关键的部分就是各个中间节点。它们对特征取值进行，根据判断结果将样本归入其特定子分支中。中间节点根据判断特征属性分为三种情况：&emsp;&emsp;1. 特征是离散值且不要求生成二叉树（即每个节点只能有两个分支）。此时将特征每个离散值作为一个分支。&emsp;&emsp;2. 特征是离散值且要求生成二叉树。此时将离散值划分为两个子集，每个子集作为一个分支。&emsp;&emsp;3. 特征是连续值。此时需要确定一个特征取值作为分裂点（split point），将特征根据取值大于或小于分裂点分为两种情况，每种情况作为一个分支。&emsp;&emsp;此时决策树的构建就取决于两个核心问题：如何选择特征？ 以及 如何选择划分点？ 对此，我们有如下算法来解决这两个问题：ID3、C4.5与CART。 ID3&emsp;&emsp;ID3（Iterative Dichotomiser 3）是一种构建分类决策树的算法。介绍它之前，我们首先要引入信息论中熵（entropy）的概念：熵表示状态的混乱程度，熵越大越混乱，状态的不确定性就越大。对ID3来说，数据集的熵越大，数据集就越“不纯”。若有数据集$D$，将它根据（$m$个）离散输出进行划分，则$D$的熵表示为：$$H(D)=-\\sum_{i=1}^{m}p_i\\log_2(p_i)$$ &emsp;&emsp;其中$p_i$表示分类$i$在整个数据集中的概率。若将$D$根据特征$A$划分为$v$个子数据集，每个子数据集的熵亦可由上式求出，则可以求得在经过特征$A$划分后，数据集$D$的期望熵为：$$H(D|A)=\\sum_{j=1}^v{|D_j| \\over |D|}H(D_j)$$ &emsp;&emsp;于是划分前后数据集$D$熵的下降值就是该划分的信息增益：$$\\Bbb{gain}(D,A)=H(D)-H(D|A)$$ &emsp;&emsp;在分类问题中，我们希望划分后的子数据集越“纯”，也就是数据集$D$的熵最小，因此我们使用贪婪算法（greedy algorithn），选择信息增益最大的划分。每次划分都会使树长高一层，这样逐步下去，我们就构建了一棵决策树。下面以一个简单的例子来说明划分的选择过程： 日志数量 好友数量 是否使用真实头像 帐号是否真实 少 少 否 否 少 多 是 是 多 中 是 是 中 中 是 是 多 中 是 是 中 多 否 是 中 少 否 否 多 中 否 是 中 少 否 是 少 少 是 否 &emsp;&emsp;上表中的数据一共有三个特征：日志数量（L），好友数量（F）以及是否使用真实头像（H）。我们需要通过这三个特征来判断帐号是否真实。首先我们计算数据集的熵。数据集共有10个样本，其中7个是真的，3个是假的，因此数据集的熵为： $$ H(D)=-(0.7\\log_20.7+0.3\\log_20.3)\\approx 0.812$$ &emsp;&emsp;若根据日志数量进行划分，将样本划分为三个子数据集（多：$D_1$、中：$D_2$、少：$D_3$），则三个子数据集的熵分别为：$$\\begin{eqnarray}&amp;H(D_1)&amp;=-({0 \\over 3}\\log_2{0 \\over 3}+{3 \\over 3}\\log_2{3 \\over 3})=0\\&amp;H(D_2)&amp;=-({1 \\over 4}\\log_2{1 \\over 4}+{3 \\over 4}\\log_2{3 \\over 4})\\approx0.811\\&amp;H(D_3)&amp;=-({1 \\over 3}\\log_2{1 \\over 3}+{2 \\over 3}\\log_2{2 \\over 3})\\approx0.918\\end{eqnarray}$$ &emsp;&emsp;划分后的期望熵为：$$H(D|L)=0.3H(D_1)+0.4H(D_2)+0.3H(D_3)\\approx0.603$$ &emsp;&emsp;根据日志数量划分的信息增益为：$$\\Bbb{gain}(D,L)=H(D)-H(D|L)=0.209$$ &emsp;&emsp;同理可以得到$F$与$H$的信息增益分别为0.486与-0.034，因此我们选择好友数量作为第一次划分，以此类推就可以得到一棵决策树了。 C4.5&emsp;&emsp;ID3有一个缺陷，就是它只考虑了子数据集的纯度，但忽略了特征本身的混乱程度。举个极端的情况，若前例数据集中每一个样本都有一个不同的账户ID，那么若以此为特征进行划分，就会将样本分为10个子数据集，每个子数据集只有一个样本，纯度最大。因此ID3会选择账户ID作为划分特征，但这是不可取的。&emsp;&emsp;C4.5提供了一种解决方案，它考虑了特征的熵，并以此将信息增益标准化。特征的熵表示为：$$H_A(D)=-\\sum_{j=1}^v{|D_j| \\over |D|}\\log_2({|D_j| \\over |D|})$$ &emsp;&emsp;标准化后的信息增益为：$$\\Bbb{gain}_R(D,A)={\\Bbb{gain}(D,A) \\over H_A(D)}$$ &emsp;&emsp;之后就与ID3类似，选择信息增益标准化后最大的划分就可以了。接着之前的例子，我们可以将所得的信息增益标准化。特征$L$的熵为：$$H_L(D)=-(0.3\\log_20.3+0.4\\log_20.4+0.3\\log_20.3)\\approx1.571$$ &emsp;&emsp;因此标准化后的信息增益为：$$\\Bbb{gain}_R(D,L)={\\Bbb{gain}(D,L) \\over H_L(D)}={0.209 \\over 1.571}\\approx0.133$$ &emsp;&emsp;同理可以得到$F$与$H$标准化后的信息增益分别为0.319与-0.034，因此我们选择好友数量作为第一次划分，以此类推就可以得到一棵决策树了。 CART&emsp;&emsp;CART（Classification And Regression Tree）是一种二叉决策树，可以对数据分类或者回归。与ID3、C4.5类似，CART也是通过选取最优划分来构建决策树的，但它使用的不是信息增益，而是划分的代价函数。&emsp;&emsp;对于回归问题，代价函数表示为两个子数据集真实值与预测值的均方差之和：$${\\rm Regression:} J=\\sum_i(y^{(i)}-prediction_1)^2+\\sum_j(y^{(j)}-prediction_2)^2$$ &emsp;&emsp;其中预测值为每个子数据集输出的均值。若中间节点将数据集$D$划分为两个子数据集$D_1$、$D_2$，则$prediction_1$为$D_1$的均值，$prediction_2$为$D_2$的均值。&emsp;&emsp;对于分类问题，我们使用Gini系数来表示一个数据集的混乱程度：$$G =\\sum_i(p_i(1-p_i))=\\sum_ip_i-\\sum_ip_i^2=1-\\sum_ip_i^2$$ &emsp;&emsp;其中$p_i$为数据集中相同类别数据的比例。若数据集中只存在一个分类，是最好的情况，此时$G=0$。最糟糕的情况就是数据集中所有类别都有相同的比例，此时$G=1-p_i$。&emsp;&emsp;分类问题的代价函数表示为两个子数据集Gini系数的期望：$${\\rm Classification:} J={|D_1| \\over |D|}G_1+{|D_2| \\over |D|}G_2$$ &emsp;&emsp;无论回归还是分类问题，当构建决策树时，算法会考虑每个特征，计算它们的代价函数，并选取最小代价的特征及划分点，以此下去即可。 优化&emsp;&emsp;若数据的特征很多，决策树的构建过程就会一直进行下去，这样会产生过拟合问题，因此在实际操作中我们需要指定停止条件，并可以适当对枝叶进行修剪，除此之外还可以使用集成树模型来进一步提升预测效果。 设置停止条件&emsp;&emsp;通常设置的停止条件有如下几种：&emsp;&emsp;1. 叶节点中只有一种类型时停止：这是最直观的方式，仅限于分类问题，并容易导致过拟合。&emsp;&emsp;2. 设置叶节点的最小样本数量：若训练过程中，数据集到达叶节点的样本数量过小，那么我们就舍去这个叶节点。&emsp;&emsp;3. 设置树的最大深度：即限制样本到达叶节点时所通过的中间节点数量。 修剪&emsp;&emsp;对决策树进行修剪可以进一步提高其分类表现。修剪过程中，在不影响准确度的情况下，我们可以将不重要的枝叶移除。这样可以降低决策树的复杂度，并减少过拟合问题。 集成树模型&emsp;&emsp;单棵决策树稳定性通常较低且方差较高，难以达到很好的效果。使用集成学习可以降低降低决策树的方差，从而提高整体的预测能力。常见的集成树模型有随机森林、GBM（Gradient boosting machine）与xgboost。&emsp;&emsp;随机森林（Random Forest）采用多个决策树的投票机制来改善决策树的预测能力。训练过程中，我们每次都随机取部分训练样本来训练决策树，一直重复，我们就能训练出许多不同的决策树，它们形成了一个森林。之后使用这个森林对未知数据进行预测。每个决策树可能会预测出不同的结果，将这些结果汇总并选取投票最多的分类作为预测值。实践证明，随机森林的运行效率和准确率较高，实现起来也较为简单，相较普通决策树更为常用。&emsp;&emsp;GBM与xgboost我还没有研究，在此先省略。 小结 决策树通过选取特征及划分点来对数据进行划分。 构建决策树的算法有：ID3、C4.5与CART，其中ID3与C4.5仅用于分类问题，CART能够用于分类或回归问题。 决策树容易过拟合，需要进行优化。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://haosutopia.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[]},{"title":"K-Means聚类算法","slug":"K-Means-01","date":"2018-04-15T13:27:06.000Z","updated":"2018-05-30T20:50:08.000Z","comments":true,"path":"2018/04/K-Means-01/","link":"","permalink":"https://haosutopia.github.io/2018/04/K-Means-01/","excerpt":"","text":"&emsp;&emsp;K-Means聚类算法（k-means clustering algorithm）是一种无监督学习算法。它可以通过多次迭代，将一系列无标记的数据根据它们的特征分布划分为$k$个子集，使得子集内部元素之间的相异度尽可能低，而不同子集元素相异度尽可能高。我们称这样的子集为簇（cluster），而将数据划分为簇的过程称为聚类。 算法思想&emsp;&emsp;假设我们有一组无标记的训练样本${x^{(1)},…,x^{(m)}},\\ x\\in{\\Bbb R^n}$，使用K-Means聚类算法将它们划分过程如下：&emsp;&emsp;&emsp;1. 随机初始化簇中心（cluster centroid）$\\mu_1,…,\\mu_k\\in{\\Bbb R}$&emsp;&emsp;&emsp;2. 循环直到收敛：${$&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;${\\rm For}\\ i=1,…,m{$$$c^{(i)}:=\\arg\\min_j||x^{(i)}-\\mu_j||^2$$ &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$}$&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;${\\rm For}\\ j=1,…,k{$$$\\mu_j:={\\sum_{i=1}^m1{c^{(i)}=j}x^{(i)} \\over \\sum_{i=1}^m1{c^{(i)}=j}}$$ &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$}$&emsp;&emsp;&emsp;&emsp;$}$&emsp;&emsp;其中$k$是我们想要划分的簇数，$c^{(i)}$表示当前迭代中第$i$个样本所在的簇（$c^{(i)}=1$就代表$x^{(i)}$属于第$1$个簇），$\\mu_j$表示当前迭代中第$j$个簇的中心。&emsp;&emsp;整个算法思想大致是：首先为每一个簇初始化簇中心，我们有$k$个簇因此有$k$个簇中心。再使用训练集对簇中心进行迭代。迭代过程中，先将每一个样本分配给与其距离最小的簇中心所对应的簇，这样就相当于对样本进行了一次划分，构建了$k$个簇。再将每个簇中样本的算数平均值作为新的簇中心。不断进行这样的迭代直到簇中心不再变化，此时对样本的划分也不再改变。此时我们就得到了样本的$k$个簇。下面的举例能够很好地说明整个算法的过程：&emsp;&emsp;图中(a)表示原始数据集；(b)表示随机初始化簇中心；(c)表示将样本分配给簇；(d)表示根据(c)中样本分配得到新的簇中心；(e)(f)表示用相同方法迭代直至收敛。&emsp;&emsp;在实际中，我们通常不会像上图举例那样对簇中心进行初始化，而是随机选择$k$个样本作为初始的簇中心（此时需保证$k&lt;m$），这样能使算法更快地收敛。 失真函数&emsp;&emsp;为了更好地描述簇划分的好坏，我们引入失真函数（distortion function）$J$，它表示簇中样本与簇中心的相异度：$$J(c,\\mu)=\\sum_{i=1}^m||x^{(i)}-\\mu_{c^{(i)} }||^2$$ &emsp;&emsp;其中$\\mu_{c^{(i)} }$表示第$i$个样本所在簇的簇中心，因此$J$计算的是样本与其所在簇的簇中心距离的平方和。$J$越小表示相异度越小，对样本的划分越好。现在再回头看，我们会发现，算法中迭代的每一步都是在保持一部分参数不变的情况下，使用另一部分参数来最小化失真函数。因此在K-Means聚类算法运行过程中，失真函数只会减小不会增加，这也可以被用来检验算法程序是否由BUG。&emsp;&emsp;由于失真函数是一个非凸函数，因此不能保证每次算法都给出一个全局最小值，可能存在下图所示的局部最小值情况：&emsp;&emsp;因此在实际中我们常常使用不同的随机初始值运行若干次算法，并找出失真函数值最小的那个作为样本的划分。 $k$的选择&emsp;&emsp;我们之前的讨论都是在给定$k$的情况下进行的。但面对一组数据集时，通常很难判断$k$到底取多少合适。下面的方法可以一定程度上帮助我们判断$k$的取值。 Elbow方法&emsp;&emsp;Elbow方法（Elbow method）是由Robert Tibshirani、Guenther Walther与Trevor Hastie于2000年提出的。它的思想很简单：从1开始尝试使用不同的$k$值对数据集进行K-Means聚类，运行后每个$k$值对应一个失真函数。以$k$值为横坐标，失真函数值$J$为纵坐标作折线图，得到一个单调递减的曲线。若曲线中存在一个明显的拐点（如下图所示，此时$k=3$），则取其中拐点对应的$k$值作为我们的选择。&emsp;&emsp;由于此时图中曲线像人的手臂，而拐点就相当于肘部，因此该方法被称为Elbow方法。但很多时候曲线并没有明显的拐点，该方法就无法奏效。实际中我们只会将它作为一种尝试，并不会对它报太大的期望。 通过实际目的选择&emsp;&emsp;通常我们对数据分类是有一定目的的，因此可以根据实际目的对$k$进行选择。换句话说，就是我觉得分几类好就分几类。&emsp;&emsp;比如T恤尺寸问题：我们需要设计$k$种T恤尺寸来供应市场。现在有一批不同人身高体重的数据，我们将它们分为$k$类，并将每一类的簇中心取值作为该类T恤的标准尺寸。此时$k$的取值可以为3（S、M、L），也可以为5（XS、S、M、L、XL），这完全取决于市场以及生产成本，是通过实际目的进行选择的。&emsp;&emsp;大多数情况下我们都会采用这种方法来选择$k$。因为根据丑小鸭定理（The Ugly Duckling Theorem），所有无标记的数据都是同等独特的，只有根据实际目的对它们的分类才有意义。 小结 K-Means聚类算法可以自动将数据分为$k$类。 失真函数是簇划分好坏的判断标准。 $k$值可以通过Elbow方法以及根据实际目的选择，大多数情况我们选择后者。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://haosutopia.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"无监督学习","slug":"无监督学习","permalink":"https://haosutopia.github.io/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"}]},{"title":"分布参数系统控制（七）：边界输入","slug":"RSVP-07","date":"2018-04-12T22:39:55.000Z","updated":"2018-04-12T22:39:54.000Z","comments":true,"path":"2018/04/RSVP-07/","link":"","permalink":"https://haosutopia.github.io/2018/04/RSVP-07/","excerpt":"","text":"&emsp;&emsp;分布参数系统的输入可以分为点输入、分布式输入和边界输入。之前的章节我们讨论的都是分布式输入，此时输入分布在定义域的某个区间。而若输入$u(t)$定义在边界，则称为边界输入（Randeingriff），此时系统方程会有所不同，相应的计算方法也会有差别。因此本文讨论边界输入以及它在反馈控制中的应用。 边界输入系统方程&emsp;&emsp;分布式输入中，输入$u(t)$在系统状态方程中；而边界输入中，$u(t)$在系统边界条件中。一个典型的边界输入系统如下：$$\\begin{eqnarray}\\partial_tx(z,t)&amp;=&amp;{\\cal A}x(z,t),\\ t&gt;0,\\ z\\in(0,1)\\tag{1}\\x(0,t)&amp;=&amp;0,\\ t&gt;0\\tag{2}\\x(1,t)&amp;=&amp;u(t),\\ t&gt;0\\tag{3}\\x(z,0)&amp;=&amp;x_0(z),\\ z\\in[0,1]\\tag{4}\\end{eqnarray}$$ &emsp;&emsp;其中式(3)边界条件表示输入为$z=1$处的边界输入，同时式(4)所示的初始条件需要满足式(2)(3)的边界条件，即：$$\\begin{eqnarray}x_0(0)&amp;=&amp;0\\x_0(1)&amp;=&amp;u(0)\\end{eqnarray}$$ 分布式输入转换&emsp;&emsp;在不改变系统的前提下，我们可以使用特殊的方法将边界输入系统转换为分布式输入。以式(1)~(4)所示系统为例，设${\\cal A}h={\\rm d}_z^2h$。首先将系统状态变量分解：$$x(z,t)=x_H(z,t)+x_I(z,t)\\tag{5}$$ &emsp;&emsp;其中$x_H(z,t)$满足齐次边界条件：$$\\begin{eqnarray}x_H(0,t)&amp;=&amp;0\\tag{6}\\x_H(1,t)&amp;=&amp;0\\tag{7}\\end{eqnarray}$$ &emsp;&emsp;$x_I(z,t)$满足非齐次边界条件：$$\\begin{eqnarray}x_I(0,t)&amp;=&amp;0\\tag{8}\\x_I(1,t)&amp;=&amp;u(t)\\tag{9}\\end{eqnarray}$$ &emsp;&emsp;简单来说齐次边界条件就是输入$u(t)$不存在时的边界条件；非齐次边界条件就是$u(t)$存在时的边界条件。这有点像零输入响应与零状态响应，不过还是有区别的。&emsp;&emsp;非齐次部分$x_I(z,t)$可以表示为一个多项式$f(z)$与$u(t)$的乘积:$$x_I(z,t)=f(z)u(t)\\tag{10}$$ &emsp;&emsp;其中$f(z)$的阶数可以取到足够高为止。我们先假设$f(z)$为一阶：$$f(z)=az+b\\tag{11}$$ &emsp;&emsp;通过式(8)(9)可以求解出$f(z)$：$$\\begin{eqnarray}x_I(0,t)&amp;=&amp;f(0)u(t)=bu(t)=0\\x_I(1,t)&amp;=&amp;f(1)u(t)=(a+b)u(t)=u(t)\\end{eqnarray}$$ &emsp;&emsp;我们得到$a=1,b=0$（若无法求解，则增加$f(z)$的阶数）。此时：$$x_I(z,t)=zu(t)\\tag{12}$$ &emsp;&emsp;将式(12)带入式(1)，就可以得到一个$x_H(z,t)$的方程：$$\\begin{eqnarray}&amp;&amp;\\partial_t(x_H(z,t)+x_I(z,t))=\\partial_z^2(x_H(z,t)+x_I(z,t))\\\\Rightarrow&amp;&amp;\\partial_t(x_H(z,t)+zu(t))=\\partial_z^2(x_H(z,t)+zu(t))\\\\Rightarrow&amp;&amp;\\partial_tx_H(z,t)=\\partial_z^2x_H(z,t)-z\\partial_tu(t)\\end{eqnarray}$$ &emsp;&emsp;将式(6)(7)的边界条件以及相应初始条件加入后，我们便将原系统转换为了$x_H(z,t)$为状态变量的分布式输入系统：$$\\begin{eqnarray}\\partial_tx_H(z,t)&amp;=&amp;{\\cal A}x_H(z,t)-z\\partial_tu(t),\\ t&gt;0,\\ z\\in(0,1)\\x_H(0,t)&amp;=&amp;0,\\ t&gt;0\\x_H(1,t)&amp;=&amp;0,\\ t&gt;0\\x_H(z,0)&amp;=&amp;x_0(z)-zu(0),\\ z\\in[0,1]\\end{eqnarray}$$ 反馈控制（Backstepping）&emsp;&emsp;若不转换为分布式输入，我们也可使用Backstepping来设计边界输入的反馈控制。这种控制方法来源于非线性控制（详见Backstepping（反步控制）），但个人觉得两者差别还是挺大的。 基本思路&emsp;&emsp;Backstepping基本思路如下：&emsp;&emsp;1. 识别系统中不稳定的状态。&emsp;&emsp;2. 选择所要达到的稳定目标系统。&emsp;&emsp;3. 计算Backstepping变换以达到目标。&emsp;&emsp;4. 通过Backstepping得到Backstepping控制器。&emsp;&emsp;下面我们将通过一个例子来分步演示Backstepping的设计过程。 识别系统不稳定状态&emsp;&emsp;我们仍然以式(1)~(4)所示系统为例，并设系统算子为${\\cal A}h=\\partial_z^2h+\\gamma h,$ $\\gamma&gt;\\pi^2$。系统表示如下：$$\\begin{eqnarray}\\partial_tx(z,t)&amp;=&amp;\\partial_z^2x(z,t)+\\gamma x(z,t),\\ t&gt;0,\\ z\\in(0,1),\\tag{13}\\x(0,t)&amp;=&amp;0,\\ t&gt;0\\tag{14}\\x(1,t)&amp;=&amp;u(t),\\ t&gt;0\\tag{15}\\x(z,0)&amp;=&amp;x_0(z),\\ z\\in[0,1]\\tag{16}\\end{eqnarray}$$ &emsp;&emsp;计算系统特征值，我们得到$\\lambda_i=\\gamma-(i\\pi)^2$。系统存在部分特征值实部大于零，不稳定。又系统只有一个状态变量$x(z,t)$，因此我们的目标就是将$x(z,t)$通过反馈转换为稳定状态。 选择目标系统&emsp;&emsp;根据第三章节式(13)：$$x(t)=\\sum_{i=1}^{\\infty}e^{\\lambda_it}\\phi_ix_i^\\ast (0)$$ &emsp;&emsp;可以发现，若我们将$x(t)$乘以一个指数函数$e^{-\\mu t}$，只要$\\mu$足够大，则结果一定收敛。因此我们设目标状态变量为：$$\\tilde{x}(z,t)=e^{-\\mu t}x(z,t)$$ &emsp;&emsp;对于目标变量来说：$$\\begin{eqnarray}&amp;&amp;\\partial_t\\tilde{x}(z,t)&amp;=&amp;-\\mu e^{-\\mu t}x(z,t)+e^{-\\mu t}\\partial_tx(z,t)\\&amp;&amp; &amp;=&amp;e^{-\\mu t}(\\partial_z^2x(z,t)-(\\mu-\\gamma)x(z,t))\\&amp;&amp; &amp;=&amp;\\partial_z^2\\tilde{x}(z,t)-(\\mu-\\gamma)\\tilde{x}(z,t)\\&amp;&amp;\\tilde{x}(0,t)&amp;=&amp;e^{-\\mu t}x(0,t)=0\\&amp;&amp;\\tilde{x}(1,t)&amp;=&amp;e^{-\\mu t}x(1,t)=0\\end{eqnarray}$$ &emsp;&emsp;最后一项成立，是系统齐次的要求，即目标系统是原系统通过输入$u(t)$得到的，故目标系统中不能再存在$u(t)$。设$\\mu_R=\\mu-\\gamma$，就得到了目标系统：$$\\begin{eqnarray}&amp;&amp;\\partial_t\\tilde{x}(z,t)&amp;=&amp;\\partial_z^2\\tilde{x}(z,t)-\\mu_R\\tilde{x}(z,t),\\ t&gt;0,\\ z\\in(0,1)\\tag{17}\\&amp;&amp;\\tilde{x}(0,t)&amp;=&amp;0,\\ t&gt;0\\tag{18}\\&amp;&amp;\\tilde{x}(1,t)&amp;=&amp;0,\\ t&gt;0\\tag{19}\\end{eqnarray}$$ &emsp;&emsp;当$\\mu_R&gt;-\\pi^2$时，目标系统指数稳定。 Backstepping变换&emsp;&emsp;Backstepping变换表达式如下：$$\\tilde{x}(z,t)=x(z,t)-\\int_0^z\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta\\tag{20}$$ &emsp;&emsp;其中积分核$\\kappa(z,\\zeta)$是变换的关键。&emsp;&emsp;将$z=1$带入得到：$$\\tilde{x}(1,t)=x(1,t)-\\int_0^1\\kappa(1,\\zeta)x(\\zeta,t){\\rm d}\\zeta\\tag{21}$$ &emsp;&emsp;结合式(15)(19)(21)，我们得到输入$u(t)$的表达式：$$u(t)=\\int_0^1\\kappa(1,\\zeta)x(\\zeta,t){\\rm d}\\zeta\\tag{22}$$ &emsp;&emsp;因此我们现在要做的就是将式(20)的Backstepping变换带入目标系统方程，求出积分核$\\kappa(z,\\zeta)$，再带入式(22)就得到了输入函数$u(t)$。通过求得的$u(t)$，在理论上就可以将原系统控制为稳定的目标系统。下面我们开始计算积分核，该过程较为繁琐，故建议在安静的时候耐心推导一遍（用Latex打出来太不容易了！）。 建立核方程&emsp;&emsp;将式(20)带入式(17)，方程左侧为：$$\\begin{split}\\partial_t\\tilde{x}(z,t)=&amp;\\partial_tx(z,t)-\\int_0^z\\kappa(z,\\zeta)\\partial_tx(\\zeta,t){\\rm d}\\zeta\\=&amp;\\partial_z^2x(z,t)+\\gamma x(z,t)-\\int_0^z\\kappa(z,\\zeta)\\partial_{\\zeta}^2x(\\zeta,t){\\rm d}\\zeta-\\int_0^z\\gamma\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta\\end{split}$$ &emsp;&emsp;方程右侧为：$$\\begin{split}\\partial_z^2\\tilde{x}(z,t)&amp;-\\mu_R\\tilde{x}(z,t)=\\&amp;\\partial_z^2x(z,t)-\\partial_z^2\\int_0^z\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta-\\mu_Rx(z,t)+\\int_0^z\\mu_R\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta\\end{split}$$ &emsp;&emsp;之后将方程左侧与右侧联立得到：$$\\begin{split}&amp;\\partial_z^2x(z,t)+\\gamma x(z,t)-\\int_0^z\\kappa(z,\\zeta)\\partial_{\\zeta}^2x(\\zeta,t){\\rm d}\\zeta-\\int_0^z\\gamma\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta\\=&amp;\\partial_z^2x(z,t)-\\partial_z^2\\int_0^z\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta-\\mu_Rx(z,t)+\\int_0^z\\mu_R\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta\\end{split}$$ &emsp;&emsp;将所有项移动到一边：$$\\begin{split}0=&amp;-\\partial_z^2x(z,t)+\\partial_z^2\\int_0^z\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta+\\mu_Rx(z,t)-\\int_0^z\\mu_R\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta\\&amp;+\\partial_z^2x(z,t)+\\gamma x(z,t)-\\int_0^z\\kappa(z,\\zeta)\\partial_{\\zeta}^2x(\\zeta,t){\\rm d}\\zeta-\\int_0^z\\gamma\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta\\end{split}$$ &emsp;&emsp;到了这一步我们需要处理其中两个个麻烦的项。它们分别为：$$\\begin{eqnarray}&amp;&amp;-\\int_0^z\\kappa(z,\\zeta)\\partial_{\\zeta}^2x(\\zeta,t){\\rm d}\\zeta\\tag{23}\\&amp;&amp;\\partial_z^2\\int_0^z\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta\\tag{24}\\end{eqnarray}$$ &emsp;&emsp;我们可以通过分部积分法将式(23)展开得到：$$\\begin{split}&amp;-&amp;\\int_0^z\\kappa(z,\\zeta)\\partial_{\\zeta}^2x(\\zeta,t){\\rm d}\\zeta=-[\\kappa(z,\\zeta)\\partial_{\\zeta}x(\\zeta,t)]_ 0^z+\\int_0^z\\partial_{\\zeta}\\kappa(z,\\zeta)\\partial_{\\zeta}x(\\zeta,t){\\rm d}\\zeta\\&amp;=&amp;-[\\kappa(z,\\zeta)\\partial_{\\zeta}x(\\zeta,t)]_ 0^z+[\\partial_{\\zeta}\\kappa(z,\\zeta)x(\\zeta,t)]_ 0^z-\\int_0^z\\partial_{\\zeta}^2\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta\\&amp;=&amp;-\\kappa(z,z)\\partial_zx(z,t)+\\kappa(z,0)\\partial_zx(0,t)+\\partial_{\\zeta}\\kappa(z,z)x(z,t)\\&amp;&amp;-\\partial_{\\zeta}\\kappa(z,0)x(0,t)-\\int_0^z{\\rm d}_{\\zeta}^2\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta\\end{split}$$ &emsp;&emsp;对于式(24)我们直接使用求导计算即可：$$\\partial_z\\int_0^z\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta=\\kappa(z,z)x(z,t)+\\int_0^z\\partial_z\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta$$ &emsp;&emsp;我们再求一次导，得到式(24)的表达式：$$\\begin{split}&amp;\\partial_z^2\\int_0^z\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta=\\partial_z\\left(\\kappa(z,z)x(z,t)+\\int_0^z\\partial_z\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta\\right)\\&amp;={\\rm d}_z\\kappa(z,z)x(z,t)+\\kappa(z,z)\\partial_zx(z,t)+\\partial_z\\kappa(z,z)x(z,t)+\\int_0^z\\partial_z^2\\kappa(z,\\zeta)x(\\zeta,t){\\rm d}\\zeta\\end{split}$$ &emsp;&emsp;在这里需要注意$\\kappa(z,z)$的本质是$\\kappa(z,\\zeta)$，只是第二个参数$\\zeta$为$z$的函数$\\zeta(z)=z$。因此$\\kappa(z,z)$的导数为：$$\\begin{split}{\\rm d}z\\kappa(z,z)={\\rm d}_z\\kappa(z,\\zeta(z))&amp;=\\partial_z\\kappa(z,z){ {\\rm d}z \\over {\\rm d}z}+\\partial{\\zeta}\\kappa(z,z){ {\\rm d}\\zeta \\over {\\rm d}z}\\&amp;=\\partial_z\\kappa(z,z)+\\partial_{\\zeta}\\kappa(z,z)\\end{split}$$ &emsp;&emsp;将麻烦项的展开带入原方程，合并同类项得到：$$\\begin{split}0&amp;=({\\rm d}z\\kappa(z,z)+\\partial_z\\kappa(z,z)+\\partial_{\\zeta}\\kappa(z,z)+\\mu_R+\\gamma)x(z,t)\\&amp;+(\\kappa(z,z)-\\kappa(z,z))\\partial_zx(z,t)\\&amp;+\\int_0^z(\\partial_z^2\\kappa(z,\\zeta)-\\partial{\\zeta}\\kappa(z,\\zeta)-(\\mu_R+\\gamma)\\kappa(z,\\zeta))x(\\zeta,t){\\rm d}\\zeta\\&amp;+\\kappa(z,0)\\partial_zx(0,t)+\\partial_{\\zeta}\\kappa(z,0)x(0,t)\\end{split}$$ &emsp;&emsp;由于$x(z,t)$是状态变量，因此若需要上式始终为$0$，则需要将$x(z,t)$及其导数前的系数方程都设置为$0$，因此得到：$$\\begin{eqnarray}2{\\rm d}_z\\kappa(z,z)+\\mu&amp;=&amp;0\\tag{25}\\\\partial_z^2\\kappa(z,\\zeta)-\\partial_{\\zeta}^2\\kappa(z,\\zeta)-\\mu\\kappa(z,\\zeta)&amp;=&amp;0\\tag{26}\\\\kappa(z,0)&amp;=&amp;0\\tag{27}\\end{eqnarray}$$ &emsp;&emsp;考虑完式(17)的系统状态方程，我们再来看式(18)(19)的边界条件。将式(20)带入式(18)：$$\\tilde{x}(0,t)=x(0,t)-\\int_0^0\\kappa(0,\\zeta)x(\\zeta,t){\\rm d}\\zeta=0$$ &emsp;&emsp;式(18)恒成立，而式(19)在之前计算$u(t)$时已经考虑过了。因此综上所述我们得到了所有积分核需要满足的条件，即式(25)~(27)，使用它们建立核方程（Kerngleichung）：$$\\begin{eqnarray}\\partial_z^2\\kappa(z,\\zeta)-\\partial_{\\zeta}^2\\kappa(z,\\zeta)&amp;=&amp;\\mu\\kappa(z,\\zeta),\\ 0&lt;\\zeta&lt;z&lt;1\\tag{28}\\\\kappa(z,0)&amp;=&amp;0\\tag{29}\\\\kappa(z,z)&amp;=&amp;-{\\mu \\over 2}z\\tag{30}\\end{eqnarray}$$ &emsp;&emsp;接下来我们只要求解核方程就能得到积分核的表达式了。 求解核方程&emsp;&emsp;可以发现式(28)所示的核方程是一个波动方程，我们通过坐标转换将它写成标准形式。引入坐标$\\xi$与$\\eta$：$$\\begin{cases}\\xi=z+\\zeta\\\\eta=z-\\zeta\\end{cases}$$ &emsp;&emsp;由于原坐标存在约束条件$0&lt;\\zeta&lt;z&lt;1$，因此它的作用范围（Ortsbereich）在如下三条直线所围成的区域内：&emsp;&emsp;1. $\\zeta=0$&emsp;&emsp;2. $\\zeta=z$&emsp;&emsp;3. $z=1$&emsp;&emsp;通过坐标变换我们可以将这三条曲线转换为$\\xi,\\eta$坐标形式：&emsp;&emsp;1. $\\begin{bmatrix}\\xi\\\\eta\\end{bmatrix}{\\zeta=0}=\\begin{bmatrix}1 &amp;&amp; 1 \\ 1 &amp;&amp; -1 \\end{bmatrix}\\begin{bmatrix}z\\0\\end{bmatrix}=\\begin{bmatrix} 1 \\ 1 \\end{bmatrix}z\\Rightarrow \\xi=\\eta$&emsp;&emsp;2. $\\begin{bmatrix}\\xi\\\\eta\\end{bmatrix}{\\zeta=z}=\\begin{bmatrix}1 &amp;&amp; 1 \\ 1 &amp;&amp; -1 \\end{bmatrix}\\begin{bmatrix}z\\z\\end{bmatrix}=\\begin{bmatrix} 2 \\ 0 \\end{bmatrix}z\\Rightarrow \\eta=0$&emsp;&emsp;3. $\\begin{bmatrix}\\xi\\\\eta\\end{bmatrix}_{z=1}=\\begin{bmatrix}1 &amp;&amp; 1 \\ 1 &amp;&amp; -1 \\end{bmatrix}\\begin{bmatrix}1\\\\zeta\\end{bmatrix}=\\begin{bmatrix} 1+\\zeta \\ 1-\\zeta \\end{bmatrix}z\\Rightarrow \\xi+\\eta=2$ &emsp;&emsp;因此核方程作用范围${\\cal T}_R$变化为$\\tilde{\\cal T}_R$，如下图所示：&emsp;&emsp;设核方程$\\kappa(z,\\zeta)$通过坐标变换转换为$G(\\xi,\\eta)$，即：$$\\kappa(z,\\zeta)=G(\\xi,\\eta)=G(\\xi(z,\\zeta),\\eta(z,\\zeta))\\tag{31}$$ &emsp;&emsp;带入式(28)~(30)得到$G(\\xi,\\eta)$所满足的方程。其中：$$\\begin{eqnarray}\\partial_z\\kappa(z,\\zeta)&amp;=&amp;\\partial_{\\xi}G(\\xi,\\eta){\\rm d}z\\xi+\\partial{\\eta}G(\\xi,\\eta){\\rm d}z\\eta\\&amp;=&amp;\\partial{\\xi}G(\\xi,\\eta)+\\partial_{\\eta}G(\\xi,\\eta)\\\\partial_z^2\\kappa(z,\\zeta)&amp;=&amp;\\partial_{\\xi}^2G(\\xi,\\eta){\\rm d}z\\xi+\\partial{\\eta}\\partial_{\\xi}G(\\xi,\\eta){\\rm d}z\\eta+\\partial{\\xi}\\partial_{\\eta}G(\\xi,\\eta){\\rm d}z\\xi+\\partial{\\eta}^2G(\\xi,\\eta){\\rm d}z\\eta\\&amp;=&amp;\\partial{\\xi}^2G(\\xi,\\eta)+2\\partial_{\\eta}\\partial_{\\xi}G(\\xi,\\eta)+\\partial_{\\eta}^2G(\\xi,\\eta)\\\\partial_{\\zeta}\\kappa(z,\\zeta)&amp;=&amp;\\partial_{\\xi}G(\\xi,\\eta){\\rm d}{\\zeta}\\xi+\\partial{\\eta}G(\\xi,\\eta){\\rm d}{\\zeta}\\eta\\&amp;=&amp;\\partial{\\xi}G(\\xi,\\eta)-\\partial_{\\eta}G(\\xi,\\eta)\\\\partial_{\\zeta}^2\\kappa(z,\\zeta)&amp;=&amp;\\partial_{\\xi}^2G(\\xi,\\eta){\\rm d}{\\zeta}\\xi+\\partial{\\eta}\\partial_{\\xi}G(\\xi,\\eta){\\rm d}{\\zeta}\\eta+\\partial{\\xi}\\partial_{\\eta}G(\\xi,\\eta){\\rm d}{\\zeta}\\xi+\\partial{\\eta}^2G(\\xi,\\eta){\\rm d}{\\zeta}\\eta\\&amp;=&amp;\\partial{\\xi}^2G(\\xi,\\eta)-2\\partial_{\\eta}\\partial_{\\xi}G(\\xi,\\eta)+\\partial_{\\eta}^2G(\\xi,\\eta)\\\\kappa(z,0)&amp;=&amp;G(\\xi,\\xi)=0\\\\kappa(z,z)&amp;=&amp;G(\\xi,0)=-{\\mu \\over 4}\\xi\\end{eqnarray}$$ &emsp;&emsp;于是我们得到坐标转换后的核方程：$$\\begin{eqnarray}\\partial_{\\eta}\\partial_{\\xi}G(\\xi,\\eta)&amp;=&amp;{\\mu \\over 4}G(\\xi,\\eta),\\ (\\xi,\\eta)\\in\\tilde{\\cal T}_R\\tag{32}\\G(\\xi,\\xi)&amp;=&amp;0\\tag{33}\\G(\\xi,0)&amp;=&amp;-{\\mu \\over 4}\\xi\\tag{34}\\end{eqnarray}$$ &emsp;&emsp;现在我们开始求解该核方程。将式(32)在作用范围$\\tilde{\\cal T}R$上积分：$$\\int{\\eta}^{\\xi}\\int_0^{\\eta}\\partial_{\\eta’}\\partial_{\\xi’}G(\\xi’,\\eta’){\\rm d}\\eta’{\\rm d}\\xi’={\\mu \\over 4}\\int_{\\eta}^{\\xi}\\int_0^{\\eta}G(\\xi’,\\eta’){\\rm d}\\eta’{\\rm d}\\xi’$$ &emsp;&emsp;化简后得到：$$G(\\xi,\\eta)=-{\\mu \\over 4}(\\xi-\\eta)+{\\mu \\over 4}\\int_{\\eta}^{\\xi}\\int_0^{\\eta}G(\\xi’,\\eta’){\\rm d}\\eta’{\\rm d}\\xi’\\tag{35}$$ &emsp;&emsp;在这里我们使用不动点迭代（Fixpunktiteration）求解式(35)中的$G(\\xi,\\eta)$。将式(35)右侧的第一项看作$G$的初始项$G_0$，第二项看作对$G$的方程$F[G]$：$$\\begin{eqnarray}G_0(\\xi,\\eta)&amp;=&amp;-{\\mu \\over 4}(\\xi-\\eta)\\F[G] (\\xi,\\eta)&amp;=&amp;{\\mu \\over 4}\\int_{\\eta}^{\\xi}\\int_0^{\\eta}G(\\xi’,\\eta’){\\rm d}\\eta’{\\rm d}\\xi’\\end{eqnarray}$$ &emsp;&emsp;此时式(35)就化为了：$$G(\\xi,\\eta)=G_0(\\xi,\\eta)+F[G] (\\xi,\\eta)\\tag{36}$$ &emsp;&emsp;使用如下等式对$G$进行迭代：$$G_{i+1}(\\xi,\\eta)=G_0(\\xi,\\eta)+F[G_i] (\\xi,\\eta)\\tag{37}$$ &emsp;&emsp;一直迭代下去$G_i(\\xi,\\eta)$就会收敛为核方程$G(\\xi,\\eta)$：$$\\begin{eqnarray}G_1&amp;=&amp;G_0+F[G_0]\\G_2&amp;=&amp;G_0+F[G_1]=G_0+F[G_0]+F^2[G_0]\\&amp;\\vdots&amp;\\end{eqnarray}$$ &emsp;&emsp;最终得到：$$G=\\lim_{i\\to\\infty}G_i=\\sum_{i=0}^{\\infty}F^i[G_0]\\tag{38}$$ &emsp;&emsp;设：$$F^i[G_0]=\\Delta G_i$$ &emsp;&emsp;并且$\\Delta G_i$满足：$$\\Delta G_{i+1}=F[\\Delta G_i],\\ \\ \\Delta G_0=G_0$$ &emsp;&emsp;将$G_0$带入，经过一系列计算（这里我也没有算过，直接搬课件上的）得到：$$\\Delta G_i(\\xi,\\eta)=-\\left({\\mu \\over 4}\\right)^{i+1}{(\\xi-\\eta)\\xi^i\\eta^i \\over i!(i+1)!},\\ i\\in{\\Bbb N}\\tag{39}$$ &emsp;&emsp;带入式(38)得到：$$G(\\xi,\\eta)=\\sum_{i=0}^{\\infty}\\Delta G_i(\\xi,\\eta)=-{\\mu \\over 2}(\\xi-\\eta){I_1(\\sqrt{\\mu\\xi\\eta}) \\over \\sqrt{\\mu\\xi\\eta}}\\tag{40}$$ &emsp;&emsp;其中$I_1(\\cdot)$代表第一类贝塞尔函数。终于我们求得了$G(\\xi,\\eta)$的表达式，最后对其进行坐标变换我们就得到了最初的核函数$\\kappa(z,\\zeta)$：$$\\kappa(z,\\zeta)=-\\mu\\zeta{I_1(\\sqrt{\\mu(z^2-\\zeta^2)}) \\over \\sqrt{\\mu(z^2-\\zeta^2)}}\\tag{41}$$ &emsp;&emsp;将是(41)带入式(22)我们就求得了相应输入$u(t)$：$$u(t)=-(\\gamma+\\mu_R)\\int_0^1z{I_1(\\sqrt{(\\gamma+\\mu_R)(1-z^2)}) \\over \\sqrt{(\\gamma+\\mu_R)(1-z^2)}}x(z,t){\\rm d}z$$ &emsp;&emsp;通过这个输入可以控制不稳定的原系统，将其转化为稳定的目标系统。在这里还需要注意的是，Backstepping求得的边界输入能够将系统所有特征值进行移动，因此它需要保证系统所有特征值都是能控的。 小结 边界输入是作用在系统定义域边界的输入，因此它通过边界条件表示。 边界输入的系统方程能够通过特殊方法转换为分布式输入的系统方程。 使用Backstepping可以设计边界输入的反馈控制，但是计算相当繁琐。","categories":[{"name":"控制理论","slug":"控制理论","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/"},{"name":"分布参数系统控制","slug":"控制理论/分布参数系统控制","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F%E6%8E%A7%E5%88%B6/"}],"tags":[{"name":"Backstepping","slug":"Backstepping","permalink":"https://haosutopia.github.io/tags/Backstepping/"},{"name":"分布参数系统","slug":"分布参数系统","permalink":"https://haosutopia.github.io/tags/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F/"}]},{"title":"分布参数系统控制（六）：反馈控制","slug":"RSVP-06","date":"2018-04-10T22:19:02.000Z","updated":"2018-04-11T20:52:52.000Z","comments":true,"path":"2018/04/RSVP-06/","link":"","permalink":"https://haosutopia.github.io/2018/04/RSVP-06/","excerpt":"","text":"&emsp;&emsp;在控制理论中，稳定性始终是一个系统最基本的要求，之后一切的控制都要建立在系统稳定的基础上。与集总参数系统控制类似，我们可以通过增加反馈使系统达到稳定状态，并满足一定特性。分布参数系统状态方程如下：$$\\dot{x}(t)={\\cal A}x(t)\\tag{1}$$ 指数稳定&emsp;&emsp;首先我们给出分布参数系统的指数稳定（Exponentielle Stabilität）的定义：若存在常数$M\\geq 1,\\alpha&gt;0$，使得系统状态变量$x(t)$满足：$$||x(t)||\\leq Me^{-\\alpha t }x_0,\\ t\\geq 0,\\ \\forall x_0\\in {\\cal H}\\tag{2}$$ &emsp;&emsp;则我们称系统指数稳定。其中范数$||x||$由内积定义：$$||x||=\\sqrt{\\langle x,x\\rangle}=\\sqrt{\\int_0^1|x(z)|^2{\\Bbb d}z}$$ &emsp;&emsp;为了更好地描述系统稳定性，我们定义稳定裕量（Stabilitätsreserve）$\\alpha_{stab}$为衰减率$\\alpha$的下确界（相当于最小值），它可以用来表示$x(t)$趋向于$0$的速度。若$\\alpha_{stab}&gt;0$，则系统指数稳定。&emsp;&emsp;根据第三章节式(13)：$$\\begin{split}x(t)&amp;=&amp;\\sum_{i=1}^{\\infty}e^{\\lambda_it}\\phi_ix_i^\\ast (0)\\&amp;\\leq&amp; e^{(\\sup_{i\\geq 1} {\\rm Re} \\ \\lambda_i)t}\\sum_{i=1}^{\\infty}\\phi_ix_i^\\ast (0)\\&amp;=&amp;e^{(\\sup_{i\\geq 1} {\\rm Re} \\ \\lambda_i)t}x(0)\\end{split}\\tag{3}$$ &emsp;&emsp;我们得到Riesz-Spektral系统的稳定裕量：$$\\alpha_{stab}=-\\sup_{i\\geq 1}{\\rm Re}\\lambda_i\\tag{4}$$ &emsp;&emsp;对一般系统来说，稳定裕量可表示为：$$\\alpha_{stab}=-\\sup_{\\lambda\\in\\sigma({\\cal A})}{\\rm Re}\\lambda\\tag{5}$$ &emsp;&emsp;根据式(5)我们就可以得到系统指数稳定的条件：$\\sup_{\\lambda\\in\\sigma({\\cal A})}{\\rm Re}\\lambda&lt;0$，也就是说${\\cal A}$的谱集（特征值）要在复平面的左半平面，这与集总参数系统类似。 状态反馈反馈算子&emsp;&emsp;状态反馈的目的就是将原系统算子的谱集移动到复平面左半平面，使得系统稳定。它需要给系统增加一个输入$u(t)$，并且$u(t)$由系统状态变量决定。增加输入后的系统可表示为：$$\\begin{eqnarray}\\dot{x}(t)&amp;=&amp;{\\cal A}x(t)+{\\cal B}u(t)\\tag{6}\\u(t)&amp;=&amp;-{\\cal K}x(t)\\tag{7}\\end{eqnarray}$$ &emsp;&emsp;其中$\\cal K$为反馈算子（Rückführoperator），它将系统变量$x(t)$映射为一个$p$维向量。根据里斯表示定理（Rieszscher Darstellungssatz）：每个${\\cal H}\\to {\\Bbb C}$的泛函$f$，都存在一个定义域上的函数$\\xi_0$，使得$f(\\cdot)=\\langle\\ \\cdot,\\xi_0\\rangle$。也就是说，泛函空间的每个函数都可以表示为内积的形式。因此$\\cal K$可以表示为：$${\\cal K}=\\begin{bmatrix}\\langle\\ \\cdot\\ ,k_1\\rangle \\ \\vdots \\ \\langle\\ \\cdot\\ ,k_p\\rangle\\end{bmatrix}\\tag{8}$$ &emsp;&emsp;其中$k_1,…,k_p$均为定义域上函数，它们可以在状态空间中分解：$$k_i=\\sum_{j=1}^{\\infty}\\langle k_i,\\phi_j\\rangle\\psi_j=\\sum_{j=1}^{\\infty}k_{ij}^{\\ast}\\psi_j\\tag{9}$$ &emsp;&emsp;这里要注意，式(9)的分解与之前不同，它是以${\\cal A}^\\ast$的特征元素$\\psi_i$为基进行的，至于原因纯粹是为了之后的计算。&emsp;&emsp;将输入$u(t)$带入原系统得到：$$\\dot{x}(t)=({\\cal A-BK})x(t)=\\tilde{\\cal A}x(t)\\tag{10}$$ &emsp;&emsp;通过状态反馈，我们将原系统算子$\\cal A$变成了$\\tilde{\\cal A}$，并且希望$\\tilde{\\cal A}$的谱集在复平面左半平面。 系统可稳定条件&emsp;&emsp;系统可稳定条件（Stabilisierbarkeit）是反馈能够有效的前提，它包含如下条件：&emsp;&emsp;1. 原系统算子$\\cal A$在右半平面只有有限个特征值。&emsp;&emsp;2. 系统能控。&emsp;&emsp;3. 左半平面特征值不能无限趋近于虚轴。&emsp;&emsp;由于分布参数系统是无限维的，故$\\cal A$可能有无限个$\\lambda$在右半平面，仅使用$p$维输入无法将它们全部移动到理想位置，因此系统需要满足条件一。我们设系统前$n$个特征值$\\lambda_1,…,\\lambda_n$在右半平面，其余$\\lambda$均在左半平面，因此反馈只需要改变前$n$个$\\lambda$，而其余特征值保持不变：$$\\begin{matrix}{\\cal A} &amp;&amp; \\lambda_1 &amp;&amp; … &amp;&amp; \\lambda_n &amp;&amp; \\lambda_{n+1} &amp;&amp; \\lambda_{n+2} &amp;&amp; … \\\\downarrow &amp;&amp; \\downarrow &amp;&amp; &amp;&amp; \\downarrow &amp;&amp; \\downarrow &amp;&amp; \\downarrow &amp;&amp; \\\\tilde{\\cal A} &amp;&amp; \\tilde{\\lambda}1 &amp;&amp; … &amp;&amp; \\tilde{\\lambda}n &amp;&amp; \\lambda{n+1} &amp;&amp; \\lambda{n+2} &amp;&amp; …\\end{matrix}$$ &emsp;&emsp;由于$\\lambda_i(i&gt;n)$同时是$\\cal A$与$\\tilde{\\cal A}$的特征值，因此满足：$$\\begin{eqnarray}&amp;&amp;({\\cal A-BK})\\phi_i=\\lambda_i\\phi_i,\\ i&gt;n\\\\Rightarrow&amp;&amp;{\\cal BK}\\phi_i=0,\\ i&gt;n\\\\Rightarrow&amp;&amp;\\begin{bmatrix}\\langle \\phi_i,k_1\\rangle\\ \\vdots \\ \\langle \\phi_i,k_p\\rangle\\end{bmatrix}=0,\\ i&gt;n\\end{eqnarray}$$ &emsp;&emsp;其中：$$\\begin{split}\\langle \\phi_i,k_j\\rangle&amp;=\\langle \\phi_i,\\sum_{l=1}^{\\infty}k_{jl}^{\\ast}\\psi_l\\rangle\\&amp;=\\sum_{l=1}^{\\infty}\\overline{k_{jl}^{\\ast} }\\langle\\phi_i,\\psi_l\\rangle\\&amp;=\\sum_{l=1}^{\\infty}\\overline{k_{jl}^{\\ast} }\\delta_{il}\\&amp;=\\overline{k_{ji}^{\\ast} }=0,\\ i&gt;n\\end{split}$$ &emsp;&emsp;因此式(9)转化为：$$k_i=\\sum_{j=1}^{\\infty}k_{ij}^{\\ast}\\psi_j=\\sum_{j=1}^nk_{ij}^{\\ast}\\psi_j\\tag{11}$$ &emsp;&emsp;同时$\\cal K$能够表示为：$$\\begin{split}{\\cal K}&amp;=\\begin{bmatrix}\\langle\\ \\cdot\\ ,k_1\\rangle \\ \\vdots \\ \\langle\\ \\cdot\\ ,k_p\\rangle\\end{bmatrix}=\\begin{bmatrix}\\langle\\ \\cdot\\ ,\\sum_{j=1}^nk_{1j}^{\\ast}\\psi_j\\rangle \\ \\vdots \\ \\langle\\ \\cdot\\ ,\\sum_{j=1}^nk_{pj}^{\\ast}\\psi_j\\rangle\\end{bmatrix}=\\begin{bmatrix}\\sum_{j=1}^n\\overline{k_{1j}^{\\ast} }\\langle\\ \\cdot\\ ,\\psi_j\\rangle \\ \\vdots \\ \\sum_{j=1}^n\\overline{k_{pj}^{\\ast} }\\langle\\ \\cdot\\ ,\\psi_j\\rangle\\end{bmatrix}\\&amp;=\\underbrace{\\begin{bmatrix}\\overline{k_{11}^{\\ast} } &amp;&amp; \\overline{k_{12}^{\\ast} } &amp;&amp; \\cdots &amp;&amp; \\overline{k_{1n}^{\\ast} }\\\\vdots &amp;&amp; \\vdots &amp;&amp; \\ddots &amp;&amp; \\vdots\\\\overline{k_{p1}^{\\ast} } &amp;&amp; \\overline{k_{p2}^{\\ast} } &amp;&amp; \\cdots &amp;&amp; \\overline{k_{pn}^{\\ast} }\\end{bmatrix} }_{\\overline{K^{\\ast} }(p\\times n)} \\begin{bmatrix}\\langle\\ \\cdot\\ ,\\psi_1\\rangle \\ \\vdots \\ \\langle\\ \\cdot\\ ,\\psi_n\\rangle\\end{bmatrix}\\end{split}$$ &emsp;&emsp;通过上述推导我们将反馈算子$\\cal K$转化为了反馈矩阵$\\overline{K^{\\ast} }$与一个算子的乘积，其中算子是原系统决定的，因此$\\cal K$只由$\\overline{K^{\\ast} }$决定。接下来我们只要设计反馈矩阵就能得到相应的反馈算子。若将反馈算子作用于状态变量：$${\\cal K}x(t)=\\sum_{i=1}^{\\infty}x_i^{\\ast}{\\cal K}\\phi_i=\\overline{K^{\\ast} }\\begin{bmatrix}x_1^{\\ast} \\ \\vdots \\ x_n^{\\ast} \\end{bmatrix}$$ &emsp;&emsp;我们发现$\\cal K$对前$n$项的$\\phi_i$都有相应反馈，并且反馈到$p$个输入的大小分别为：$\\overline{k_{1i}^{\\ast} },\\overline{k_{2i}^{\\ast} },…,\\overline{k_{pi}^{\\ast} }$，以此来移动前$n$个特征值；同时对其余$\\phi_i$均无反馈，故其余特征值保持不变。 系统能控性条件&emsp;&emsp;系统可稳定条件中条件二要求系统是能控的，这里我们直接给出系统能控性条件，其证明能够在之后的推导过程中得到。&emsp;&emsp;在反馈控制中我们使用${\\cal B}u(t)$来控制前$n$个特征值$\\lambda_1,…,\\lambda_n$，其中：$${\\cal B}u(t)=\\sum_{i=1}^pb_iu_i(t)=\\sum_{i=1}^{\\infty}\\phi_ib_i^{\\ast}u(t)$$ &emsp;&emsp;系统能控的条件为：$B^{\\ast}$无零行。其中$B^{\\ast}$定义如下：$$B^{\\ast}=\\begin{bmatrix}{b_1^{\\ast} }^T \\ \\vdots \\ {b_n^{\\ast} }^T\\end{bmatrix}=\\begin{bmatrix}\\langle b_1,\\psi_1\\rangle &amp;&amp; \\cdots &amp;&amp; \\langle b_p,\\psi_1\\rangle\\\\vdots &amp;&amp; \\ddots &amp;&amp; \\vdots \\\\langle b_1,\\psi_n\\rangle &amp;&amp; \\cdots &amp;&amp; \\langle b_p,\\psi_n\\rangle\\end{bmatrix}\\tag{12}$$ 反馈矩阵计算&emsp;&emsp;现在就剩设计有限维的反馈矩阵了。我们利用反馈后系统算子$\\tilde{\\cal A}$的特征值$\\tilde{\\lambda}_1,…,\\tilde{\\lambda}_n$与特征向量$\\tilde{\\phi}_1,…,\\tilde{\\phi}_n$，得到：$$\\begin{eqnarray}&amp;&amp;({\\cal A-BK})\\tilde{\\phi}_i=\\tilde{\\lambda}_i\\tilde{\\phi}_i,\\ i=1,2,…,n\\tag{13}\\\\Rightarrow&amp;&amp;(\\tilde{\\lambda}_iI-{\\cal A})\\tilde{\\phi}_i=-{\\cal BK}\\tilde{\\phi}_i\\tag{14}\\end{eqnarray}$$ &emsp;&emsp;在这里我们引入参数向量（Parametervektoren）$p_i$，它是一个$p$维向量：$$p_i={\\cal K}\\tilde{\\phi}_i=\\overline{K^{\\ast} }\\begin{bmatrix}\\langle\\tilde{\\phi}_i,\\psi_1\\rangle \\ \\vdots \\ \\langle\\tilde{\\phi}_i,\\psi_n\\rangle\\end{bmatrix}=\\overline{K^{\\ast} }\\tilde{v}_i\\tag{15}$$ &emsp;&emsp;其中：$$\\tilde{v}i=\\begin{bmatrix}\\langle\\tilde{\\phi}_i,\\psi_1\\rangle \\ \\vdots \\ \\langle\\tilde{\\phi}_i,\\psi_n\\rangle\\end{bmatrix}=\\begin{bmatrix}c{i1}^{\\ast} \\ \\vdots \\ c_{in}^{\\ast}\\end{bmatrix}\\tag{16}$$ &emsp;&emsp;将$p_i$合并得到：$$[p_1,…,p_n]=\\overline{K^{\\ast} }[\\tilde{v}_1,…,\\tilde{v}_n]=\\overline{K^{\\ast} }\\tilde{V}\\tag{17}$$ &emsp;&emsp;在这里我们先来看看这些$p_i$、$\\tilde{v}i$具体表示什么意义。首先看$p_i$，根据式(15)定义我们可以知道$p_i$表示状态变量$x=\\tilde{\\phi}_i$时的反馈量。而一般情况下状态变量可由基$\\tilde{\\phi}_i$排列组合而成：$$x=\\sum{i=1}^{\\infty}\\tilde{x}_i^{\\ast}\\tilde{\\phi}_i$$ &emsp;&emsp;取前$n$个$\\tilde{x}i^{\\ast}$为$x$的坐标（由于$n$项之后的特征向量均没有反馈，故省略），则$x$所产生的反馈量为：$$\\begin{split}{\\cal K}x&amp;=\\sum{i=1}^n{\\cal K}\\tilde{\\phi}_i\\tilde{x}i^{\\ast}=\\sum{i=1}^np_i\\tilde{x}_i^{\\ast}\\&amp;=[p_1,…,p_n]\\begin{bmatrix}\\tilde{x}_1^{\\ast} \\ \\vdots \\ \\tilde{x}_n^{\\ast} \\end{bmatrix}\\end{split}\\tag{18}$$ &emsp;&emsp;同样状态变量也可由基$\\phi_i$排列组合而成：$$x=\\sum_{i=1}^{\\infty}x_i^{\\ast}\\phi_i$$ &emsp;&emsp;根据$\\cal K$定义，$x$所产生的反馈量也可以表示为：$${\\cal K}x=\\overline{K^{\\ast} }\\begin{bmatrix}x_1^{\\ast} \\ \\vdots \\ x_n^{\\ast} \\end{bmatrix}\\tag{19}$$ &emsp;&emsp;根据式(17)~(19)可以得到：$$[\\tilde{v}_1,…,\\tilde{v}_n]\\begin{bmatrix}\\tilde{x}_1^{\\ast} \\ \\vdots \\ \\tilde{x}_n^{\\ast} \\end{bmatrix}=\\begin{bmatrix}x_1^{\\ast} \\ \\vdots \\ x_n^{\\ast} \\end{bmatrix}$$ &emsp;&emsp;因此$[\\tilde{v}1,…,\\tilde{v}n]$实际上就是从基$\\tilde{\\phi}$到基$\\phi$的坐标变换矩阵。而$c{i1}^{\\ast},…,c{in}^{\\ast}$则表示新特征向量$\\tilde{\\phi}_i$在基$\\phi$下前$n$个坐标。&emsp;&emsp;搞清楚符号含义后我们就要根据新定义的$p_i$、$\\tilde{v}_i$来得到反馈矩阵了。假设新特征值$\\tilde{\\lambda}_i$没有重复，并且与原特征值$\\lambda$不相等，则$\\tilde{v}_1,…,\\tilde{v}_n$线性无关（可用反证法证明），$\\tilde{V}$存在逆矩阵。又根据式(17)可以得到：$$\\overline{K^{\\ast} }=[p_1,…,p_n] [\\tilde{v}_1,…,\\tilde{v}_n]^{-1}\\tag{20}$$ &emsp;&emsp;现在我们需要在式(20)中将$\\tilde{v}_i$消去。根据式(14)(15)：$$\\begin{eqnarray}\\tilde{\\phi}i&amp;=&amp;({\\cal A}-\\tilde{\\lambda}_iI)^{-1}{\\cal B}p_i\\tag{21}\\&amp;=&amp;\\sum{j=1}^{\\infty}{\\langle {\\cal B}p_i,\\psi_j\\rangle \\over \\lambda_j - \\tilde{\\lambda}_i}\\phi_j\\tag{22}\\&amp;=&amp;\\sum_{j=1}^{\\infty}{ {b_j^{\\ast} }^Tp_i \\over \\lambda_j - \\tilde{\\lambda}_i}\\phi_j\\tag{23}\\end{eqnarray}$$ &emsp;&emsp;其中式(21)推导式(22)时用到了下列等式（可以通过状态空间分解证明）：$$(\\lambda I-{\\cal A})^{-1}=\\sum_{i=1}^{\\infty}{\\langle \\ \\cdot\\ ,\\psi_i\\rangle \\over \\lambda - \\lambda_i}\\phi_i$$ &emsp;&emsp;由于$c_{ij}^{\\ast}$为$\\tilde{\\phi}i$在基$\\phi_j$下的坐标，因此$\\tilde{\\phi}_i$又可表示为：$$\\tilde{\\phi}i=\\sum{j=1}^{\\infty}c{ij}^{\\ast}\\phi_j\\tag{24}$$ &emsp;&emsp;将式(23)(24)联立得到：$$c_{ij}^{\\ast}={ {b_j^{\\ast} }^Tp_i \\over \\lambda_j - \\tilde{\\lambda}_i}\\tag{25}$$ &emsp;&emsp;带入式(16)中：$$\\begin{split}\\tilde{v}i&amp;=\\begin{bmatrix}c{i1}^{\\ast} \\ \\vdots \\ c_{in}^{\\ast}\\end{bmatrix}=\\begin{bmatrix}{ {b_1^{\\ast} }^Tp_i \\over \\lambda_1 - \\tilde{\\lambda}i} \\ \\vdots \\ { {b_n^{\\ast} }^Tp_i \\over \\lambda_n - \\tilde{\\lambda}_i}\\end{bmatrix}\\&amp;=\\underbrace{\\begin{bmatrix}{ 1 \\over \\lambda_1 - \\tilde{\\lambda}_i} &amp;&amp; &amp;&amp; \\ &amp;&amp; \\ddots &amp;&amp; \\ &amp;&amp; &amp;&amp; { 1 \\over \\lambda_n - \\tilde{\\lambda}_i}\\end{bmatrix} }{(\\Lambda-\\tilde{\\lambda}i I)^{-1} }\\underbrace{\\begin{bmatrix}{b_1^{\\ast} }^T \\ \\vdots \\ {b_n^{\\ast} }^T\\end{bmatrix} }{B^{\\ast} }p_i\\end{split}$$ &emsp;&emsp;因此：$$\\tilde{v}_i=(\\Lambda-\\tilde{\\lambda}_i I)^{-1}B^{\\ast}p_i\\tag{26}$$ &emsp;&emsp;式(26)带入式(20)得到$\\overline{K^{\\ast} }$的最终结果：$$\\overline{K^{\\ast} }=[p_1,…,p_n] [(\\Lambda-\\tilde{\\lambda}_1 I)^{-1}B^{\\ast}p_1,…,(\\Lambda-\\tilde{\\lambda}_n I)^{-1}B^{\\ast}p_n]^{-1}\\tag{27}$$ &emsp;&emsp;式(27)可以得到系统能控性条件的证明：若$B^{\\ast}$有全零行，则式(27)最后项无法取逆，因此$B^{\\ast}$不能有全零行。同时它表明，要设计一个反馈我们只需要设计$\\tilde{\\lambda}$和$p$就可以了。 反馈控制设计条件&emsp;&emsp;1. 原算子$\\cal A$只有有限$\\lambda&gt;0$。&emsp;&emsp;2. 系统能控。&emsp;&emsp;3. $\\cal A$的其余$\\lambda$不能无限接近虚轴。&emsp;&emsp;4. 设计后$\\tilde{\\cal A}$的特征值$\\tilde{\\lambda}$为实数或为成对出现的虚数，且相互不重合。&emsp;&emsp;5. $\\tilde{\\lambda}$不与$\\cal A$的特征值或特征值聚点重合。&emsp;&emsp;6. 若$\\tilde{\\lambda}_i=\\overline{\\tilde{\\lambda}_j }$，则需满足$p_i=\\overline{p_j}$。&emsp;&emsp;7. 选取$\\tilde{\\lambda}$与$p$时，需保证$\\tilde{v}$线性不相关。&emsp;&emsp;以上将反馈控制设计条件做了一个总结，其中前三条为对原算子$\\cal A$的要求，后四条为对$\\tilde{\\lambda}$与$p$的要求。 小结 分布参数系统指数稳定条件与集总参数系统类似，要求谱集位于复平面的左半平面。 设置$\\tilde{\\lambda}$和$p$来得到反馈算子。 反馈控制设计需满足一定条件。","categories":[{"name":"控制理论","slug":"控制理论","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/"},{"name":"分布参数系统控制","slug":"控制理论/分布参数系统控制","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F%E6%8E%A7%E5%88%B6/"}],"tags":[{"name":"分布参数系统","slug":"分布参数系统","permalink":"https://haosutopia.github.io/tags/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F/"}]},{"title":"分布参数系统控制（五）：模型近似","slug":"RSVP-05","date":"2018-03-24T19:19:41.000Z","updated":"2018-03-25T14:09:44.000Z","comments":true,"path":"2018/03/RSVP-05/","link":"","permalink":"https://haosutopia.github.io/2018/03/RSVP-05/","excerpt":"","text":"&emsp;&emsp;分布参数系统是个无限维的系统，在第一章节所提到的early-lumping控制中，我们先将系统过程近似为集总参数系统，再使用现代控制方法对系统进行分析控制。本文将阐述模型近似的过程及相关注意事项。 模型近似&emsp;&emsp;分布参数系统状态方程如下：$$\\begin{eqnarray}\\dot{x}(t)&amp;=&amp;\\mathcal{A}x(t)+\\mathcal{B}u(t),\\ &amp;t&gt;0,\\ x(0)=x_0\\in X\\tag{1}\\y(t)&amp;=&amp;\\mathcal{C}x(t),\\ &amp;t\\geq0\\tag{2}\\end{eqnarray}$$ &emsp;&emsp;在第三章节中我们把系统算子$\\cal A$的特征元素${\\phi_i,i\\geq 1}$作为整个状态空间的基，将系统分解后得到：$$\\sum_{i=1}^{\\infty}\\dot{x}i^\\ast (t)\\phi_i=\\sum_{i=1}^{\\infty}x_i^\\ast (t)\\mathcal{A}\\phi_i+\\sum{i=1}^{\\infty}\\phi_i{b_i^\\ast }^Tu(t)\\tag{3}$$ &emsp;&emsp;其中每一个分量的方程为：$$\\dot{x}_i^\\ast (t)=\\lambda_ix_i^\\ast (t)+{b_i^\\ast }^Tu(t)\\tag{4}$$ &emsp;&emsp;此时状态量$x(t)$可以表示为：$$x(t)=\\sum_{i=1}^{\\infty}\\langle x(t),\\psi_i\\rangle\\phi_i=\\sum_{i=1}^{\\infty}x_i^\\ast (t)\\phi_i\\tag{5}$$ &emsp;&emsp;它由无限个特征元素$\\phi_i$排列组合而成，在这里我们只取前$n$个特征元素作为状态变量的近似，并将其余特征元素舍弃：$$x(t)=\\sum_{i=1}^nx_i^\\ast (t)\\phi_i\\tag{6}$$ &emsp;&emsp;根据式(4)，$x_i^\\ast(t)$可由下列微分方程组得到：$$\\begin{cases}\\dot{x}_1^\\ast (t)=\\lambda_1x_1^\\ast (t)+{b_1^\\ast }^Tu(t)\\\\dot{x}_2^\\ast (t)=\\lambda_2x_2^\\ast (t)+{b_2^\\ast }^Tu(t)\\\\quad\\vdots\\\\dot{x}_n^\\ast (t)=\\lambda_nx_n^\\ast (t)+{b_n^\\ast }^Tu(t)\\end{cases}$$ &emsp;&emsp;在这里我们可以把$x_i^\\ast(t)$看作状态分量，将方程组表示为向量形式得到：$$\\begin{bmatrix}\\dot{x}_1^\\ast\\\\dot{x}_2^\\ast\\\\vdots\\\\dot{x}_n^\\ast\\end{bmatrix}=\\begin{bmatrix}\\lambda_1&amp; &amp; &amp; \\ &amp;\\lambda_2 &amp; &amp; \\&amp; &amp; \\ddots &amp; \\ &amp; &amp; &amp; \\lambda_n \\end{bmatrix} \\begin{bmatrix} x_1^\\ast \\ x_2^\\ast \\ \\vdots \\ x_n^\\ast \\end{bmatrix} + \\begin{bmatrix} {b_1^\\ast }^T \\ {b_2^\\ast }^T \\ \\vdots \\ {b_n^\\ast }^T \\end{bmatrix} \\begin{bmatrix} x_1^\\ast \\ x_2^\\ast \\ \\vdots \\ x_n^\\ast \\end{bmatrix}$$ &emsp;&emsp;设近似系统状态变量：$$x^\\ast(t)=\\begin{bmatrix} x_1^\\ast(t) \\ x_2^\\ast(t) \\ \\vdots \\ x_n^\\ast(t) \\end{bmatrix}\\tag{7}$$ &emsp;&emsp;系统矩阵：$$\\Lambda=\\begin{bmatrix}\\lambda_1&amp; &amp; &amp; \\ &amp;\\lambda_2 &amp; &amp; \\&amp; &amp; \\ddots &amp; \\ &amp; &amp; &amp; \\lambda_n \\end{bmatrix}\\tag{8}$$ &emsp;&emsp;输入矩阵：$$B^\\ast=\\begin{bmatrix} {b_1^\\ast }^T \\ {b_2^\\ast }^T \\ \\vdots \\ {b_n^\\ast }^T \\end{bmatrix}\\tag{9}$$ &emsp;&emsp;结合式(7)~(9)，我们可以将式(1)近似为：$$x^\\ast(t)=\\Lambda x^\\ast(t)+B^\\ast u(t),\\ t&gt; 0,\\ x^\\ast(0)=x_0^\\ast\\in\\Bbb{C}^n\\tag{10}$$ &emsp;&emsp;下面我们再来看式(2)，根据式(2)(6)，得到$y(t)$的近似值$y^\\ast(t)$：$$y^\\ast(t)=\\sum_{i=1}^nx_i^\\ast(t){\\cal C}\\phi_i$$ &emsp;&emsp;将其向量化后得到：$$y^\\ast(t)=\\begin{bmatrix}{\\cal C}\\phi_1 &amp; \\cdots &amp; {\\cal C}\\phi_n\\end{bmatrix}x^\\ast(t)$$ &emsp;&emsp;设输出矩阵：$$C^\\ast=\\begin{bmatrix}{\\cal C}\\phi_1 &amp; \\cdots &amp; {\\cal C}\\phi_n\\end{bmatrix}\\tag{11}$$ &emsp;&emsp;因此式(2)可以近似为：$$y^\\ast(t)=C^\\ast x^\\ast(t),\\ t\\geq 0\\tag{12}$$ &emsp;&emsp;将式(10)(12)合并，就是近似后的有限维集总参数系统：$$\\begin{eqnarray}x^\\ast(t)&amp;=&amp;\\Lambda x^\\ast(t)+B^\\ast u(t),\\ &amp;t&gt; 0,\\ x^\\ast(0)=x_0^\\ast\\in\\Bbb{C}^n\\tag{13}\\y^\\ast(t)&amp;=&amp;C^\\ast x^\\ast(t),\\ &amp;t\\geq 0\\tag{14}\\end{eqnarray}$$ &emsp;&emsp;此后我们就能够使用现代控制中的方法对它进行分析或控制了。 注意事项&emsp;&emsp;在上述过程中，我们取了$\\cal A$的前$n$个特征元素作为系统的近似，只保留了系统的部分信息，因此在实际控制中会存在一些偏差（spillover），并且在一定程度影响系统的稳定值，这也是early-lumping的最大缺陷。$n$越大，系统舍弃的信息越少，偏差也会越小，故在实际使用中通常会取相对较大的$n$值来减少偏差的影响。&emsp;&emsp;系统近似后，我们使用现代控制相应方法设计控制器。但由于控制器是根据近似后系统得到的，因此若中途更改$n$值，需要根据改变后的状态方程重新设计控制器，这也是early-lumping的缺陷所在。另外提一句，我们这里得到的系统矩阵为$n\\times n$，用同样的方法也能将原始系统算子转化为矩阵，只不过此时矩阵的维度为无限维。 小结 通过取前$n$个特征元素将分布参数系统近似为集总参数系统。 近似系统的一些注意事项。","categories":[{"name":"控制理论","slug":"控制理论","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/"},{"name":"分布参数系统控制","slug":"控制理论/分布参数系统控制","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F%E6%8E%A7%E5%88%B6/"}],"tags":[{"name":"分布参数系统","slug":"分布参数系统","permalink":"https://haosutopia.github.io/tags/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F/"}]},{"title":"分布参数系统控制（四）：解的存在唯一性","slug":"RSVP-04","date":"2018-03-21T00:35:21.000Z","updated":"2018-04-10T22:24:34.000Z","comments":true,"path":"2018/03/RSVP-04/","link":"","permalink":"https://haosutopia.github.io/2018/03/RSVP-04/","excerpt":"","text":"&emsp;&emsp;在求解微分方程的初值问题时，我们通常要考虑解的存在唯一性，以及解受初始条件的影响。一般来说，我们希望问题的解存在且唯一，并且连续地取决于初始条件（通常无法准确得知状态变量初始值，导致实际应用中会产生一定的偏差。因此我们希望当初始值$x(0)$微小变化时，$x(t)$的变化足够小）。我们称这样的问题为适定性问题（wohlgestelltes Problem）。&emsp;&emsp;本文的目标就是得到某些条件，并用它们来判断分布参数模型的初值问题是否为适定性问题。系统状态方程如下：$$\\begin{eqnarray}\\dot{x}(t)&amp;=&amp;\\mathcal{A}x(t)+\\mathcal{B}u(t),\\ &amp;t&gt;0,\\ x(0)=x_0\\in X\\tag{1}\\y(t)&amp;=&amp;\\mathcal{C}x(t),\\ &amp;t\\geq0\\tag{2}\\end{eqnarray}$$ &emsp;&emsp;由于其中数学证明较为困难，因此这里只给出一些结论以及方法。 $C_0$半群引入&emsp;&emsp;在第三章节式(13)中我们得到了分布参数系统状态变量$x(z,t)$的通解为：$$x(z,t)=\\sum_{i=1}^{\\infty}e^{\\lambda_it}\\phi_i(z)x_i^\\ast (0)+\\sum_{i=1}^{\\infty}\\int_0^te^{\\lambda_i(t-\\tau)}\\phi_i(z){b_i^\\ast }^Tu(\\tau){\\rm d}\\tau\\tag{3}$$ &emsp;&emsp;而集总参数系统中状态变量的通解为：$$x(t)=e^{At}x(0)+\\int_0^te^{A(t-\\tau)}{\\rm b}u(\\tau){\\rm d}\\tau\\tag{4}$$ &emsp;&emsp;比较式(3)与式(4)发现它们能表达为统一形式：$$x(t)={\\cal T}(t)x(0)+\\int_0^t{\\cal T}(t-\\tau){\\cal B}u(\\tau){\\rm d}\\tau\\tag{5}$$ &emsp;&emsp;式(3)中：$${\\cal T}(t)=\\sum_{i=1}^{\\infty}e^{\\lambda_it}\\phi_i\\langle\\ \\cdot\\ ,\\psi_i\\rangle\\tag{6}$$ &emsp;&emsp;式(4)中：$${\\cal T}(t)=e^{At}\\tag{7}$$ &emsp;&emsp;式(6)式(7)有着共同的特性，它们都是$C_0$半群。 定义&emsp;&emsp;$C_0$半群是指数函数$e^{At}$的推广。其定义如下：设$X$为Banach空间（有范数的完备空间），${ {\\cal T}(t),t\\geq 0}$是$X$上的$C_0$半群，当且仅当它满足：&emsp;&emsp;1. ${\\cal T}(0)=I$（$I$是$X$上的单位元）&emsp;&emsp;2. ${\\cal T}(t+\\tau)={\\cal T}(t)\\cdot{\\cal T}(\\tau),\\ t\\geq 0,\\ \\tau\\geq 0$&emsp;&emsp;3. $\\lim_{t\\to 0^+}||{\\cal T}(t)x_0-x_0||=0,\\ \\forall x_0\\in X$ &emsp;&emsp;其中条件一、二分别表示$C_0$半群存在幺元（单位元）和结合律，只有群的一半性质，因此是一个半群（群性质为：封闭性，结合律，幺元和逆元）；条件三表示算子是强连续的。因此$C_0$半群也称为强连续算子半群，它的元素都是有界线性算子。它们作用于空间$X$中元素时，得到：$$x(t)={\\cal T}(t)x(0)\\tag{8}$$ 无穷小母元&emsp;&emsp;若有算子$\\cal A$，它的定义域为空间$X$的子集，即$D({\\cal A})\\subset X$，并且满足：$${\\cal A}=\\lim_{t\\to 0^+}{1 \\over t}({\\cal T}(t)-I)\\tag{9}$$ &emsp;&emsp;则称$\\cal A$为$C_0$半群的无穷小母元（infinitesimaler Generator）。若将$\\cal A$作用于空间$X$中元素$x(t)$：$$\\begin{split}{\\cal A}x(t)&amp;=\\lim_{\\epsilon\\to 0^+}{1\\over \\epsilon}({\\cal T}(\\epsilon)-{\\cal T}(0))x(t)\\&amp;=\\lim_{\\epsilon\\to 0^+}{1\\over \\epsilon}(x(t+\\epsilon)-x(t))\\&amp;=\\dot{x}(t)\\end{split}$$ &emsp;&emsp;我们得到：$$\\dot{x}(t)={\\cal A}x(t)\\tag{10}$$ &emsp;&emsp;$C_0$半群的无穷小母元可以作为分布参数系统状态方程的系统算子，而无穷小母元和$C_0$半群又是一一对应的（这里不考虑定义域问题）。因此若式(1)中系统算子$\\cal A$对应的$C_0$半群存在，就可以将$x(t)$表示为式(8)的形式，那么该系统初值问题就是适定性问题。 适定性问题判定&emsp;&emsp;若要判定式(1)的系统初值问题为适定性问题，只需要证明系统算子$\\cal A$对应的$C_0$半群存在就行了。通常我们有三种方法：Riesz-Spektral系统判别、Hille-Yoshida定理和Lumer-Phillips定理。 Riesz-Spektral系统判别&emsp;&emsp;若系统为Riesz-Spektral系统，并且系统算子满足$\\sup_{i\\geq 1}{\\rm Re}\\lambda_i&lt;\\infty$，则根据第三章节,系统算子所对应的$C_0$半群为：$${\\cal T}(t)=\\sum_{i=1}^{\\infty}e^{\\lambda_it}\\phi_i\\langle\\ \\cdot\\ ,\\psi_i\\rangle$$ &emsp;&emsp;因此系统算子$\\cal A$存在对应的$C_0$半群。 Hille-Yoshida定理&emsp;&emsp;若$\\cal A$是定义在空间$X$上的闭线性算子，并且满足：&emsp;&emsp;1. $D({\\cal A})$在$X$上稠密&emsp;&emsp;2. 对所有实$\\lambda&gt;\\omega$且$\\lambda\\in\\rho({\\cal A})$，下式对所有正整数$n$均成立：$$||(\\lambda I-{\\cal A})^{-n}||\\leq{M \\over (\\lambda-\\omega)^n},\\ M&gt;0\\tag{11}$$ &emsp;&emsp;则$\\cal A$存在对应的$C_0$半群${\\cal T(t)}$，且满足$||{\\cal T}(t)||\\leq Me^{\\omega t},\\ t\\geq 0$。在这里式(9)左侧部分可以看作$C_0$半群的泰勒展开，右侧部分看作指数函数的泰勒展开。若$C_0$半群的泰勒展开每一项都小于等于指数函数，则合起来之后就得到了$||{\\cal T}(t)||\\leq Me^{\\omega t}$。 Lumer-Phillips定理&emsp;&emsp;若$\\cal A$是定义在空间$X$上的闭线性算子，并且满足：&emsp;&emsp;1. $D({\\cal A})$在$X$上稠密&emsp;&emsp;2. ${\\cal A}$是消散的（disspativ）&emsp;&emsp;3. $\\lambda I-{\\cal A}$映射为$X$，其中$\\lambda$为任意正数&emsp;&emsp;则$\\cal A$存在对应的$C_0$半群${\\cal T}(t)$，且满足$||{\\cal T}(t)||\\leq 1,\\ t\\geq 0$。这样的$C_0$半群也叫收缩半群（Kontraktionshalbgruppe） &emsp;&emsp;在这里我们不去证明它们。我们只需要使用它们来证明系统算子$\\cal A$对应的$C_0$半群存在，就可以判定系统初值问题为适定性问题，它的解存在且唯一，并且连续地取决于初始条件。 二阶系统适定性条件二阶系统&emsp;&emsp;二阶系统相对比较复杂，这里我们以弹性梁（Elastischer Balken）系统为例：&emsp;&emsp;一般来说，弹性梁系统可以表示为如下形式：$$\\begin{eqnarray}\\ddot{w}(t)+{\\cal D}\\dot{w}(t)+{\\cal S}w(t)&amp;=&amp;{\\cal G}u(t),\\ &amp;t&gt;0&amp;\\tag{12}\\y(t)&amp;=&amp;{\\cal H}_0w(t)+{\\cal H}_1\\dot{w}(t),\\ &amp;t\\geq 0\\tag{13}&amp;\\end{eqnarray}$$ &emsp;&emsp;其中：&emsp;&emsp;$\\circ$ 状态变量$w(t)={w(z,t),\\ z\\in\\Omega}$，表示$z$点处梁的形变量；&emsp;&emsp;$\\circ$ 算子${\\cal S}h={\\rm d}_z^4h:D({\\cal S})\\subset H\\to H$；&emsp;&emsp;$\\circ$ 阻尼算子${\\cal D}:D({\\cal D})\\subset H\\to H$，通常${\\cal D}=0$或${\\cal S}^{1\\over 2}$；&emsp;&emsp;$\\circ$ 输入算子${\\cal G}:\\Bbb{C}^p\\to H$；&emsp;&emsp;$\\circ$ 输出算子${\\cal H}_0:H\\to \\Bbb{C}^m$与${\\cal H}_1:H\\to \\Bbb{C}^m$。 &emsp;&emsp;我们定义状态变量：$$\\begin{split}x_1(t)&amp;={w(z,t),\\ z\\in\\Omega}\\x_2(t)&amp;={\\partial_tw(z,t),\\ z\\in\\Omega}\\end{split}$$ &emsp;&emsp;设系统算子$\\cal A$：$${\\cal A}=\\begin{bmatrix}0 &amp; I\\ -{\\cal S} &amp; -{\\cal D}\\end{bmatrix},\\ D({\\cal A})=\\left{\\begin{bmatrix}h_1\\h_2\\end{bmatrix}\\in\\begin{bmatrix}D({\\cal S}^{ {1\\over 2} })\\D({\\cal S}^{ {1\\over 2} })\\end{bmatrix}|{\\cal S}(h_1+{\\cal S}^{-1}{\\cal D}h_2)\\in H\\right}$$ &emsp;&emsp;输入算子$\\cal B$和输出算子$\\cal C$：$${\\cal B}=\\begin{bmatrix}0\\{\\cal G}\\end{bmatrix},\\ {\\cal C}=\\begin{bmatrix}{\\cal H}_0 &amp; {\\cal H}_1\\end{bmatrix}$$ &emsp;&emsp;我们就得到了式(1)(2)形式的状态参数方程。现在我们还需要定义状态变量的内积，使得状态空间是个Hilbert空间（有内积的完备空间）。若选择标准内积：$$\\begin{split}\\left\\langle\\begin{bmatrix}x_1\\x_2\\end{bmatrix}\\begin{bmatrix}y_1\\y_2\\end{bmatrix}\\right\\rangle=&amp;\\langle x_1,y_1\\rangle_H+\\langle x_2,y_2\\rangle_H,\\&amp;x_1,y_1\\in H,\\ x_2,y_2\\in H\\end{split}\\tag{14}$$ &emsp;&emsp;则状态变量$x(t)$的范数平方为：$$\\begin{split}||x(t)||^2&amp;=\\langle x_1(t),x_1(t)\\rangle_H+\\langle x_2(t),x_2(t)\\rangle_H\\&amp;=\\int_0^1(w(z,t))^2{\\rm d}z+\\int_0^1(\\partial_tw(z,t))^2{\\rm d}z\\end{split}\\tag{15}$$ &emsp;&emsp;而根据物理公式，我们可以将弹性梁的动能及势能用$w(z,t)$表示出来：$$\\begin{eqnarray}E_{kin}(t)&amp;=&amp;{1\\over 2}\\int_0^1(\\partial_tw(z,t))^2{\\rm d}z\\tag{16}\\E_{pot}(t)&amp;=&amp;{1\\over 2}\\int_0^1(\\partial_z^2w(z,t))^2{\\rm d}z\\tag{17}\\end{eqnarray}$$ &emsp;&emsp;比较式(15)~(17)，我们发现梁的总能量无法用状态变量的范数平方$||x(t)||^2$来表示。因此我们重新定义内积：$$\\begin{split}\\left\\langle\\begin{bmatrix}x_1\\x_2\\end{bmatrix}\\begin{bmatrix}y_1\\y_2\\end{bmatrix}\\right\\rangle=&amp;\\langle {\\cal S}^{1 \\over 2}x_1, {\\cal S}^{1 \\over 2}y_1\\rangle_H+\\langle x_2,y_2\\rangle_H,\\&amp;x_1,y_1\\in D({\\cal S}^{1 \\over 2}),\\ x_2,y_2\\in H\\end{split}\\tag{18}$$ &emsp;&emsp;则状态变量$x(t)$的范数平方为：$$\\begin{split}||x(t)||^2&amp;=\\langle{\\cal S}^{1\\over 2}x_1(t),{\\cal S}^{1\\over 2}x_1(t)\\rangle_H+\\langle x_2(t),x_2(t)\\rangle_H\\&amp;=\\int_0^1(\\partial_z^2w(z,t))^2{\\rm d}z+\\int_0^1(\\partial_tw(z,t))^2{\\rm d}z\\&amp;=2E_{pot}(t)+2E_{kin}(t)\\end{split}\\tag{19}$$ &emsp;&emsp;式(19)可知，梁的总能量能使用式(18)内积得到的范数平方能表示，因此式(18)又被称为能量内积（Energie-Skalarprodukt）。为了判定系统算子$\\cal A$对应的$C_0$半群存在，我们使用Lumer-Phillips定理。由于系统算子$\\cal A$是二阶的，难以直接操作，因此下面分别从条件二与条件三入手进行推导（条件一是普遍满足的）。 条件二：$\\cal A$的消散性&emsp;&emsp;消散性是指系统能量只会降低不会升高。式(19)中范数平方可以表示弹性梁系统的总能量，因此设能量函数$V(x)$为：$$V(x)=||x(t)||^2\\tag{20}$$ &emsp;&emsp;将能量函数对$t$求导，得到：$$\\begin{split}\\dot{V}(x)&amp;=\\langle\\dot{x},x\\rangle+\\langle x,\\dot{x}\\rangle\\&amp;=\\langle\\dot{x},x\\rangle+\\overline{\\langle\\dot{x},x\\rangle}\\&amp;=2{\\rm Re}\\langle\\dot{x},x\\rangle\\&amp;=2{\\rm Re}\\langle{\\cal A}x,x\\rangle\\end{split}\\tag{21}$$ &emsp;&emsp;由于系统要求是消散的，因此能量函数的导数要小于等于$0$，我们得到：$$\\dot{V}(x)=2{\\rm Re}\\langle{\\cal A}x,x\\rangle\\leq 0\\tag{22}$$ &emsp;&emsp;此时系统Lyapunov稳定：$$\\begin{split}\\dot{V}(x)&amp;={\\rm d}_t||x(t)||^2=2||x(t)||{\\rm d}_t||x(t)||\\leq 0\\&amp;\\Rightarrow {\\rm d}_t||x(t)||\\leq 0\\&amp;\\Rightarrow ||x(t)||\\leq ||x(0)||\\end{split}$$ &emsp;&emsp;得到系统的解是收缩的（kontraktiv），因此Lumer-Phillips定理最终得到的$C_0$半群也叫收缩半群。综上系统算子$\\cal A$消散的条件为：&emsp;&emsp;$\\circ$ ${\\cal S}^{1\\over 2}$存在且唯一&emsp;&emsp;$\\circ$ ${\\rm Re}\\langle{\\cal A}x,x\\rangle\\leq 0$ &emsp;&emsp;对于第一个条件，我们可以用如下定理得到：$$\\cal S为正算子\\Leftrightarrow{\\cal S}={\\cal S}^{1\\over 2}\\cdot{\\cal S}^{1\\over 2}$$ &emsp;&emsp;其中${\\cal S}^{1\\over 2}$为根算子（Wurzeloperator），它存在唯一，并且也是正算子。其中正算子定义为：若$\\cal S$为自伴随算子，且满足：$$\\langle{\\cal S}h,h\\rangle_H&gt;0$$ &emsp;&emsp;则称$\\cal S$为正算子。因此第一个条件可以转化为证明$\\cal S$为正算子。&emsp;&emsp;对于第二个条件，我们将$\\cal A$的定义带入得到：$$\\begin{split}{\\rm Re}\\langle{\\cal A}h,h\\rangle&amp;={\\rm Re}\\left\\langle\\begin{bmatrix}0 &amp; I \\ -{\\cal S} &amp; -{\\cal D}\\end{bmatrix}\\begin{bmatrix}h_1\\h_2\\end{bmatrix},\\begin{bmatrix}h_1\\h_2\\end{bmatrix}\\right\\rangle\\&amp;={\\rm Re}\\langle{\\cal S}^{1\\over 2}h_2,{\\cal S}^{1\\over 2}h_1\\rangle_H+{\\rm Re}\\langle -{\\cal S}h_1-{\\cal D}h_2,h_2\\rangle_H\\&amp;={\\rm Re}\\langle{\\cal S}^{1\\over 2}h_2,{\\cal S}^{1\\over 2}h_1\\rangle_H-{\\rm Re}\\langle {\\cal S}h_1,h_2\\rangle_H-{\\rm Re}\\langle {\\cal D}h_2,h_2\\rangle_H\\&amp;={\\rm Re}\\langle{\\cal S}^{1\\over 2}h_2,{\\cal S}^{1\\over 2}h_1\\rangle_H-{\\rm Re}\\langle{\\cal S}^{1\\over 2}h_1,{\\cal S}^{1\\over 2}h_2\\rangle_H-{\\rm Re}\\langle {\\cal D}h_2,h_2\\rangle_H\\&amp;=-{\\rm Re}\\langle {\\cal D}h_2,h_2\\rangle_H\\leq 0\\end{split}$$ &emsp;&emsp;其中用到了$\\cal S$的正算子性质：$$\\begin{split}{\\rm Re}\\langle {\\cal S}h_1,h_2\\rangle_H&amp;={\\rm Re}\\langle{\\cal S}^{1\\over 2}{\\cal S}^{1\\over 2}h_1,h_2\\rangle_H\\&amp;={\\rm Re}\\langle{\\cal S}^{1\\over 2}h_1,{\\cal S}^{1\\over 2}h_2\\rangle_H\\end{split}$$ &emsp;&emsp;以及内积的性质：$$\\begin{split}{\\rm Re}\\langle{\\cal S}^{1\\over 2}h_1,{\\cal S}^{1\\over 2}h_2\\rangle_H&amp;={\\rm Re}\\overline{\\langle{\\cal S}^{1\\over 2}h_2,{\\cal S}^{1\\over 2}h_1\\rangle}_H\\&amp;={\\rm Re}\\langle{\\cal S}^{1\\over 2}h_2,{\\cal S}^{1\\over 2}h_1\\rangle_H\\end{split}$$ &emsp;&emsp;第二个条件最后转化为了：$${\\rm Re}\\langle {\\cal D}h_2,h_2\\rangle_H\\geq 0\\tag{23}$$ &emsp;&emsp;因此$\\cal A$的消散性最终得到了以下两个条件：&emsp;&emsp;$\\circ$ $\\cal S$为正算子&emsp;&emsp;$\\circ$ ${\\rm Re}\\langle {\\cal D}h_2,h_2\\rangle_H\\geq 0$ 条件三：$\\lambda I-{\\cal A}\\to X$&emsp;&emsp;若系统算子$\\cal A$存在逆算子${\\cal A}^{-1}$，则条件“$\\lambda I-{\\cal A}\\to X$”等价于“${\\cal A}^{-1}$为有界线性算子”。若不存在逆算子，则未解。因此我们可以将条件三转化${\\cal A}^{-1}$存在且为有界线性算子，带入$\\cal A$定义来求解${\\cal A}^{-1}$：$$\\begin{split}{\\cal A}x=y&amp;\\Rightarrow\\begin{bmatrix}0 &amp; I \\ -{\\cal S} &amp; -{\\cal D}\\end{bmatrix}\\begin{bmatrix}x_1\\x_2\\end{bmatrix}=\\begin{bmatrix}y_1\\y_2\\end{bmatrix}\\end{split}$$ &emsp;&emsp;将矩阵展开得到：$$\\begin{split}&amp;\\begin{cases}x_2=y_1\\-{\\cal S}x_1-{\\cal D}x_2=y_2\\end{cases}\\\\Rightarrow &amp;\\begin{cases}x_2=y_1\\-{\\cal S}x_1={\\cal D}x_2+y_2={\\cal D}y_1+y_2\\end{cases}\\\\Rightarrow &amp;\\begin{cases}x_2=y_1\\x_1=-{\\cal S}^{-1}{\\cal D}y_1-{\\cal S}^{-1}y_2\\end{cases}\\end{split}$$ &emsp;&emsp;化为矩阵形式得到：$$\\begin{bmatrix}-{\\cal S}^{-1}{\\cal D} &amp; -{\\cal S}^{-1} \\ I &amp; 0\\end{bmatrix}\\begin{bmatrix}y_1\\y_2\\end{bmatrix}=\\begin{bmatrix}x_1\\x_2\\end{bmatrix}\\Rightarrow{\\cal A}^{-1}y=x$$ &emsp;&emsp;则可以得到系统算子$\\cal A$的逆算子${\\cal A}^{-1}$表示为：$${\\cal A}^{-1}=\\begin{bmatrix}-{\\cal S}^{-1}{\\cal D} &amp; -{\\cal S}^{-1} \\ I &amp; 0\\end{bmatrix}\\tag{24}$$ &emsp;&emsp;根据式(24)，${\\cal A}^{-1}$存在要求${\\cal S}^{-1}$存在；${\\cal A}^{-1}$为有界线性算子则要求${\\cal S}^{-1}{\\cal D}$与${\\cal S}^{-1}$均为有界线性算子。因此条件$\\lambda I-{\\cal A}\\to X$最终也转化为了两个条件：&emsp;&emsp;$\\circ$ ${\\cal S}^{-1}$存在且为有界线性算子&emsp;&emsp;$\\circ$ ${\\cal S}^{-1}{\\cal D}$为有界线性算子 总结&emsp;&emsp;通过上面的推导，我们将Lumer-Phillips定理转化为了以下四个条件：&emsp;&emsp;$\\circ$ $\\cal S$为正算子&emsp;&emsp;$\\circ$ ${\\rm Re}\\langle {\\cal D}h_2,h_2\\rangle_H\\geq 0$&emsp;&emsp;$\\circ$ ${\\cal S}^{-1}$存在且为有界线性算子&emsp;&emsp;$\\circ$ ${\\cal S}^{-1}{\\cal D}$为有界线性算子 &emsp;&emsp;在这里我们引入正定算子的概念：若算子$\\cal S$为自伴随算子，且满足：$$\\langle{\\cal S}h,h\\rangle_H\\geq \\epsilon||h||^2_H,\\ \\epsilon&gt;0,\\ \\forall h\\in D({\\cal S})\\tag{25}$$ &emsp;&emsp;则称$\\cal S$为正定算子。&emsp;&emsp;若$\\cal S$为正定算子，根据定义，显然$\\cal S$是正算子；同时可以证明$\\cal S^{-1}$为有界线性算子，其过程如下：$$\\begin{split}&amp;取h={\\cal S}^{-1}h\\\\Rightarrow &amp;\\langle{\\cal S}{\\cal S}^{-1}h,{\\cal S}^{-1}h\\rangle_H\\geq\\epsilon||{\\cal S}^{-1}h||^2_H\\\\Rightarrow &amp;||{\\cal S}^{-1}h||^2_H\\leq{1\\over\\epsilon}\\langle{\\cal S}{\\cal S}^{-1}h,{\\cal S}^{-1}h\\rangle_H\\leq{1\\over\\epsilon}\\left(\\langle h,h\\rangle\\langle{\\cal S}^{-1}h,{\\cal S}^{-1}h\\rangle\\right)^{1\\over 2}={1\\over\\epsilon}||h||\\cdot||{\\cal S}^{-1}h||\\\\Rightarrow &amp;||{\\cal S}^{-1}h||_ H\\leq{1\\over\\epsilon}||h||\\\\Rightarrow &amp;{\\cal S}^{-1}有界\\end{split}$$ &emsp;&emsp;上述证明过程中用到了柯西-施瓦茨不等式（Cauchy-Schwarz inequality）：$$|\\langle x,y\\rangle|^2\\leq \\langle x,x\\rangle\\cdot\\langle y,y\\rangle$$ &emsp;&emsp;因此可以将条件“$\\cal S$为正算子”与“${\\cal S}^{-1}$存在且为有界线性算子”合并为一个条件“$\\cal S$为正定算子”。最终我们需要证明的条件为：&emsp;&emsp;1. $\\cal S$为正定算子&emsp;&emsp;2. ${\\rm Re}\\langle {\\cal D}h_2,h_2\\rangle_H\\geq 0$&emsp;&emsp;3. ${\\cal S}^{-1}{\\cal D}$为有界线性算子 &emsp;&emsp;在实际计算中，我们可以先通过定义得到条件1，并判断条件2。若都成立，则可以说明$\\cal A$是消散的。再根据式(22)的推导过程得到$\\cal A^{-1}$，并判断条件3。若成立，则可以说明${\\cal A}^{-1}$存在且为有界线性算子，并且$\\lambda I-{\\cal A}\\to X$成立。最后综合上述结论，根据Lumer-Phillips定理判断系统的适定性。由于${\\cal D}$通常为$0$或${\\cal S}^{1\\over 2}$，因此条件2、3通常满足，因此实际需要判断的也就是条件1：$\\cal S$为正定算子。 小结 $C_0$半群与无穷小母元一一对应，因此可以通过系统算子对应的$C_0$半群是否存在来判断系统初值问题是否为适定性问题。 判断$C_0$半群存在通常有三种方法：Riesz-Spektral系统判别、Hille-Yoshida定理和Lumer-Phillips定理。 二阶分布参数系统适定性问题举例。","categories":[{"name":"控制理论","slug":"控制理论","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/"},{"name":"分布参数系统控制","slug":"控制理论/分布参数系统控制","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F%E6%8E%A7%E5%88%B6/"}],"tags":[{"name":"分布参数系统","slug":"分布参数系统","permalink":"https://haosutopia.github.io/tags/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F/"}]},{"title":"支持向量机（四）","slug":"SVM-04","date":"2018-03-16T20:51:52.000Z","updated":"2019-03-18T21:49:58.000Z","comments":true,"path":"2018/03/SVM-04/","link":"","permalink":"https://haosutopia.github.io/2018/03/SVM-04/","excerpt":"","text":"&emsp;&emsp;到目前为止，我们把SVM优化问题化为了更容易解决的对偶问题，并且使用核函数及正则化来改善它的特性。现在我们只需解决这个对偶问题就可以得到最优间隔分类器，可以说是“万事具备，只欠东风”了。我们先将第三章节得到的对偶问题搬运过来：$$\\begin{eqnarray}\\max_{\\alpha}\\ &amp;&amp;\\theta_{\\cal{D}}(\\alpha)=\\sum_{i=1}^m\\alpha_i-{1 \\over 2}\\sum_{i,j=1}^my^{(i)}y^{(j)}\\alpha_i\\alpha_j\\langle x^{(i)},x^{(j)}\\rangle\\tag{1}\\{\\rm s.t.}\\ &amp;&amp;0\\leq\\alpha_i\\leq C,\\ i=1,…,m\\tag{2}\\&amp;&amp;\\sum_{i=1}^m\\alpha_iy^{(i)}=0\\tag{3}\\end{eqnarray}$$ &emsp;&emsp;在机器学习中，求函数极值点常用的方法除了常规的求导，就是梯度下降法了。但式(1)(2)(3)所示的对偶问题对所有参数均有约束。若使用梯度下降法，在每次更新权重时还需要保持所有参数都满足约束，这是难以做到的。1998年，John C.Platt的论文Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines针对上述问题提出了SMO（sequential minimal optimization）算法，它能够很好地解决并快速求解出SVM的最优解。 坐标下降法&emsp;&emsp;为了引入SMO算法，我们来看一个相似的算法——坐标下降法（coordinate ascent）。考虑如下无约束优化问题：$$\\max_{\\alpha}W(\\alpha_1,\\alpha_2,…,\\alpha_m)$$ &emsp;&emsp;其中$W$是以$\\alpha_1,…,\\alpha_m$为参数的函数。坐标下降法求取极值的过程如下：&emsp;&emsp;&emsp;&emsp;循环直到收敛：${$&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;${\\rm For}\\ i=1,…,m{$&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$\\alpha_i:=\\arg{\\rm max}{\\hat{\\alpha_i}}W(\\alpha_1,…,\\alpha{i-1},\\hat{\\alpha_i},\\alpha_{i+1},…,\\alpha_m)$&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$}$&emsp;&emsp;&emsp;&emsp;$}$ &emsp;&emsp;其中$\\arg\\max$表示使式子达到最大值时变量的取值。在上述算法中，每次迭代我们只更新其中一个参数而保持其他参数不变，并且参数更新的顺序是它们下标的顺序，这就是坐标下降法。当然我们也可以按照一定规则来选择更新的参数，比方说每次更新选择梯度最大的参数。下图为一个简单的例子：&emsp;&emsp;图中的椭圆为优化函数的等高线。参数初始值为$(2,-2)$。我们每次只更新其中一个参数，在经过数次更新后，函数值趋向于全局最大值。 SMO算法&emsp;&emsp;现在让我们来看式(1)(2)(3)所表示的对偶问题。若我们使用坐标下降法，保持$\\alpha_2,…,\\alpha_m$不变并且更新$\\alpha_1$，就会出现问题：由于式(3)的约束，因此有：$$\\alpha_1y^{(1)}=-\\sum_{i=2}^m\\alpha_iy^{(i)}\\tag{4}$$ &emsp;&emsp;等式两边同时乘以$y^{(1)}$得到：$$\\alpha_1=-y^{(1)}\\sum_{i=2}^m\\alpha_iy^{(i)}\\tag{5}$$ &emsp;&emsp;我们发现$\\alpha_1$已经被约束条件限制住了。如果$\\alpha_2,…,\\alpha_m$不变，那么$\\alpha_1$就不能改变。因此只更新一个参数是行不通的，我们必须同时更新两个以上的参数。在此引出SMO算法： &emsp;&emsp;&emsp;&emsp;循环直到收敛：${$&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;1. 选取参数对$\\alpha_i$与$\\alpha_j$（使用启发式方法选取参数对）&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;2. 更新$\\alpha_i$与$\\alpha_j$，并保持其他参数不变&emsp;&emsp;&emsp;&emsp;$}$ 更新参数对&emsp;&emsp;更新参数时要注意，更新后的参数必须满足KKT条件。下面以更新$\\alpha_1,\\alpha_2$为例来说明SMO算法的思想。根据式(3)，我们得到一个$\\alpha_1$与$\\alpha_2$所满足的方程：$$\\alpha_1y^{(1)}+\\alpha_2y^{(2)}=-\\sum_{i=3}^m\\alpha_iy^{(i)}=\\zeta\\tag{6}$$ &emsp;&emsp;由于$\\alpha_3,…,\\alpha_m$固定，因此$\\zeta$为一个常值。根据式(6)，$\\alpha_1$可以表示为$\\alpha_2$的函数：$$\\alpha_1=(\\zeta-\\alpha_2y^{(2)})y^{(1)}\\tag{7}$$ &emsp;&emsp;函数在$\\alpha_1,\\alpha_2$坐标系下表示为一条直线，并且它根据$y^{(1)},y^{(2)}$是否相等分为以下两种情况：&emsp;&emsp;根据式(2)，我们知道$\\alpha_1,\\alpha_2$的取值只能在上图中$[0,C]\\times [0,C]$的方框中，而它们又满足方程$\\alpha_1y^{(1)}+\\alpha_2^{(2)}y^{(2)}=\\zeta$，因此根据图像可以得到$\\alpha_2$的取值范围，记为$[L,H]$。&emsp;&emsp;准备工作做完后，我们开始优化式(1)。将式(7)带入式(1)中得到：$$\\theta_{\\cal{D}}(\\alpha_1,\\alpha_2,…,\\alpha_m)=\\theta_{\\cal{D}}((\\zeta-\\alpha_2y^{(2)})y^{(1)},\\alpha_2,…,\\alpha_m)\\tag{8}$$ &emsp;&emsp;式(8)将优化目标变为了$\\alpha_2$的函数。若观察式(1)我们发现式中只有$\\alpha_i$与$\\alpha_i\\alpha_j$项，因此式(8)可以表示为$a\\alpha_2^2+b\\alpha_2+c$的形式，是一个抛物线。这时问题就转化为了在$[L,H]$区间中求抛物线的极值。我们令$\\alpha_2^{temp}$为抛物线的最值点（此时$\\alpha_2^{temp}$不一定在$[L,H]$内），则$[L,H]$区间中抛物线的极值点$\\alpha_2^{new}$为：$$\\alpha_2^{new}=\\begin{cases}H\\ &amp;,\\alpha_2^{temp}&gt;H\\\\alpha_2^{temp}\\ &amp;,L\\leq\\alpha_2^{temp}\\leq H\\L\\ &amp;,\\alpha_2^{temp}&lt;L\\end{cases}$$ &emsp;&emsp;根据式(7)求出$\\alpha_1^{new}$：$$\\alpha_1^{new}=(\\zeta-\\alpha_2^{new}y^{(2)})y^{(1)}$$ 更新偏差值$b$&emsp;&emsp;更新$\\alpha$值后。我们需要调整$b$的值使得解满足KKT条件。这里将第三章节最后的三种情况重新罗列在下面：$$\\begin{eqnarray}\\alpha_i=0&amp;\\Leftrightarrow&amp; y^{(i)}(w^Tx^{(i)}+b)\\geq 1\\tag{9}\\\\alpha_i=C&amp;\\Leftrightarrow&amp; y^{(i)}(w^Tx^{(i)}+b)\\leq 1\\tag{10}\\0&lt;\\alpha_i&lt;C&amp;\\Leftrightarrow&amp; y^{(i)}(w^Tx^{(i)}+b)= 1\\tag{11}\\\\end{eqnarray}$$ &emsp;&emsp;若$0&lt;\\alpha_1^{new}&lt;C$，通过式(11)可求得满足KKT条件的$b_1$（此时$\\alpha_1,\\alpha_2$值已更新）：$$b_1=y^{(1)}-\\sum_{i=1}^m\\alpha_iy^{(i)}\\langle x^{(i)},x^{(1)}\\rangle\\tag{12}$$ &emsp;&emsp;若$0&lt;\\alpha_2^{new}&lt;C$，同样可求得$b_2$：$$b_2=y^{(2)}-\\sum_{i=1}^m\\alpha_iy^{(i)}\\langle x^{(i)},x^{(2)}\\rangle\\tag{13}$$ &emsp;&emsp;若$0&lt;\\alpha_1^{new}&lt;C$与$0&lt;\\alpha_2^{new}&lt;C$同时满足，则$\\alpha_2^{new}$未经过区间$[L,H]$的裁剪，为式(8)的最值点，满足：$${\\partial\\theta_{\\cal{D}} \\over \\partial\\alpha_2}={\\partial\\alpha_1 \\over \\partial\\alpha_2}+1-\\sum_{i=1}^my^{(1)}y^{(i)}{\\partial\\alpha_1 \\over \\partial\\alpha_2}\\alpha_i\\langle x^{(1)},x^{(i)}\\rangle-\\sum_{i=1}^my^{(2)}y^{(i)}\\alpha_i\\langle x^{(2)},x^{(i)}\\rangle=0$$ &emsp;&emsp;其中偏微分项由式(7)得到：$${\\partial\\alpha_1 \\over \\partial\\alpha_2}=-y^{(1)}y^{(2)}$$ &emsp;&emsp;带回原式：$$-y^{(1)}y^{(2)}+1+\\sum_{i=1}^my^{(2)}y^{(i)}\\alpha_i\\langle x^{(1)},x^{(i)}\\rangle-\\sum_{i=1}^my^{(2)}y^{(i)}\\alpha_i\\langle x^{(2)},x^{(i)}\\rangle=0$$ &emsp;&emsp;等式两边同时乘以$y^{(2)}$，整理后得到：$$y^{(1)}-\\sum_{i=1}^m\\alpha_iy^{(i)}\\langle x^{(i)},x^{(1)}\\rangle=y^{(2)}-\\sum_{i=1}^m\\alpha_iy^{(i)}\\langle x^{(i)},x^{(2)}\\rangle$$ &emsp;&emsp;带入式(12)(13)，我们发现，若$0&lt;\\alpha_1^{new}&lt;C$与$0&lt;\\alpha_2^{new}&lt;C$同时满足，则必有： $$b_1=b_2\\tag{14}$$ &emsp;&emsp;若$0&lt;\\alpha_1^{new}&lt;C$与$0&lt;\\alpha_2^{new}&lt;C$同时不满足，则$\\alpha_1^{new},\\alpha_2^{new}$为$0$或$C$。根据式(9)(10)，得到$b$的取值范围$[b_1,b_2]$。$b^{new}$可以取其中任意值，通常取：$$b^{new}={b_1+b_2 \\over 2}\\tag{15}$$ &emsp;&emsp;综合式(12)~(15)四种情况，我们得到$b^{new}$的取值为：$$b^{new}=\\begin{cases}b_1,&amp;0&lt;\\alpha_1^{new}&lt;C\\b_2,&amp;0&lt;\\alpha_2^{new}&lt;C\\(b_1+b_2)/2&amp;其他\\end{cases}$$ &emsp;&emsp;更新完$b$后，就算完成了一次迭代。重复这个过程，一直循环到收敛，就得到对偶问题的最优解了。 选择参数对&emsp;&emsp;在算法刚开始时，$\\alpha$值初始化为$[0,C]$区间中的随机数，因此必然有很多$\\alpha$值不满足KKT条件。而我们算法的目标是使得所有$\\alpha$满足KKT条件，因此在选择第一个参数$\\alpha_i$时，算法会遍历整个训练集，找到不满足KKT条件的训练样本，并将它设为需要更新的参数。&emsp;&emsp;得到第一个参数$\\alpha_i$后，算法会选择第二个参数$\\alpha_j$来最大化优化步长。因此算法会遍历所有非边界样本（即满足$0&lt;\\alpha_i&lt;C$的样本），找到第二个参数$\\alpha_j$使得$|E_i-E_j|$最大。其中$E$为训练样本预测值与实际值的偏差：$$E_i=w^Tx^{(i)}+b-y^{(i)}$$ &emsp;&emsp;若$E_i$是正偏差，则在所有负偏差中找到最小的$E_j$；若$E_i$是负偏差，则在所有正偏差中找到最大的$E_j$。这样就选择出了需要更新的参数对$\\alpha_i,\\alpha_j$。 小结 坐标下降法每次只更新一个参数而固定其他所有参数。 SMO算法每次更新一个参数对，同时更新$b$使它们满足KKT条件。 使用启发式方法选择每次更新的参数对。 &emsp;&emsp;到这里SVM就算告一段落了，这是我大二时候学习的内容，如今再看一遍还是有很多收获的，之前一些模糊的概念也都算理清楚了，完结撒花！","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://haosutopia.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"https://haosutopia.github.io/tags/SVM/"}]},{"title":"支持向量机（三）","slug":"SVM-03","date":"2018-03-15T20:42:35.000Z","updated":"2018-03-23T21:15:50.000Z","comments":true,"path":"2018/03/SVM-03/","link":"","permalink":"https://haosutopia.github.io/2018/03/SVM-03/","excerpt":"","text":"&emsp;&emsp;支持向量机的强大不止在于它能在特征空间找到合适的超平面将正负样本分开并保持最大间隔，通过引入核函数，它还能在更高维，甚至无限维空间中对样本进行分离，以此来解决非线性的问题。 核特征映射&emsp;&emsp;在之前的推导中，我们使用的一直是样本的原始特征$x$，并通过线性平面$w^Tx+b$进行分割。但如果样本是线性不可分的，我们就需要先将特征映射到更高维空间，再分离正负样本。考虑如下训练集：&emsp;&emsp;这是一个拥有两个特征$(x_1,x_2)$的训练集，它的分布是非线性的，因此我们无法使用直线将正负样本分开。但观察可知，若决策边界为圆形，就能完全分离正负样本。因此可以令决策边界为一个二次曲线，其一般形式如下：$$w_1x_1+w_2x_2+w_3x_1^2+w_4x_2^2+w_5x_1x_2+b=0\\tag{1}$$ &emsp;&emsp;如果我们把$x_1,x_2,x_1^2,x_2^2,x_1x_2$都作为特征，那么就相当于将一个二维的特征空间映射到了一个五维的特征空间。在这个五维空间我们就能够使用线性分割将样本分离。这种映射我们称为特征映射（feature mapping），记为$\\phi$。式(1)中的特征映射为：$$\\phi(x)=\\begin{bmatrix}x_1\\x_2\\x_1^2\\x_2^2\\x_1x_2\\end{bmatrix}$$ &emsp;&emsp;使用特征映射代替原来的特征就能够实现样本的高维分离。在第二章节最后我们得到预测函数只与样本特征的内积$\\langle x_i,x_j\\rangle$有关，因此我们将其中的特征$x$替换为特征映射$\\phi$，并称它为$\\phi$对应的核（Kernel）：$$K(x,z)=\\phi(x)^T\\phi(z)\\tag{2}$$ &emsp;&emsp;在使用中我们只需要将之前所有的$\\langle x,z\\rangle$替换成$K(x,z)$就可以了。 核函数&emsp;&emsp;上面我们使用特征映射得到了核函数，但实际计算时，我们通常不计算特征映射，而是直接计算核函数。例如：$$K(x,z)=(x^Tz)^2$$ &emsp;&emsp;将它化为式(2)的形式：$$\\begin{split}K(x,z)&amp;=\\left(\\sum_{i=1}^nx_iz_i\\right)\\left(\\sum_{i=1}^nx_iz_i \\right)\\&amp;=\\sum_{i=1}^n\\sum_{j=1}^nx_ix_jz_iz_j\\&amp;=\\sum_{i,j=1}^n(x_ix_j)(z_i,z_j)\\end{split}$$ &emsp;&emsp;得到该核函数对应的特征映射（取$n=2$）：$$\\phi(x)=\\begin{bmatrix}x_1x_1\\x_1x_2\\x_2x_1\\x_2x_2\\end{bmatrix}$$ &emsp;&emsp;可以发现，若通过特征映射计算，我们需要$n^2$数量级的计算次数。而若直接通过核函数公式，我们只需$n$数量级的计算次数，大大降低了高维空间的计算成本。 常用核函数&emsp;&emsp;下面介绍几个常用核函数：&emsp;&emsp;1. 线性核：$K(x,z)=\\langle x,z\\rangle$。这个实际上就是原特征空间的内积。只是在一些SVM程序中需要我们定义核函数。将内积写成核函数形式就可以将线性与非线性SVM统一起来了。&emsp;&emsp;2. 多项式核：$K(x,z)=(\\langle x,z\\rangle+R)^d$。上面举例的核函数就是一个多项式核。若原特征空间维度为$n$，则多项式核对应特征映射的维度是$C_{n+d}^d$，并且可以通过计算将特征映射写出来。&emsp;&emsp;3. 高斯核：$$K(x,z)=\\exp\\left(-{||x-z||^2 \\over 2\\sigma^2}\\right)\\tag{3}$$ &emsp;&emsp;&emsp;高斯核可以说是最常用的核函数了。它的表达式中存在指数函数，泰勒展开后得到：$$K(x,z)=1+\\left(-{||x-z||^2 \\over 2\\sigma^2}\\right)+{\\left(-{||x-z||^2 \\over 2\\sigma^2}\\right)^2 \\over 2!}+…+{\\left(-{||x-z||^2 \\over 2\\sigma^2}\\right)^n \\over n!}+…$$ &emsp;&emsp;&emsp;由于泰勒展开得到的项数是无穷的，于是我们得到了一个无穷维的映射，因此高斯核对应的特征映射维度是无穷。需要注意的是，高斯核有个参数$\\sigma$，在使用时需要合理选择。若$\\sigma$过小，样本间稍有不同就会得到一个很大的数值，这就会导致过拟合；若$\\sigma$过大，高斯核就起不了应有的效果。 Mercer定理&emsp;&emsp;如果有一个核函数，我们难以写出它对应的特征映射，就很难判断它到底是不是一个有效的核函数。这时Mercer定理提供了一个有效核函数的充要条件。&emsp;&emsp;Mercer定理：若有一个函数$K:\\Bbb{R}^n\\times\\Bbb{R}^n\\to\\Bbb{R}$，它是一个有效核函数的充要条件为：对任意数据集${x^{(1)},…,x^{(m)}},(m&lt;\\infty)$，它所对应的核函数矩阵$K$是一个对称且半正定的矩阵。&emsp;&emsp;其中核函数矩阵$K$是一个$m\\times m$的矩阵，它的元素$K_{ij}=K(x^{(i)},x^{(j)})$。在这里核函数与核函数矩阵使用了相同的符号$K$。&emsp;&emsp;在此我们省略它的证明。实际上使用SVM时绝大部分用的都是高斯核，毕竟它映射到的是无限维，理论上可以完美分离任意训练集。而其他核几乎是用不到的。 SVM正则化&emsp;&emsp;在第二章节我们了解到，SVM优化问题的决策边界是由整个训练集中少数几个支持向量决定的。若训练集中存在极端值（outliers），又碰巧是支持向量的话，就会对决策边界的确定产生很大影响。考虑以下情况：&emsp;&emsp;左图是没有极端值的情况，最优间隔分类器能够很好地正负样本分开。但若加入一个极端值，如右图，它会使决策边界发生很大的变化来适应这个极端值，但这显然不是我们想要的。考虑到这种情况，我们将第一章节最后推导出的优化问题做些许改变，也就是正则化。我们将允许训练样本的函数间隔小于$1$，并且引入松弛变量（slack variable）来表示与$1$的偏差量。因此优化问题的约束条件变为：$$y^{(i)}(w^Tx^{(i)}+b)\\geq 1-\\xi_i,\\ i=1,…,m\\tag{4}$$ &emsp;&emsp;但我们也不能让$\\xi_i$任意大，因此在优化目标中增加一项作为偏差$\\xi_i$的惩罚：$$\\min_{\\gamma,w,b}\\ {1 \\over 2}||w||^2+C\\sum_{i=1}^m\\xi_i\\tag{5}$$ &emsp;&emsp;我们既希望训练样本间隔最大，又希望其偏差量最小，故使用参数$C$来保持两者之间的平衡，它是需要我们人为确定的。完整的优化问题描述如下：$$\\begin{split}\\min_{\\gamma,w,b}\\ &amp;{1 \\over 2}||w||^2+C\\sum_{i=1}^m\\xi_i\\{\\rm s.t.}\\ &amp;y^{(i)}(w^Tx^{(i)}+b)\\geq1-\\xi_i,\\ i=1,…,m\\&amp;\\xi_i\\geq 0,\\ i=1,…,m\\end{split}\\tag{6}$$ &emsp;&emsp;写出式(6)优化问题的拉格朗日方程$\\mathcal{L}(w,b,\\xi,\\alpha,r)$：$$\\mathcal{L}={1 \\over 2}||w||^2+C\\sum_{i=1}^m\\xi_i-\\sum_{i=1}^m\\alpha_i[y^{(i)}(w^Tx^{(i)}+b)-1+\\xi_i]-\\sum_{i=1}^mr_i\\xi_i\\tag{7}$$ &emsp;&emsp;与之前一样，通过求取偏导来最小化$\\mathcal{L}(w,b,\\xi,\\alpha,r)$：$$\\begin{eqnarray}\\nabla_w\\mathcal{L}&amp;=&amp;w-\\sum_{i=1}^m\\alpha_iy^{(i)}x^{(i)}=0\\tag{8}\\{\\partial \\over \\partial b}\\mathcal{L}&amp;=&amp;\\sum_{i=1}^m\\alpha_iy^{(i)}=0\\tag{9}\\{\\partial \\over \\partial\\xi_i}\\mathcal{L}&amp;=&amp;C-\\alpha_i-r_i=0\\tag{10}\\end{eqnarray}$$ &emsp;&emsp;将式(8)(9)(10)带回式(7)，我们得到：$$\\mathcal{L}=\\sum_{i=1}^m\\alpha_i-{1 \\over 2}\\sum_{i,j=1}^my^{(i)}y^{(j)}\\alpha_i\\alpha_j\\langle x^{(i)},x^{(j)}\\rangle\\tag{11}$$ &emsp;&emsp;它与第二章节式(25)得到的结果一致。但由于我们有条件$C-\\alpha_i-r_i=0$，并且根据KKT条件，$r_i\\geq 0$，得出$\\alpha_i\\leq C$。这表明$\\alpha_i$不能无限大，以此限制了单个样本对整体的影响。因此整个对偶问题表示为：$$\\begin{split}\\max_{\\alpha}\\ &amp;\\theta_{\\cal{D}}(\\alpha)=\\sum_{i=1}^m\\alpha_i-{1 \\over 2}\\sum_{i,j=1}^my^{(i)}y^{(j)}\\alpha_i\\alpha_j\\langle x^{(i)},x^{(j)}\\rangle\\{\\rm s.t.}\\ &amp;0\\leq\\alpha_i\\leq C,\\ i=1,…,m\\&amp;\\sum_{i=1}^m\\alpha_iy^{(i)}=0\\end{split}\\tag{12}$$ &emsp;&emsp;正则化后的对偶问题（式(12)）与原对偶问题（第二章节式(26)）唯一区别就是$\\alpha_i$的取值范围变成了$0\\leq\\alpha_i\\leq C$。并且通过$\\alpha_i$的值我们可以大致判断训练样本$(x^{(i)},y^{(i)})$的状态：&emsp;&emsp;1. $\\alpha_i=0$，则$y^{(i)}(w^Tx^{(i)}+b)\\geq 1$，此时训练样本在边界或边界内部，可能为支持向量或普通点。&emsp;&emsp;2. $\\alpha_i=C$，则$y^{(i)}(w^Tx^{(i)}+b)\\leq 1$，同时$r_i=0$，$\\xi_i\\geq 0$，此时训练样本在边界或两个边界之间，为支持向量，并可能为极端点。&emsp;&emsp;3. $0&lt;\\alpha_i&lt; C$，则$y^{(i)}(w^Tx^{(i)}+b)=1$，此时训练样本在边界上，为支持向量。&emsp;&emsp;若我们能解决式(12)中的对偶问题，得到相应$\\alpha_i$，再将它们带回原式，就能得到最优解了。在之后的章节我们将使用SMO算法来解决这类对偶问题。 小结 使用核函数可以将原特征映射到更高维空间，从而得到非线性的决策边界。 SVM正则化能有效减少极端值对模型的影响。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://haosutopia.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"https://haosutopia.github.io/tags/SVM/"}]},{"title":"支持向量机（二）","slug":"SVM-02","date":"2018-03-14T20:37:57.000Z","updated":"2018-03-23T21:16:40.000Z","comments":true,"path":"2018/03/SVM-02/","link":"","permalink":"https://haosutopia.github.io/2018/03/SVM-02/","excerpt":"","text":"&emsp;&emsp;上一章节我们将SVM的目标转换为了一个带不等式约束条件的优化问题，现在我们的目标就是解决这个优化问题。在此我们先将SVM及最优间隔分类器放一放，来看看一般约束优化问题的求解方法。 约束优化问题等式约束优化问题&emsp;&emsp;等式优化问题一般描述如下：$$\\begin{split}\\min_x\\ &amp;f(x)\\{\\rm s.t.}\\ &amp;h_i(x)=0,\\ i=1,…,l\\end{split}\\tag{1}$$ &emsp;&emsp;它包含一个优化目标和一系列约束等式。我们可以构建一个拉格朗日函数（Lagrangian）：$$\\mathcal{L}(x,\\beta)=f(x)+\\sum_{i=1}^l\\beta_ih_i(x)\\tag{2}$$ &emsp;&emsp;其中$\\beta$为拉格朗日乘子（Lagrange multiplier）。$x$与$\\beta$均为$\\mathcal{L}$的参数，若我们让它们的偏导置$0$，就能得到$\\mathcal{L}$的极值点：$$\\begin{cases}{\\partial\\mathcal{L} \\over \\partial x_i}=0,\\ i=1,…,n\\{\\partial\\mathcal{L} \\over \\partial \\beta_j}=0,\\ j=1,…,l\\end{cases}\\tag{3}$$ &emsp;&emsp;这里我们发现式(3)第二项等价于式(1)中的约束条件。因此拉格朗日函数将原约束优化问题的优化目标及约束组合在了一起，转换为对$\\mathcal{L}(x,\\beta)$的无约束优化问题。这两个优化问题是等价的，通过式(3)便可求解原优化问题。这就是拉格朗日乘数法。 不等式约束优化问题&emsp;&emsp;不等式优化问题一般描述如下：$$\\begin{split}\\min_x\\ &amp;f(x)\\{\\rm s.t.}\\ &amp;g_i(x)\\leq0,\\ i=1,…,k\\&amp;h_i(x)=0,\\ i=1,….l\\end{split}\\tag{4}$$ &emsp;&emsp;它比等式约束优化增加了不等式的约束条件。我们希望它与等式约束优化一样，能将优化目标及约束合并为一个函数。因此我们引入广义拉格朗日函数（generalized Lagrangian）：$$\\mathcal{L}(x,\\alpha,\\beta)=f(x)+\\sum_{i=1}^k\\alpha_ig_i(x)+\\sum_{i=1}^l\\beta_ih_i(x)\\tag{5}$$ &emsp;&emsp;其中$\\alpha$与$\\beta$为拉格朗日乘子。由于$g_i(x)$为不等式，不能直接套用式(3)的形式求解。因此我们考虑如下方程：$$\\theta_{\\cal{P}}(x)=\\max_{\\alpha,\\beta:\\alpha_i\\geq0}\\mathcal{L}(x,\\alpha,\\beta)\\tag{6}$$ &emsp;&emsp;其中$\\cal{P}$代表原始（之后我们还会引入对偶）。可以发现，若$x$不满足约束条件，即$g_i(x)&gt;0$或$h_i(x)\\neq 0$，我们可以令$\\alpha \\to\\infty$或$\\beta\\to\\infty$将$\\theta_{\\cal{P}}(x)$最大化为正无穷；若$x$满足约束条件，则$\\alpha_ig_i(x)$与$\\beta_ih_i(x)$最大均为$0$。而$f(x)$与$\\alpha,\\beta$无关，满足：$$\\max_{\\alpha,\\beta:\\alpha_i\\geq0}f(x)=f(x)\\tag{7}$$ &emsp;&emsp;综上我们就得到：$$\\theta_{\\cal{P}}(x)=\\begin{cases}f(x)\\ &amp;若x满足约束\\\\infty\\ &amp;若x不满足约束\\end{cases}$$ &emsp;&emsp;若我们最小化$\\theta_{\\cal{P}}(x)$，得到的$x$必然满足约束条件（否则$\\theta_{\\cal{P}}(x)\\to\\infty$），并且能够最小化$f(x)$。以此就能将优化目标与约束合并成一个函数了，同时优化问题转换为：$$\\min_x\\theta_{\\cal{P}}(x)=\\min_x\\max_{\\alpha,\\beta:\\alpha_i\\geq0}\\mathcal{L}(x,\\alpha,\\beta)\\tag{8}$$ &emsp;&emsp;我们来仔细观察$\\alpha_ig_i(x)$项。若$\\alpha_i&lt;0$，那么当$x$满足约束条件$g_i(x)\\leq 0$时，$\\max\\alpha_ig_i(x)\\geq0$，而我们希望此时$\\max\\alpha_ig_i(x)=0$，因此式(6)中规定：$$\\alpha_i\\geq 0\\tag{9}$$ &emsp;&emsp;再来看，若$g_i(x)&lt;0$，那么$\\max\\alpha_ig_i(x)=0$时所取的$\\alpha_i=0$；若$g_i(x)=0$，不管$\\alpha_i$取什么值，$\\alpha_ig_i(x)$都为$0$。因此在$\\theta_{\\cal{P}}(x)$中$\\alpha_i$所满足的条件为：$$\\alpha_i\\begin{cases}=0,\\ &amp;g_i(x)&lt;0\\\\geq 0,\\ &amp;g_i(x)=0\\end{cases}$$ &emsp;&emsp;可以将上式合并为：$$\\alpha_ig_i(x)=0\\tag{10}$$ &emsp;&emsp;根据以上所述，我们就能够推导出不等式约束优化问题解所满足的KKT条件了。 KKT条件&emsp;&emsp;我们假设式(8)存在解$x^\\ast ,\\alpha^\\ast ,\\beta^\\ast $，则它们满足如下Karush-Kuhn-Tucker（KKT）条件：$$\\begin{eqnarray}{\\partial \\over \\partial x_i}\\mathcal{L}(x^\\ast ,\\alpha^\\ast ,\\beta^\\ast ) &amp;=&amp;0,\\ i=1,…,n\\tag{11}\\{\\partial \\over \\partial \\beta_i}\\mathcal{L}(x^\\ast ,\\alpha^\\ast ,\\beta^\\ast ) &amp;=&amp;0,\\ i=1,…,l\\tag{12}\\\\alpha_i^\\ast g_i(x^\\ast ) &amp;=&amp;0,\\ i=1,…,k\\tag{13}\\g_i(x^\\ast )&amp;\\leq&amp; 0,\\ i=1,…,k\\tag{14}\\\\alpha_i^\\ast &amp;\\geq&amp; 0,\\ i=1,…,k\\tag{15}\\end{eqnarray}$$ &emsp;&emsp;其中条件(11)(12)与式(3)类似，通过求偏导来得到$x$与$\\beta$的极值点；条件(13)由式(10)得到；条件(14)为式(4)中的约束条件；而条件(15)由式(9)得到。&emsp;&emsp;这里要注意，对于一般的优化问题，KKT条件只是一个必要条件，也就是说可能存在其他极值点也满足KKT条件，但是它们不是最小极值点。若我们希望使用它来求解优化问题，就需要增加一些条件使它变成充要条件。因此接下来我们引入对偶问题。 拉格朗日对偶对偶优化问题&emsp;&emsp;通过引入广义拉格朗日函数我们将优化问题转换为了：$$\\min_x\\max_{\\alpha,\\beta:\\alpha_i\\geq0}\\mathcal{L}(x,\\alpha,\\beta)\\tag{16}$$ &emsp;&emsp;我们称式(16)为原优化问题（primal optimization problem），它先对$\\alpha,\\beta$求最大值，再对$x$求最小值。若我们将最大最小的顺序换一换，就得到了原问题的对偶问题。与之前相同，我们设方程：$$\\theta_{\\cal{D}}(\\alpha,\\beta)=\\min_x\\mathcal{L}(x,\\alpha,\\beta)\\tag{17}$$ &emsp;&emsp;其中$\\cal{D}$代表对偶。通过它我们得到对偶优化问题（dual optimization problem）：$$\\max_{\\alpha,\\beta:\\alpha_i\\geq0}\\theta_{\\cal{D}}(\\alpha,\\beta)=\\max_{\\alpha,\\beta:\\alpha_i\\geq0}\\min_x\\mathcal{L}(x,\\alpha,\\beta)\\tag{18}$$ &emsp;&emsp;数学上可以证明，$\\max\\min\\leq\\min\\max$（这里证明貌似挺难的，不过我们只需记住：瘦死的骆驼比马大）。因此我们得到：$$d^\\ast =\\max_{\\alpha,\\beta:\\alpha_i\\geq0}\\min_x\\mathcal{L}(x,\\alpha,\\beta)\\leq\\min_x\\max_{\\alpha,\\beta:\\alpha_i\\geq0}\\mathcal{L}(x,\\alpha,\\beta)=p^\\ast \\tag{19}$$ &emsp;&emsp;通常来说，求解对偶问题比求解原问题要更容易，并且由于$d^\\ast \\leq p^\\ast $，对偶问题的解提供了原问题解的下界。因此有时候我们可以同时用软件求解原问题与对偶问题，若它们得到的结果之差（也叫对偶距离）小于$\\epsilon$时，我们就可以说此时函数值与极值的距离小于$\\epsilon$。 slater条件&emsp;&emsp;对于一般问题，根据式(19)，$d^\\ast \\leq p^\\ast $成立，我们称它为弱对偶（weak duality）。但有时候优化问题满足$d^\\ast =p^\\ast $，此时我们称它为强对偶（strong duality）。若优化问题是强对偶，那么我们就能将原问题转换为对偶问题进行解决。因此我们引入slater条件，它是强对偶的一个充分条件。&emsp;&emsp;slater条件：对于式(4)所示的不等式优化问题，存在一个可行的$x$，使得所有不等式约束严格成立，则该优化问题为强对偶，即$d^\\ast =p^\\ast $。&emsp;&emsp;稍微解释一下：$x$可行（feasible）表示$x$满足所有约束条件；不等式严格成立就是指$g_i(x)&lt;0$。&emsp;&emsp;slater条件是一个判断强对偶的方法。并且可以证明，若优化问题为凸优化，并且满足slater条件，则式(11)～(15)所示的KKT条件就变成了最优解的充要条件，并且该解是原问题与对偶问题共同的解。此时我们可以直接通过KKT条件求解优化问题。 最优间隔分类器&emsp;&emsp;现在我们回到SVM。第一章节最后我们推导出了求解最优间隔分类器的优化问题：$$\\begin{split}\\min_{\\gamma,w,b}\\ &amp;{1 \\over 2}||w||^2\\{\\rm s.t.}\\ &amp;y^{(i)}(w^Tx^{(i)}+b)\\geq1,\\ i=1,…,m\\end{split}\\tag{20}$$ &emsp;&emsp;我们可以把约束写为：$$g_i(w)=-y^{(i)}(w^Tx^{(i)}+b)+1\\leq 0\\tag{21}$$ &emsp;&emsp;此时优化问题就变为了式(4)的形式。根据KKT条件中式(13)，只有$g_i(w)=0$，即训练样本的函数间隔等于$1$时，$\\alpha_i$才能大于$0$。它在SVM中具有直观的描述：&emsp;&emsp;图中SVM的决策边界穿过了三个样本，这三个样本的函数间隔均等于$1$，因此在优化问题中只有它们所对应的$\\alpha_i$才能大于$0$。也就是说只有它们对优化问题的决策边界起决定作用，而其他训练样本并没有什么贡献。因此这三个样本被称为支持向量。在一个训练集中，支持向量所占整体样本的比例通常很小。&emsp;&emsp;根据上图我们也能轻松证明SVM优化问题满足slater条件：把图中决策边界的$(w,b)$同时增大$k$倍，此时所有样本的几何间隔不变，但函数间隔增长为原来的$k$倍。只要$k&gt;1$，则式(21)中所有$g_i(w)&lt;0$成立。由于优化问题为凸优化，又满足slater条件，因此它的原问题与对偶问题拥有共同的最优解，并且KKT条件为最优解的充要条件。&emsp;&emsp;我们写出优化问题的拉格朗日方程：$$\\mathcal{L}(w,b,\\alpha)={1 \\over 2}||w||^2-\\sum_{i=1}^m\\alpha_i[y^{(i)}(w^Tx^{(i)}+b)-1]\\tag{22}$$ &emsp;&emsp;由于SVM优化问题只有不等式约束，因此式(22)中没有$\\beta$乘子。为了使用对偶问题求解，根据式(17)，我们先求偏导来最小化$\\mathcal{L}(w,b,\\alpha)$，得到$\\theta_{\\cal{D}}$：$$\\begin{eqnarray}\\nabla_w\\mathcal{L}(w,b,\\alpha)&amp;=&amp;w-\\sum_{i=1}^m\\alpha_iy^{(i)}x^{(i)}=0\\tag{23}\\{\\partial \\over \\partial b}\\mathcal{L}(w,b,\\alpha)&amp;=&amp;\\sum_{i=1}^m\\alpha_iy^{(i)}=0\\tag{24}\\end{eqnarray}$$ &emsp;&emsp;将式(23)(24)带回式(22)，我们得到： $$\\begin{split}\\mathcal{L}(w,b,\\alpha)&amp;=\\sum_{i=1}^m\\alpha_i-{1 \\over 2}\\sum_{i,j=1}^my^{(i)}y^{(j)}\\alpha_i\\alpha_j(x^{(i)})^Tx^{(j)}-b\\sum_{i=1}^m\\alpha_iy^{(i)}\\&amp;=\\sum_{i=1}^m\\alpha_i-{1 \\over 2}\\sum_{i,j=1}^my^{(i)}y^{(j)}\\alpha_i\\alpha_j(x^{(i)})^Tx^{(j)}\\end{split}\\tag{25}$$ &emsp;&emsp;此时优化问题就化为式(18)中最大化$\\theta_{\\cal{D}}$的对偶问题： $$\\begin{split}\\max_{\\alpha}\\ &amp;\\theta_{\\cal{D}}(\\alpha)=\\sum_{i=1}^m\\alpha_i-{1 \\over 2}\\sum_{i,j=1}^my^{(i)}y^{(j)}\\alpha_i\\alpha_j\\langle x^{(i)},x^{(j)}\\rangle\\{\\rm s.t.}\\ &amp;\\alpha_i\\geq0,\\ i=1,…,m\\&amp;\\sum_{i=1}^m\\alpha_iy^{(i)}=0\\end{split}\\tag{26}$$ &emsp;&emsp;之后我们能够通过SMO算法求解式(26)所示的对偶问题，得到相应$\\alpha_i$的值。将它们带入式(23)就能得到$w$的最优解$w^\\ast $。而$b$的最优解$b^\\ast $可以通过KKT条件中式(13)得到（此处省略证明）：$$b^\\ast =-{\\max_{i:y^{(i)}=-1}w^{\\ast T}x^{(i)}+\\min_{i:y^{(i)}=1}w^{\\ast T}x^{(i)} \\over 2}\\tag{27}$$ &emsp;&emsp;它表明决策边界在正负支持向量的中间。在得到最优解后我们便能用训练好的模型来预测新的样本了。根据式(23)，预测函数$h(x)$可以写为： $$\\begin{split}h(x)={\\rm sign}(w^Tx+b)&amp;={\\rm sign}\\left(\\left(\\sum_{i=1}^m\\alpha_iy^{(i)}x^{(i)}\\right)^Tx+b\\right)\\&amp;={\\rm sign}\\left(\\sum_{i=1}^m\\alpha_iy^{(i)}\\langle x^{(i)},x\\rangle+b\\right)\\end{split}\\tag{28}$$ &emsp;&emsp;我们可以发现，在预测新样本时，预测值只与新样本与训练样本特征的内积有关。而除了支持向量外，其余训练样本的$\\alpha_i$都为$0$。故我们只需计算新样本与支持向量的内积即可得到新样本的预测值。因此内积在SVM中起着关键作用。&emsp;&emsp;在之后的章节我们会将内积替换成更高维的核函数以达到更好的效果，并引入正则化来减少极端值的影响。 小结 可以使用拉格朗日称数法来求解优化问题。 求解优化问题的对偶问题能帮助我们解决原问题。若问题为强对偶，那么对偶问题与原问题的最优解相同。 SVM属于强对偶，可以借此求解SVM优化问题。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://haosutopia.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"https://haosutopia.github.io/tags/SVM/"}]},{"title":"支持向量机（一）","slug":"SVM-01","date":"2018-03-12T23:48:13.000Z","updated":"2018-07-01T22:00:32.000Z","comments":true,"path":"2018/03/SVM-01/","link":"","permalink":"https://haosutopia.github.io/2018/03/SVM-01/","excerpt":"","text":"&emsp;&emsp;支持向量机（Support Vector Machine）是一种强大的监督式学习算法。尽管目前深度学习非常流行，但这种经典的机器学习算法仍然有着非常大的研究价值，并且它在机器学习领域仍占有着一席之地。本次学习材料主要是CS229课程以及一些网上的资料。 SVM目标&emsp;&emsp;为了引入SVM思想，我们来看一个二分类问题。假设我们有一组训练样本${(x_1,y_1),…,(x_m,y_m)}$，其中$x_i\\in\\Bbb{R}^n$为第$i$个训练样本的特征，是一个$n$维向量；$y_i$为第$i$个训练样本的输出，由于这是个二分类问题，因此$y_i$只能取$1$或$0$，分别代表正负样本。&emsp;&emsp;若使用逻辑线性回归模型（logistic regression）对样本进行分类，预测函数则为：$$h(x)={\\rm sigmoid}(w^Tx+b)\\tag{1}$$ &emsp;&emsp;其中${\\rm sigmoid}(z)$是一个特殊函数，它的定义如下：$${\\rm sigmoid}(z)={1 \\over {1+e^{-z}}}\\tag{2}$$ &emsp;&emsp;我们可以将预测函数的输出看作是$y=1$的概率。若输入特征$x$使得$w^Tx+b\\geq0$，预测函数$h(x)\\geq0.5$，则预测输出$y=1$；若输入特征$x$使得$w^Tx+b&lt;0$，预测函数$h(x)&lt;0.5$，则预测输出$y=0$。我们发现，$w^Tx+b$越大，预测函数输出越接近$1$，我们就越有把握预测$y=1$；同样$w^Tx+b$越小，预测函数输出越接近$0$，我们就越有把握预测$y=0$。若一个模型能使得所有训练样本满足：$$\\begin{cases}w^Tx^{(i)}+b\\gg0,\\ 若y^{(i)}=1\\w^Tx^{(i)}+b\\ll0,\\ 若y^{(i)}=0\\end{cases}\\tag{3}$$ &emsp;&emsp;我们就认为该模型是个高可信度的模型，它的决策边界$w^Tx+b=0$能够将正负样本分开，且样本点距离决策边界足够远。换句话说，就是能够将正负样本分得足够开，这样它就不会因为微小的扰动将样本从其中一侧划到另一侧去，增加了模型抗扰动的能力。SVM的目标就是找到这样一组$(w,b)$，使得模型的可信度最高。 函数间隔与几何间隔&emsp;&emsp;在这里我们将二分类问题及模型稍微更改一下。取$y\\in{1,-1}$，并且将预测函数更改为：$$h(x)={\\rm sign}(w^Tx+b)\\tag{4}$$ &emsp;&emsp;其中${\\rm sign}(z)$为符号函数，它的定义为：$$\\begin{cases}1&amp;,z\\geq0\\-1&amp;,z&lt;0\\end{cases}\\tag{5}$$ &emsp;&emsp;实际上，更改前后的模型本质是一样的：$w^Tx+b\\geq0$时，预测为正样本；$w^Tx+b&lt;0$时，预测为负样本。只是$y$的取值变了，并且预测函数输出不再是概率而是直接输出预测值。&emsp;&emsp;为了描述正负样本分开的程度，我们引入函数间隔（functional margin）的概念。对于样本$(x^{(i)},y^{(i)})$来说，它的函数间隔定义为：$$\\hat{\\gamma}^{(i)}=y^{(i)}(w^Tx^{(i)}+b)\\tag{6}$$ &emsp;&emsp;若$y^{(i)}=1$，我们希望$w^Tx^{(i)}+b\\gg0$；若$y^{(i)}=-1$，我们希望$w^Tx^{(i)}+b\\ll0$。根据式(6)，我们可以用函数间隔的概念将这两种情况合并，即无论$y^{(i)}$取值，我们都希望训练样本的函数间隔$\\hat{\\gamma}^{(i)}\\gg0$（若$\\hat{\\gamma}&lt;0$，则表示预测错误）。若所有训练样本均满足$\\hat{\\gamma}^{(i)}\\gg0$，那么模型是一个高可信度的模型。因此我们使用所有训练样本函数间隔的最小值来定义模型$(w,b)$的函数间隔$\\hat{\\gamma}$：$$\\hat{\\gamma}=\\min_{i=1,…,m}\\hat{\\gamma}^{(i)}\\tag{7}$$ &emsp;&emsp;$\\hat{\\gamma}$代表了一个模型的可信程度。但是这里存在一个问题：如果将$(w,b)$同时放大$k$倍，我们发现$w^Tx+b$的正负没有改变，但$\\hat{\\gamma}$却放大为了原来的$k$倍。这表示模型的本质没有变化但可信度却强行提高了。为了避免这种情况产生，需要对$(w,b)$的数值有一定的限制，因此我们利用$||w||$对参数$(w,b)$进行标准化：$$(w,b)\\longrightarrow \\left({w \\over ||w||},{b \\over ||w||}\\right)\\tag{8}$$ &emsp;&emsp;由此我们可以定义样本$(x^{(i)},y^{(i)})$的几何间隔（geometric margin）：$$\\gamma^{(i)}=y^{(i)}\\left({w \\over ||w||}^Tx^{(i)}+{b \\over ||w||}\\right)\\tag{9}$$ &emsp;&emsp;$\\gamma^{(i)}$表示在$n$维空间中，样本与超平面之间的距离。考虑如下示意图： &emsp;&emsp;图中的超平面为决策边界$w^Tx+b=0$，且向量$w$与该超平面垂直（可通过数学证明）。点$B$为样本$A$在超平面上的投影，因此点$A$到超平面的距离就是$AB$的长度，且向量$BA$方向与$w$相同或相反（相同时为正样本；相反时为负样本）。设$A$点坐标为$x^{(i)}，$$AB$长度为$\\gamma^{(i)}$，则$B$点坐标可以表示为：$$\\vec{OB}=\\vec{OA}-\\vec{BA}=x^{(i)}-\\gamma^{(i)}{y^{(i)}w \\over ||w||}\\tag{10}$$ &emsp;&emsp;而$B$在超平面上，因此满足超平面方程$$w^Tx+b=0\\tag{11}$$ &emsp;&emsp;将式(10)带入式(11)中得到：$$w^T\\left(x^{(i)}-\\gamma^{(i)}{y^{(i)}w \\over ||w||}\\right)+b=0\\tag{12}$$ &emsp;&emsp;其中$w^Tw=||w||$，化简后得到：$$\\gamma^{(i)}=y^{(i)}\\left({w^T \\over ||w||}x^{(i)}+{b \\over ||w||}\\right)\\tag{13}$$ &emsp;&emsp;式(13)与式(9)一致，几何间隔就是样本与超平面之间的距离。与式(6)类似，我们也使用所有训练样本几何间隔的最小值来定义模型$(w,b)$的几何间隔$\\gamma$：$$\\gamma=\\min_{i=1,…,m}\\gamma^{(i)}\\tag{14}$$ &emsp;&emsp;我们可以发现函数间隔与几何间隔的关系为：$$\\gamma={\\hat{\\gamma \\over ||w||}}\\tag{15}$$ &emsp;&emsp;当$||w||=1$时，函数间隔与几何间隔相等。 最优间隔分类器&emsp;&emsp;在SVM目标中我们希望得到一个高可信度的模型，它的决策边界能够将正负样本分开，并且样本点距离决策边界足够远。在这里，我们使用间隔的定义更加严谨地描述一下这个优化目标：假设训练数据是线性可分离的（linearly separable），那么我们希望得到一个模型，使得它对于训练样本的几何间隔尽可能大。数学表达如下：$$\\begin{split}\\max_{\\gamma ,w,b}\\ &amp;\\gamma\\{\\rm s.t.}\\ &amp;y^{(i)}(w^Tx^{(i)}+b)\\geq \\gamma,\\ i=1,…,m\\&amp;||w||=1\\end{split}\\tag{16}$$ &emsp;&emsp;式(16)表明所有训练样本的几何间隔均大于等于$\\gamma$，而条件$||w||=1$则确保函数间隔与几何间隔相等，而我们要做的就是使这个几何间隔$\\gamma$取到最大。如果我们能解决这个问题，那么就已经可以了。但是由于条件$||w||=1$导致优化函数是非凸的，极值点不唯一，无法通过优化软件进行计算。因此我们将问题的数学描述更改一下：$$\\begin{split}\\max_{\\gamma,w,b}\\ &amp;{\\hat{\\gamma} \\over ||w||}\\{\\rm s.t.}\\ &amp;y^{(i)}(w^Tx^{(i)}+b)\\geq \\hat\\gamma,\\ i=1,…,m\\end{split}\\tag{17}$$ &emsp;&emsp;在这里我们舍弃了$||w||=1$这个条件，不再要求函数间隔等于几何间隔。同时让所有样本函数间隔均大于等于$\\hat\\gamma$，而最大化$\\hat\\gamma /||w||$。根据式(15)，$\\hat\\gamma /||w||$就是几何间隔，因此式(16)与式(17)实际是等价的。但由于$\\hat\\gamma /||w||$仍然是非凸的函数，还是无法通过优化软件进行计算。因此我们需要继续修改优化问题的数学描述。&emsp;&emsp;我们发现，这个优化问题实质是最大化几何间隔，而函数间隔是多少我们并不关心，它随时可以通过给$(w,b)$乘以常数来改变。因此我们可以假设函数间隔$\\hat\\gamma=1$，而最大化几何间隔$1/||w||$即可。因此我们将优化问题转化为： $$\\begin{split}\\min_{\\gamma,w,b}\\ &amp;{1 \\over 2}||w||^2\\{\\rm s.t.}\\ &amp;y^{(i)}(w^Tx^{(i)}+b)\\geq1,\\ i=1,…,m\\end{split}\\tag{18}$$ &emsp;&emsp;式(18)中最小化$||w||^2$与最大化$1/||w||$是等价的，并且$||w||^2$是个二次凸函数，因此可以使用优化软件来解决。通过它所得到的模型就是最优间隔分类器（optimal margin classifier）。为了更好地求解式(18)的优化问题，接下来的章节我们将介绍拉格朗日对偶并且导出式(18)的对偶形式，再由此引入支持向量以及核的概念。 小结 SVM目标是最大化模型决策边界与训练样本的几何间隔。 利用式(18)将该目标转化为一个带约束条件的优化问题。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://haosutopia.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"SVM","slug":"SVM","permalink":"https://haosutopia.github.io/tags/SVM/"}]},{"title":"利用爬虫搜集数据","slug":"Access-Web","date":"2018-03-09T22:24:15.000Z","updated":"2020-09-28T02:02:27.037Z","comments":true,"path":"2018/03/Access-Web/","link":"","permalink":"https://haosutopia.github.io/2018/03/Access-Web/","excerpt":"","text":"&emsp;&emsp;训练数据对于机器学习来说是必不可少的，因此在每个机器学习任务之前都会有一个搜集数据的过程，这个搜集过程通常来说是枯燥且费时的。不像很多公司本身就是数据的生产者，对于我们普通学习者来说，能使用的大部分数据均来自于网络。我们可以从网页上手动获取所需的数据，复制粘贴到本地，然而这是相当麻烦的。通过python我们可以模拟浏览器对网页进行抓取，并自动筛选出所需要的数据，大大提高搜集数据的效率。本次学习材料主要是Python for Everyone课程、BeautifulSoup官网以及一些网上的资料。 socket接收数据&emsp;&emsp;计算机上的应用程序通常是通过套接字（socket）与外界进行联系的。socket就相当于一个端口，程序通过它向网络服务器发出请求或应答网络请求。Python拥有一个内置的socket库，在它的帮助下我们能够轻松连接网络服务器并获取服务器上的数据。在使用之前我们需要导入socket库： 1import socket &emsp;&emsp;使用socket函数为我们的程序定义一个socket： 1mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) #定义一个socket，并命名为mysock &emsp;&emsp;第一个参数socket.AF_INET代表这个socket是用于服务器与服务器之间的网络通信；第二个参数socket.SOCK_STREAM代表它是基于TCP的流式socket通信。我们在网页抓取中使用的都是基于TCP的网络通信，因此在使用socket获取网络数据时，这些参数都是不变的。&emsp;&emsp;定义完socket后，我们需要将它连接到网络上的服务器端口。通常服务器不同端口分别提供不同的服务，TCP/IP协议规定Web服务使用80号端口，因此我们使用connect将socket连接到指定服务器的80号端口。这里我们以data.pr4e.org为例： 1mysock.connect((&#x27;data.pr4e.org&#x27;, 80)) #连接到data.pr4e.org的80号端口 &emsp;&emsp;若想获得该服务器上的某个文件（如romeo.txt），我们就需要通过socket对服务器发出请求。根据HTTP协议，请求数据时需要使用send向服务器发送一个结尾为空行的GET指令。由于python3使用Unicode来表示文字，而在网络传输中使用的是二进制格式，因此我们需要使用encode函数将Unicode编码为二进制再通过socket传递；同样在接收服务器数据后我们也需要使用decode对数据进行转换。代码如下： 12cmd = &#x27;GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n&#x27;.encode() #设置GET指令并编码mysock.send(cmd) #通过socket发送指令 &emsp;&emsp;发送完指令后就可以等待接收服务器的数据了。在这里我们使用一个for循环对数据进行接收，每次接收512个字节，直到接收完毕。代码如下： 12345while True: data = mysock.recv(512) #每次接收512字节 if(len(data) &lt; 1): #若接收数据为空，则跳出循环 breakprint(data.decode(),end=&#x27;&#x27;) #编码后打印接收到的数据 &emsp;&emsp;最后接收完毕，我们需要将socket关闭： 1mysock.close() &emsp;&emsp;程序输出结果为： 1234567891011121314HTTP&#x2F;1.1 200 OKDate: Sun, 14 Mar 2010 23:52:41 GMTServer: ApacheLast-Modified: Tue, 29 Dec 2009 01:31:22 GMTETag: &quot;143c1b33-a7-4b395bea&quot;Accept-Ranges: bytesContent-Length: 167Connection: closeContent-Type: text&#x2F;plainBut soft what light through yonder window breaksIt is the east and Juliet is the sunArise fair sun and kill the envious moonWho is already sick and pale with grief &emsp;&emsp;接收到的数据分为两个部分，它们使用一个空行分隔。空行前的部分是文件的头部（headers），它由网络服务器生成，用来描述文件的属性。比如最后一项Content-Type: text/plain表示文件是一个纯文本。空行后的部分才是文件真正的数据部分，因此在接收完数据后我们还需要对数据进行部分处理才能使用。在这里我们接收的是文本文件，而其他数据类型如图片等都是类似的，因为它们在网络中都是通过二进制传递。我们只需在接收完成后进行相应的解码就可以了。&emsp;&emsp;使用socket库需要我们手动发送请求给服务器并接收，还是有些麻烦的。接下来我们将使用另一个更加方便的库来实现之前的功能。 urllib接收数据&emsp;&emsp;urllib也是python的一个内置库，利用它我们可以把接收到的数据当作一个file进行处理，非常的方便。在使用之前我们需要导入urllib库： 1import urllib.request, urllib.parse, urllib.error &emsp;&emsp;在这里我们只需要一个命令便可以请求并接收所需文件数据。： 1fhand = urllib.request.urlopen(&#x27;http://data.pr4e.org/romeo.txt&#x27;) &emsp;&emsp;fhand是网络文件的句柄（handle），我们可以像file那样对fhand进行操作，比如使用for循环将文件内容逐行显示： 12for line in fhand: print(line.decode().strip()) #打印解码后的行 &emsp;&emsp;也可以使用read()直接将整个文件读取为一个string。这里要注意，使用read()时文件不能大过计算机的内存： 1print(fhand.read()) #读取整个文件并打印 &emsp;&emsp;与socket不同，urllib会将接收数据的头部（headers）与内容（content）分开，因此上面程序运行输出的是不含头部的文件数据。若要获取文件的头部，我们只需对fhand使用info()即可； 1info = fhand.info() BeautifulSoup分析网页源码&emsp;&emsp;通常在服务器抓取数据时，我们不知道目标文件的网络地址，因此需要通过分析网页的源码来得到它们。例如我们想获取下图百度首页上的logo图片：&emsp;&emsp;但我们只知道百度的网址是：www.baidu.com。这时候我们只能通过python抓取网页的源码，对其进行分析后才能得到该图片的网络地址。分析源码后我们得到表示该图片的img标签： 1&lt;img hidefocus=&quot;true&quot; src=&quot;//www.baidu.com/img/bd_logo1.png&quot; usemap=&quot;#mp&quot; width=&quot;270&quot; height=&quot;129&quot;&gt; &emsp;&emsp;其中src属性的值就是该图片的网络地址。由于网页的源码通常非常复杂，并且很多网站编写时并不是特别规范，因此使用传统方式对源码进行分析是非常困难的。幸好我们有个强大的工具——BeautifulSoup（不知道哪位老哥起的奇葩名字）。它可以将HTML的标签通过节点表示，建立一个文档树。通过这个文档树，我们可以方便地找到想要的标签并提取出其中的内容。在整个数据抓取过程中，我们利用urllib抓取网页源码，再使用BeatifulSoup对网页源码进行分析。不过在这之前我们需要对HTML的语法结构以及标签类型有一个初步的认识，具体可以参考MDN的教程。&emsp;&emsp;根据官网提示安装BeautifulSoup，安装完毕后便可以开始写程序了。我们以一个简单的网页（www.dr-chuck.com/page1.htm）为例，它的HTML代码为： 1234567891011&lt;html&gt; &lt;head&gt;&lt;/head&gt; &lt;body&gt; &lt;h1&gt;The First Page&lt;/h1&gt; &lt;p&gt; If you like, you can switch to the &lt;a href=&quot;http://www.dr-chuck.com/page2.htm&quot;&gt;Second Page&lt;/a&gt; . &lt;/p&gt; &lt;/body&gt;&lt;/html&gt; &emsp;&emsp;首先导入urllib与BeautifulSoup库： 12import urllib.request, urllib.parse, urllib.errorfrom bs4 import BeautifulSoup &emsp;&emsp;使用urllib读取页面源码，并储存在一个变量中： 12url = &quot;http://www.dr-chuck.com/page1.htm&quot; #设置网页链接html = urllib.request.urlopen(url).read() #将网页源码储存在变量html中 &emsp;&emsp;使用该变量创建一个BeautifulSoup对象： 1soup = BeautifulSoup(html, &#x27;html.parser&#x27;) #创建BeautifulSoup对象 &emsp;&emsp;其中第二个参数‘html.parser’表明BeautifulSoup使用的解析器为Python标准库中的HTML解析器，除此之外它还支持一些第三方解析器如lxml。lxml解析器速度更快且更加强大，不过需要安装之后才能使用。对于我们这种小打小闹，python标准库的解析器已经足够啦。&emsp;&emsp;利用BeauifulSoup我们可以快速获得源码的标签（Tag）。比如我们需要获得源码中的a标签，则可使用如下代码： 1print(soup.a) &emsp;&emsp;其输出为： 1&lt;a href=&quot;http://www.dr-chuck.com/page2.htm&quot;&gt;Second Page&lt;/a&gt; &emsp;&emsp;但是通过这种方式我们只能获得源码中的第一个a标签。若我们希望查找所有标签，可使用find_all函数，其完整定义为：find_all(name, attrs, recursive, text, **kwargs)。它有很多参数，但最常用的是name（标签类型）和attrs（标签属性）。我们可以通过指定标签类型或属性来筛选出满足条件的所有标签，并将它们合并为一个列表（list）。使用举例如下： 123print(soup.find_all(&#x27;a&#x27;)) #检索所有a标签print(soup.find_all(href=&quot;http://www.dr-chuck.com/page2.htm&quot;)) #检索所有属性满足的标签print(soup.find_all(&#x27;a&#x27;, href=&quot;http://www.dr-chuck.com/page2.htm&quot;)) #检索所有属性满足的a标签 &emsp;&emsp;由于网页源码中只有一个a标签，因此它们输出相同，均为： 1[&lt;a href=&quot;http://www.dr-chuck.com/page2.htm&quot;&gt;Second Page&lt;/a&gt;] &emsp;&emsp;由于输出的是一个列表，因此我们能够通过for循环来对列表中每一个标签进行操作。数据抓取中最常用的操作是获取标签的属性，我们可以通过以下两种方式得到： 12print(soup.a[&#x27;href&#x27;])print(soup.a.get(&#x27;href&#x27;)) &emsp;&emsp;它们的作用是一样的，输出得到a标签的’href’属性值： 12http:&#x2F;&#x2F;www.dr-chuck.com&#x2F;page2.htmhttp:&#x2F;&#x2F;www.dr-chuck.com&#x2F;page2.htm 爬虫抓取数据实例&emsp;&emsp;在这里我们将利用之前介绍的相关知识，使用爬虫从一个动漫网站上抓取动漫图片。在写代码之前我们需要观察网站及其源码，找到一些关键链接以及标签来方便之后的操作。网站外观大致如下：&emsp;&emsp;我们发现网站的图片分布在各个页面上，而每个页面的网址是有规律的，它可以表示为如下形式： 1http:&#x2F;&#x2F;konachan.net&#x2F;post?page&#x3D; + 页码数 + &amp;tags&#x3D; &emsp;&emsp;在每个页面上我们需要点击缩略图来跳转到包含相应原图的网页上，因此我们要找到每个缩略图所指向的地址。使用火狐浏览器-选项-Web开发者-查看器，我们能够轻松得到页面任意位置所对应的网页源码：&emsp;&emsp;通过查看器我们发现每个缩略图对应的超链接标签（即a标签）都有一个类属性（class）为’thumb’。同样在包含原图的网页源码中，我们发现每一张原图对应的图片标签（即img标签）都有一个类属性为’image’。至此我们就获得了写代码需要的所有信息，可以开始写代码了。首先还是导入所需的库： 12import urllib.request, urllib.parse, urllib.errorfrom bs4 import BeautifulSoup &emsp;&emsp;我们使用一个循环来获得不同页面的网址，并通过urllib获取页面的源码。这里我们选择抓取前100页的图片： 1234surl= &quot;http://konachan.net/post?page=&quot;for i in range(1,101): url = surl + str(i) + &quot;&amp;tags=&quot; #得到第i页网址 html = urllib.request.urlopen(url).read() #得到第i页源码 &emsp;&emsp;创建BeautifulSoup对象，并通过find_all函数找到所有类属性为‘thumb’的超链接标签： 12soup = BeautifulSoup(html, &#x27;html.parser&#x27;) #创建BeautifulSoup对象tags = soup.find_all(&#x27;a&#x27;, class_=&quot;thumb&quot;) #得到所有类属性为thumb的的超链接标签（由于class为python关键字，因此使用class_来表示） &emsp;&emsp;使用for循环遍历标签列表，得到每一个标签的链接地址： 12for a in tags: iurl = &#x27;http://konachan.net&#x27; + a[&#x27;href&#x27;]; #得到网络地址 &emsp;&emsp;这些网络地址均为包含原图的网页。使用与之前相同的方法得到网页源码，并利用BeautifulSoup得到图片地址： 1234ihtml = urllib.request.urlopen(iurl).read() #得到网页源码isoup = BeautifulSoup(ihtml, &#x27;html.parser&#x27;) #创建BeautifulSoup对象itag = isoup.find_all(&#x27;img&#x27;, class_=&#x27;image&#x27;) #得到所有类属性为image的img标签iiurl = &#x27;http:&#x27; + itag[0][&#x27;src&#x27;] #得到图片地址 &emsp;&emsp;最后使用urllib获得图片数据，并保存至本地。由于我们不确定图片的格式，因此使用图片地址末尾的后缀来创建文件： 123f = open(&#x27;image_name&#x27; + &#x27;.&#x27; + iiurl.split(&#x27;.&#x27;)[-1], &#x27;wb&#x27;) #创建图片文件f.write(urllib.request.urlopen(iiurl).read()) #写入图片数据f.close #关闭图片文件 &emsp;&emsp;完整代码如下： 123456789101112131415161718192021import urllib.request, urllib.parse, urllib.errorfrom bs4 import BeautifulSoupk = 1;surl= &quot;http://konachan.net/post?page=&quot;for i in range(1,101): url = surl + str(i) + &quot;&amp;tags=&quot; html = urllib.request.urlopen(url).read() soup = BeautifulSoup(html, &#x27;html.parser&#x27;) tags = soup.find_all(&#x27;a&#x27;, class_=&quot;thumb&quot;) for a in tags: iurl = &#x27;http://konachan.net&#x27; + a[&#x27;href&#x27;]; ihtml = urllib.request.urlopen(iurl).read() isoup = BeautifulSoup(ihtml, &#x27;html.parser&#x27;) itag = isoup.find_all(&#x27;img&#x27;, class_=&#x27;image&#x27;) iiurl = &#x27;http:&#x27; + itag[0][&#x27;src&#x27;] f = open(&#x27;images/&#x27; + str(k) + &#x27;.&#x27; + iiurl.split(&#x27;.&#x27;)[-1], &#x27;wb&#x27;) f.write(urllib.request.urlopen(iiurl).read()) f.close print(&#x27;image&#x27; + str(k) + &#x27; produced&#x27;) k = k + 1 &emsp;&emsp;在这里我们用递增的变量k来命名文件，以方便之后的操作，并将它们保存在images文件夹中。至此我们便使用爬虫完成了一个简单的数据抓取任务。当然在抓取数据时还会遇到各种其他问题，比如需要登录才能获取数据，或者数据被加密了（这在视频网站很常见），之后有时间再研究吧。最终抓取的图片如下： 小结 使用socket能够连接网络服务器并进行数据请求及回应。 使用urllib能够更方便地实现网页抓取。 BeautifulSoup能够帮助我们分析网页源码并获取有用信息。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://haosutopia.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://haosutopia.github.io/tags/Python/"}]},{"title":"分布参数系统控制（三）：求解模型","slug":"RSVP-03","date":"2018-03-05T00:35:21.000Z","updated":"2018-04-10T18:37:30.000Z","comments":true,"path":"2018/03/RSVP-03/","link":"","permalink":"https://haosutopia.github.io/2018/03/RSVP-03/","excerpt":"","text":"&emsp;&emsp;在建立系统方程后，便可以求解系统的状态变量随时间的函数了，当然此时系统需要满足一定条件。本文将以上一章得到的系统状态方程为基础，对一般系统求解进行推导。系统状态方程如下：$$\\begin{eqnarray}\\dot{x}(t)&amp;=&amp;\\mathcal{A}x(t)+\\mathcal{B}u(t),\\ &amp;t&gt;0,\\ x(0)=x_0\\in X\\tag{1}\\y(t)&amp;=&amp;\\mathcal{C}x(t),\\ &amp;t\\geq0\\tag{2}\\end{eqnarray}$$ 状态空间分解&emsp;&emsp;根据第一章节式(13)可知，若存在系统算子$\\mathcal{A}:X\\to X$，则可以通过$\\mathcal{A}$的特征元素${\\phi_i,i\\geq1}$及其伴随算子$\\mathcal{A}^\\ast $的特征元素${\\psi_i,i\\geq1}$将空间$X$中任意元素表示成$\\phi_i$的无穷线性组合。因此可以将式(1)中的$x$元素进行分解：$$\\begin{eqnarray}x(t)&amp;=&amp;\\sum_{i=1}^{\\infty}\\langle x(t),\\psi_i\\rangle\\phi_i=\\sum_{i=1}^{\\infty}x_i^\\ast (t)\\phi_i\\tag{3}\\x(0)&amp;=&amp;\\sum_{i=1}^{\\infty}\\langle x(0),\\psi_i\\rangle\\phi_i=\\sum_{i=1}^{\\infty}x_i^\\ast (0)\\phi_i\\tag{4}\\end{eqnarray}$$ &emsp;&emsp;其中：$$\\begin{eqnarray}x_i^\\ast (t)&amp;=&amp;\\langle x(t),\\psi_i\\rangle\\tag{5}\\x_i^\\ast (0)&amp;=&amp;\\langle x(0),\\psi_i\\rangle\\tag{6}\\end{eqnarray}$$ &emsp;&emsp;$u$也可以用相同方法分解：$$\\begin{split}\\mathcal{B}u(t)=\\sum_{i=1}^pb_iu_i(t)&amp;=\\sum_{i=1}^p\\sum_{j=1}^{\\infty}\\langle b_i,\\psi_j\\rangle\\phi_ju_i(t)\\&amp;=\\sum_{i=1}^p\\sum_{j=1}^{\\infty}b_{ij}^\\ast \\phi_ju_i(t)\\&amp;=\\sum_{j=1}^{\\infty}\\phi_j\\sum_{i=1}^pb_{ij}^\\ast u_i(t)\\&amp;=\\sum_{i=1}^{\\infty}\\phi_i{b_i^\\ast }^Tu(t)\\end{split}\\tag{7}$$ &emsp;&emsp;其中：$$\\begin{eqnarray}b_{ij}^\\ast &amp;=&amp;\\langle b_i,\\psi_j\\rangle\\tag{8}\\{b_i^\\ast }^T&amp;=&amp;\\begin{bmatrix}b_{1i}^\\ast &amp; \\cdots &amp; b_{pi}^\\ast \\end{bmatrix}\\tag{9}\\end{eqnarray}$$ &emsp;&emsp;将式(3)(7)带入式(1)中得到分解后的状态方程为：$$\\sum_{i=1}^{\\infty}\\dot{x}i^\\ast (t)\\phi_i=\\sum_{i=1}^{\\infty}x_i^\\ast (t)\\mathcal{A}\\phi_i+\\sum{i=1}^{\\infty}\\phi_i{b_i^\\ast }^Tu(t)\\tag{10}$$ &emsp;&emsp;由于$\\phi_i$为$\\mathcal{A}$的特征元素，因此满足：$\\mathcal{A}\\phi_i=\\lambda_i\\phi_i$。则式(10)中第$i$个分量（$i=1,2,…$）所满足的方程为：$$\\dot{x}_i^\\ast (t)=\\lambda_ix_i^\\ast (t)+{b_i^\\ast }^Tu(t)\\tag{11}$$ &emsp;&emsp;通过求解常微分方程得到第$i$个分量随时间变化的函数：$$x_i^\\ast (t)=e^{\\lambda_it}x_i^\\ast (0)+\\int_0^te^{\\lambda_i(t-\\tau)}{b_i^\\ast }^Tu(\\tau){\\rm d}\\tau\\tag{12}$$ &emsp;&emsp;将式(12)中每个分量带入式(3)得到状态变量$x(z,t)$： $$x(z,t)=\\sum_{i=1}^{\\infty}e^{\\lambda_it}\\phi_i(z)x_i^\\ast (0)+\\sum_{i=1}^{\\infty}\\int_0^te^{\\lambda_i(t-\\tau)}\\phi_i(z){b_i^\\ast }^Tu(\\tau){\\rm d}\\tau\\tag{13}$$ &emsp;&emsp;我们只需求得系统算子$\\mathcal{A}$的谱集及特征元素，带入式(13)便是系统状态变量的解了。再将该解带入式(2)便求得了系统输出$y(t)$。 Riesz-Spektral系统&emsp;&emsp;仔细的话可以发现，我们在式(11)的推导过程中已经无意间给整个系统增加了一个约束条件：$\\lambda$不能是连续的。也就是说上述推导只能在$\\lambda_i$为一个个孤立的值时才能成立，否则无法求得状态变量。因此我们引入Riesz-Spektral系统来满足这个约束。Riesz-Spektral系统定义为：&emsp;&emsp;1. 系统算子$\\mathcal{A}$为Riesz-Spektral算子。&emsp;&emsp;2. 输入算子$\\mathcal{B}$为有界算子。&emsp;&emsp;3. 输出算子$\\mathcal{C}$为有界算子。&emsp;&emsp;若系统（$\\mathcal{C,A,B}$）满足以上三个条件，则系统为Riesz-Spektral系统。&emsp;&emsp;其中Riesz-Spektral算子具体定义见第一章节，它保证了系统算子$\\mathcal{A}$的特征值是孤立的。有界算子定义为：若$\\mathcal{L}$为有界算子，则存在$M&gt;0$，使得对所有在定义域$X$内的$v$，满足：$$||\\mathcal{L}v||_Y\\leq M||v||_X$$ &emsp;&emsp;也就是说不管取定义域内什么元素，算子$\\mathcal{L}$作用后所得结果的模永远是有限的，不可能达到无穷。因此$\\mathcal{B}$与$\\mathcal{C}$为有界算子是显而易见的。若$\\mathcal{B}$不是有界算子，那么$x(t)$可能为无穷；若$\\mathcal{C}$不是有界算子，那么$y(t)$可能为无穷。通常来说，实际系统都满足有界算子的条件，因此我们关注的重点一般是条件一。在之后的讨论中，若不特殊说明，我们均假设系统为Riesz-Spektral系统。 谱集与特征元素&emsp;&emsp;下面我们将以导热棒系统为例，来求解系统算子的谱集及特征元素。导热棒系统中系统算子$\\mathcal{A}:X\\to X$定义为：$$\\mathcal{A}h={\\rm d}_z^2h\\tag{14}$$ $$D(\\mathcal{A})={h\\in L_2(0,1)|h,{\\rm d}_zh绝对连续,{\\rm d}_z^2h\\in L_2(0,1),且h(0)=h(1)=0}$$ &emsp;&emsp;对比第一章节式(17)可知$\\mathcal{A}$为Sturm-Liouville算子，其谱集均由孤立单根的实特征值组成。且$\\mathcal{A}$的特征元素$\\phi(z)$满足式(14)。由此可以得到求解方程：$$\\begin{eqnarray}&amp;\\mathcal{A}\\phi(z)=\\lambda\\phi(z)={\\rm d}_z^2\\phi(z)\\tag{15}\\&amp;\\phi(0)=\\phi(1)=0\\tag{16}\\end{eqnarray}$$ &emsp;&emsp;式(16)为常微分方程，求解其特征方程：$$p^2-\\lambda=0\\Longrightarrow p_{1,2}=\\pm\\sqrt{\\lambda}\\tag{17}$$ &emsp;&emsp;根据$\\lambda$取值不同分为三种情况：&emsp;&emsp;1. $\\lambda&gt;0$$$\\begin{eqnarray}\\phi(z)&amp;=&amp;Ae^{\\sqrt{\\lambda}z}+Be^{-\\sqrt{\\lambda}z}\\tag{18}\\\\Rightarrow\\phi(0)&amp;=&amp;A+B=0\\tag{19}\\\\Rightarrow\\phi(1)&amp;=&amp;Ae^{\\sqrt{\\lambda}}+Be^{-\\sqrt{\\lambda}}\\tag{20}\\end{eqnarray}$$ &emsp;&emsp;&emsp;由式(19)(20)得到：$A=B=0$，只有零解，舍去。&emsp;&emsp;2. $\\lambda=0$$$\\begin{eqnarray}\\phi(z)&amp;=&amp;A+Bz\\tag{21}\\\\Rightarrow\\phi(0)&amp;=&amp;A=0\\tag{22}\\\\Rightarrow\\phi(1)&amp;=&amp;B=0\\tag{23}\\end{eqnarray}$$ &emsp;&emsp;&emsp;由式(22)(23)得到：$A=B=0$，只有零解，舍去。&emsp;&emsp;3. $\\lambda&lt;0$$$\\begin{eqnarray}\\phi(z)&amp;=&amp;A\\sin(\\sqrt{|\\lambda|}z)+B\\cos(\\sqrt{|\\lambda|}z)\\tag{24}\\\\Rightarrow\\phi(0)&amp;=&amp;B=0\\tag{25}\\\\Rightarrow\\phi(1)&amp;=&amp;A\\sin(\\sqrt{|\\lambda|})=0\\tag{26}\\end{eqnarray}$$ &emsp;&emsp;&emsp;由式(23)可得：$$\\begin{eqnarray}\\sqrt{|\\lambda_i|}&amp;=&amp;i\\pi,\\ i=\\pm 1,\\pm 2…\\tag{27}\\\\Rightarrow|\\lambda_i|&amp;=&amp;i^2\\pi^2\\tag{28}\\\\end{eqnarray}$$ &emsp;&emsp;&emsp;由于$\\lambda&lt;0$，得到：$$\\lambda_i=-i^2\\pi^2\\tag{29}$$ &emsp;&emsp;&emsp;将式(25)(29)带入式(24)得到特征元素：$$\\phi_i=A\\sin(i\\pi z)\\tag{30}$$ &emsp;&emsp;综上所述，得到谱集及特征元素为：$$\\begin{eqnarray}\\lambda_i&amp;=&amp;-i^2\\pi^2\\tag{31}\\\\phi_i&amp;=&amp;A\\sin(i\\pi z)\\tag{32}\\end{eqnarray}$$ &emsp;&emsp;由于$\\mathcal{A}$为Sturm-Liouville算子，则其伴随算子$\\mathcal{A}^\\ast $的特征元素与$\\mathcal{A}$的特征元素相等：$$\\psi_i=\\phi_i=A\\sin(i\\pi z)\\tag{33}$$ &emsp;&emsp;其中$A$是任意取的，将求得结果带回式(13)便能得到$x(z,t)$的解了。我们也可以将特征元素标准化，根据第一章节式(12)：$$\\begin{split}\\langle\\phi_i,\\psi_i\\rangle&amp;=\\int_0^1A_i\\sin(i\\pi z)A_j\\sin(j\\pi z){\\rm d}z\\&amp;={1 \\over 2}A_iA_j\\delta_{ij}=\\delta_{ij}\\end{split}\\tag{34}$$ &emsp;&emsp;求得$A_i=\\sqrt{2}$，则标准特征元素为：$$\\phi_i(z)=\\sqrt{2}\\sin(i\\pi z),\\ i\\in\\Bbb{N}\\tag{35}$$ 频域求解&emsp;&emsp;与集总参数系统类似，分布参数系统也可以在频域中描述。首先将式(1)(2)转化到频域：$$\\begin{cases}\\dot{x}(t)={\\cal A}x(t)+{\\cal B}u(t)\\y(t)={\\cal C}x(t)\\end{cases}\\Rightarrow\\begin{cases}sx(s)={\\cal A}x(s)+{\\cal B}u(s)\\y(s)={\\cal C}x(s)\\end{cases}$$ &emsp;&emsp;转换过程中我们将$x(z,t)$转化为$x(z,s)$，而对于算子来说，它们作用的变量均为$z$，因此都保持不变。上式可以变换为：$$(sI-{\\cal A})x(s)={\\cal B}u(s),\\ s\\in \\rho({\\cal A})\\tag{36}$$ &emsp;&emsp;式(36)在状态空间中分解得到：$$\\sum_{i=1}^{\\infty}(s-\\lambda_i)x_i^{\\ast}\\phi_i=\\sum_{i=1}^{\\infty}\\phi_i{b_i^\\ast}^{T}u\\tag{37}$$ &emsp;&emsp;其中第$i$个分量所满足的方程为：$$\\begin{eqnarray}&amp;&amp;(s-\\lambda_i)x_i^\\ast={b_i^\\ast}^Tu\\\\Rightarrow&amp;&amp; x_i^\\ast={ {b_i^\\ast}^Tu \\over (s-\\lambda_i)}\\tag{38}\\end{eqnarray}$$ &emsp;&emsp;将所有分量合并得到$x(s)$以及$y(s)$：$$\\begin{eqnarray}x(s)=\\sum_{i=1}^{\\infty}x_i^\\ast\\phi_i=\\sum_{i=1}^{\\infty}{\\phi_i{b_i^\\ast}^Tu(s) \\over (s-\\lambda_i)}\\tag{39}\\y(s)={\\cal C}x(s)=\\sum_{i=1}^{\\infty}{ {\\cal C}\\phi_i{b_i^\\ast}^T \\over (s-\\lambda_i)}u(s)\\tag{40}\\end{eqnarray}$$ &emsp;&emsp;最终我们提取出系统的传递矩阵（Übertragungsmatrix）：$$F(s)=\\sum_{i=1}^{\\infty}{ {\\cal C}\\phi_i{b_i^\\ast}^T \\over (s-\\lambda_i)}\\tag{41}$$ &emsp;&emsp;式(41)为系统传递矩阵的通式，但实际计算中我们通常带入某个具体算子并求解相应微分方程从而得到传递矩阵。求解过程中$z$为微分方程变量，$s$为常数。 小结 通过谱集及特征元素可以将系统状态方程写成分量形式求解。 求解前提为系统为Riesz-Spektral系统。 频域中系统可以通过传递矩阵来描述。","categories":[{"name":"控制理论","slug":"控制理论","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/"},{"name":"分布参数系统控制","slug":"控制理论/分布参数系统控制","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F%E6%8E%A7%E5%88%B6/"}],"tags":[{"name":"分布参数系统","slug":"分布参数系统","permalink":"https://haosutopia.github.io/tags/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F/"}]},{"title":"迁移学习","slug":"Transfer-Learning","date":"2018-03-03T15:57:18.000Z","updated":"2019-03-18T21:49:52.000Z","comments":true,"path":"2018/03/Transfer-Learning/","link":"","permalink":"https://haosutopia.github.io/2018/03/Transfer-Learning/","excerpt":"","text":"&emsp;&emsp;作为一个苦逼的学生党，如果没有实验室或者公司支持，自己实践深度学习时经常会遇到一些困难：“自己网上搜集数据好麻烦，现成的数据集很多又都是要申请才能获得”；“破笔记本也太烂了，训练个两层CNN就要跑好几天，要达到满意的效果不知道要跑到猴年马月了”。在没有能力或者单纯不想从零开始训练模型时，迁移学习无疑是一大神器。我们可以从网上下载别人已经训练好的模型，将它们迁移到我们自己的任务中来。这些模型可能已经用了大量数据训练了很长时间了，因此使用它们可以大大提高我们的效率，并且能得到非常好的效果。本次学习材料主要是Deeplearning.ai课程以及当年本人毕设论文的部分内容，使用的是基于Matlab的Matcaffe框架，初步实现了人脸识别应用的One-shot learning。献丑了献丑了。 概念&emsp;&emsp;迁移学习（Transfer Learning）顾名思义就是把已经训练好的模型迁移到新的任务中来使用。这里有一个前提就是新的任务和原来模型的目标任务有一定的相关性。比方说旧的模型是用来识别人脸的，就不能把它迁移去识别汽车，但可以用它来识别表情。但是这种所谓相关性是非常主观的，目前应该并没有一个严谨的理论来描述这种相关性，更多的是凭借直觉或者经验判断两者是否拥有一些共同特征。&emsp;&emsp;通常根据我们拥有的训练数据多少以及新旧任务的相关性程度，可以对训练好的模型进行不同的操作。如果只有少量的训练数据并且新旧任务相关性较大的情况下，我们可以把训练好的模型当作一个特征提取器，通过它来提取数据特征，再进行进一步的训练。以CNN模型为例：&emsp;&emsp;这是CNN的典型结构。输入图像$x$，经过一系列层最终得到输出$\\hat{y}$，它代表了输入$x$属于各分类的概率。通常CNN最后一层为softmax层，我们可以将它删除，并根据自己的分类数量添加一个自定义softmax层。在使用数据训练时，我们将原模型的权重冻结，只训练我们自己定义的那层即可，这在很多深度学习框架中都能够实现。这种方式就相当于把数据通过模型提取出一系列特征，再将这些特征输入到我们定义的softmax层中用于分类。当然我们不止可以定义一层，也可以在原模型的基础上增加若干层。&emsp;&emsp;如果我们有大量训练数据或者新旧任务相关性不是特别大的时候，可以只将（删除softmax层后）原模型的一部分层冻结，用训练数据训练剩余的层。或者干脆就将原模型权重作为现模型的初始值，训练整个模型。这些方式需要的训练数据量以及训练迭代次数依次增加。 应用实践&emsp;&emsp;在这里使用迁移学习实现一个可交互的人脸识别应用，大致过程为：输入图像-&gt;检测图像中的人脸-&gt;剪裁人脸并缩放成标准大小-&gt;输入模型进行识别-&gt;若识别身份有误或者身份未在数据文件中，则人工修改身份-&gt;训练模型。其中人脸检测模型使用的是深圳先进技术研究所的MTCNN模型，人脸识别模型使用的是牛津大学的Visual Geometry Group的VGG-Face。由于条件有限，训练时仅训练自定义的softmax层。 模型简介MTCNN&emsp;&emsp;MTCNN由三个串联的CNN网络组成，它能够检测输入图片中的人脸，并将其眼睛、鼻子与嘴唇的几个关键位置找到并标记。其示意图如下：&emsp;&emsp;识别的步骤为：先将图片进行缩放，构造一个图像金字塔，再将它们输入到一个三级串联的网络中（P-Net，R-Net，O-Net），其中：&emsp;&emsp;Proposal Network(P-Net)**：该网络结构主要用来得到人脸区域的候选窗口和边界框的回归向量。再使用边界框的回归向量,对候选窗口进行校准,最后通过非极大值抑制(NMS:non-maximum suppression)来合并高度重叠的候选框。&emsp;&emsp;**Refine Network (R-Net)**：结构与 P-Net 类似,亦是通过边界框回归和NMS,在P-Net中输入的候选框来去掉一些不正确的候选框,从而获得更精确的选取边框。&emsp;&emsp;**Output Network（O-Net）：该层比R-Net层又多了一层卷积层，所以处理的结果会更加精细。其作用和R-Net一致，但是它对人脸区域进行了更多的监督，同时还会输出5个特征点。 VGG-Face&emsp;&emsp;VGG-Face是一个通过260万张图片训练出来的CNN模型，具有很深的结构。它由1个输入层，13个卷积层，5个pooling层，2个全连接层和1个softmax层组成。其网络结构如下图所示：&emsp;&emsp;整个网络非常深，并且它仅用了比别人少得多的数据，却取得了更好的效果，在LFW（Labeled Faces In the Wild）与YFW（YouTube Faces Dataset）中均取得了当时世界顶级的成绩。 代码实现&emsp;&emsp;应用实现时使用了Matlab的APP Designer交互界面，因此部分代码会包含App Designer的部分函数，但这不是本文的重点。&emsp;&emsp;Caffe的模型包含模型数据和模型框架两个文件，它们的后缀分别为caffemodel和prototxt。若要修改一个模型的网络结构，我们仅需在框架文件中修改层的定义即可。通常层的定义形式如下： 1234567891011layers &#123; bottom: &quot;data&quot; %输入为data top: &quot;conv1&quot; %输出为conv1 name: &quot;conv1&quot; %层名字为conv1 type: CONVOLUTION %层类型为卷积层 convolution_param &#123; %层参数定义 num_output: 64 %filter数量为64 pad: 1 %步长为1 kernel_size: 3 %filter大小为3 &#125;&#125; &emsp;&emsp;在迁移学习中，我们要把VGG-Face的最后一层softmax去除，因此只需在VGG-Face框架文件中将最后一层的定义删除即可。在VGG-Face之后我们要加一层softmax层来预测自己的图像，由于本次应用中会有人员增加，softmax层的长度也会随之增加，因此在这里没有使用框架定义softmax层，而是直接在matlab中实现。并且通过自定义的数据格式，将每个人员的名字，特征以及对应softmax层的权重储存在一个cell数组中，并保存在名为face_data.mat文件中。cell数组结构示意如下：$${W_i,\\ b_i,\\ {name_i},\\ \\begin{Bmatrix} feature_1 \\ \\vdots \\ feature_n \\end{Bmatrix}}$$&emsp;&emsp;从左至右依次为第$i$个身份对应的权重、偏差、姓名以及储存的所有人脸特征。定义完这些后我们便可以开始写程序了。首先设置caffe的运行模式为cpu（当然你也可以选择gpu，但当时我的电脑显卡太烂，不支持CUDA）： 1caffe.set_mode_cpu(); %设置运行模式 &emsp;&emsp;加载所需的模型： 123456789101112131415prototxt_dir =strcat(caffe_model_path,&#x27;/det1.prototxt&#x27;); %设置框架文件路径model_dir = strcat(caffe_model_path,&#x27;/det1.caffemodel&#x27;); %设置模型数据路径PNet=caffe.Net(prototxt_dir,model_dir,&#x27;test&#x27;); %加载模型prototxt_dir = strcat(caffe_model_path,&#x27;/det2.prototxt&#x27;);model_dir = strcat(caffe_model_path,&#x27;/det2.caffemodel&#x27;);RNet=caffe.Net(prototxt_dir,model_dir,&#x27;test&#x27;);prototxt_dir = strcat(caffe_model_path,&#x27;/det3.prototxt&#x27;);model_dir = strcat(caffe_model_path,&#x27;/det3.caffemodel&#x27;);ONet=caffe.Net(prototxt_dir,model_dir,&#x27;test&#x27;);prototxt_dir = strcat(caffe_model_path,&#x27;/det4.prototxt&#x27;);model_dir = strcat(caffe_model_path,&#x27;/det4.caffemodel&#x27;);LNet=caffe.Net(prototxt_dir,model_dir,&#x27;test&#x27;);prototxt_dir = strcat(caffe_model_path,&#x27;/VGG_FACE_deploy.prototxt&#x27;);model_dir = strcat(caffe_model_path,&#x27;/VGG_FACE.caffemodel&#x27;);Net=caffe.Net(prototxt_dir,model_dir,&#x27;test&#x27;); &emsp;&emsp;其中caffe_model_path是放置模型文件的路径。模型加载完后需要读取图像并设置检测参数： 123456img = imread([PathName FileName]]); %读取图像im_size = size(img(:,:,1)); %获取图像大小minsize=floor(min(im_size)/10); %设置最小尺寸expand_rate = 1.2; %设置扩展率imshow(img) %显示图片hold on %hold住啊 &emsp;&emsp;其中最小尺寸指的是MTCNN最小检测尺寸，我们设置为图像最小边的十分之一。它表示若图像中存在很小的人脸（尺寸小于整个图像的十分之一），那么我们便忽略它。而设置扩展率是由于MTCNN截的边界框往往比人脸略小，我们希望输入VGG-Net的图像是完整的人脸，因此需要将边界框扩展一些。接下来将图像输入MTCNN中得到人脸的边界框，并进行适当的调整： 123[box, ~]=detect_face(img,minsize,PNet,RNet,ONet,LNet,threshold,false,factor); %检测人脸得到边界框boudingboxes=scale_box(box,expand_rate,im_size); %调整边界框numbox=size(boudingboxes,1); %边界框数目 &emsp;&emsp;其中detect_face函数是MTCNN官方定义的函数，它的输出是图像中人脸的边界框以及特征点。在这里我们不需要使用特征点，因此用～替代。scale_box 函数的作用是将变量box扩展并转换为可操作的数据结构。之后使用循环对每一个人脸进行裁剪，识别，并将它们的特征进行储存： 123456789101112131415featurelist = []; %创建特征列表namelist = []; %创建人名列表for i=1:numbox rectangle(&#x27;Position&#x27;,boudingboxes(i,:),&#x27;Edgecolor&#x27;,&#x27;g&#x27;,&#x27;LineWidth&#x27;,3); %在图像上显示边界框 face = cut_face(img,boudingboxes(i,:)); %裁剪人脸 face = prepare_image(face); %图像数据预处理 feature = Net.forward(&#123;face&#125;); %输入到VGG-Face中得到人脸特征 [name,prob] = recognize_face(feature); %通过自定义softmax层检测人脸 if strcmp(name,&#x27;no data&#x27;) %若人脸被检测为未在数据文件中，则设置为&#x27;???&#x27; name = &#x27;???&#x27; end text(double(boudingboxes(i,1)),double(boudingboxes(i,2))+10,[num2str(i) &#x27;. &#x27; name],&#x27;Color&#x27;,&#x27;r&#x27;,&#x27;FontSize&#x27;,15); %将人名显示在图像中 featurelist = [featurelist feature&#123;1,1&#125;] %将人脸特征加入特征列表，至于为什么是feature&#123;1,1&#125;可以参照输出定义 namelist = [namelist &#123;name&#125;]; %将人名加入人名列表end &emsp;&emsp;这里要注意的是，由于在大家在训练模型的时候通常会对数据进行正规化（normalization），因此在使用模型时亦需要对输入数据进行相同处理，即预处理。这里使用的是prepare_image函数，其定义如下： 12345678function img = prepare_image(img) img = single(img); averageImage = [129.1863,104.7624,93.5940]; %VGG-Face模型正规化参数 img = cat(3,img(:,:,1)-averageImage(1),img(:,:,2)-averageImage(2),img(:,:,3)-averageImage(3)); img = img(:, :, [3, 2, 1]); % convert from RGB to BGR img = permute(img, [2, 1, 3]); % permute width and heightend &emsp;&emsp;利用自定义softmax层识别人脸所使用的是recognize_face函数。其定义如下： 123456789101112131415function [name,prob] = recognize_face(feature) load([data_path &#x27;/face_data.mat&#x27;]); %加载数据文件，data_path为文件所在路径 if isempty(face_data&#123;1,3&#125;) %若身份数据为空，则输出&#x27;no data&#x27; name=&#x27;no data&#x27;; prob=0; else output = fc_forward(feature&#123;1,1&#125;,face_data); %输入softmax层得到输出概率 [prob,num]=max(output); %得到概率最大相应概率及身份 if prob &lt; 0.6 name=&#x27;no data&#x27;; %若概率低于识别阈值0.6，则name等于‘no data’ else name=face_data&#123;1,3&#125;&#123;num,1&#125;; %否则name等于身份 end endend &emsp;&emsp;以上便完成了输入图像并检测识别的过程。若需要修改身份并训练，则可以在交互界面修改，此时会更改namelist中相应的项。再使用新的身份列表和特征列表训练自定义softmax层。其实现代码如下： 123456789n = size(namelist,2);if n &gt; 0 for i = 1 : n if ~strcmp(namelist&#123;1,i&#125;, &#x27;???&#x27;) face_add(namelist&#123;1,i&#125;,&#123;featurelist(:,i)&#125;) end end fc_train();end &emsp;&emsp;其中face_add函数将身份与对应特征加入数据文件中，其实现代码如下： 1234567891011121314function face_add(name,feature) load([data_path &#x27;\\face_data.mat&#x27;]); %加载数据文件 [p,n]=ismember(name,face_data&#123;1,3&#125;); %判断身份是否在库内 if ~p(1,1) %若人名未在库内 face_data&#123;1,1&#125;=[face_data&#123;1,1&#125;;(rand(1, 4096)-0.5)*2*sqrt(6 / 5000)]; %增加相应权重（weight）并初始化 face_data&#123;1,2&#125;=[face_data&#123;1,2&#125;;0]; %增加相应偏差（bias）并初始化 face_data&#123;1,3&#125;=[face_data&#123;1,3&#125;;&#123;name&#125;]; %增加相应身份 face_data&#123;1,4&#125;=[face_data&#123;1,4&#125;;&#123;feature&#123;1,1&#125;&#x27;&#125;]; %增加相应特征 else %否则 face_data&#123;1,4&#125;&#123;n,1&#125;=[face_data&#123;1,4&#125;&#123;n,1&#125;;feature&#123;1,1&#125;&#x27;]; %增加相应身份的特征 end save([data_path &#x27;\\face_data.mat&#x27;],&#x27;face_data&#x27;); %保存数据文件end &emsp;&emsp;训练自定义softmax层用的是fc_train函数，其实现代码如下： 1234567891011121314151617181920212223242526function fc_train() load([data_path &#x27;/face_data.mat&#x27;]); %加载数据文件 iteration=800; %设置迭代次数 n=size(face_data&#123;1,1&#125;,1); %得到身份总数 train_x=cell2mat(face_data&#123;1,4&#125;); %得到特征，设为训练数据输入 train_x=train_x&#x27;; train_y=[]; L=[]; for i=1:n %构造y的one-hot向量 temp_y=zeros(n,size(face_data&#123;1,4&#125;&#123;i,1&#125;,1)); temp_y(i,:)=1; train_y=[train_y temp_y]; end for i=1:iteration output=fc_forward(train_x,face_data); %前向传播得到输出 L=[L 1/2 * sum(sum((output-train_y) .^ 2)) / size((output-train_y), 2)]; %求出损失值 face_data=fc_backward(output,train_x,train_y,face_data); %反向传播更新权重 if mod(i,10) == 0 disp([&#x27;process:&#x27; num2str(i) &#x27;/&#x27; num2str(iteration)]); %输出训练进度 end end clf; plot(L); %画出损失函数曲线 save([data_path &#x27;/face_data.mat&#x27;],&#x27;face_data&#x27;); %储存数据文件end &emsp;&emsp;更新数据文件其中前向传播与反向传播均自己定义，代码如下： 123function output = fc_forward(train_x,face_data) output = sigm(face_data&#123;1,1&#125;*train_x+repmat(face_data&#123;1,2&#125;,1,size(train_x,2)));end 12345678910function face_data=fc_backward(output,train_x,train_y,face_data) l_rate=0.005; %设定学习率（learning rate） reg=0.001; %设定正则化参数（regularization） err=output-train_y; %输出与真实值偏差 doutput=err .* (output .* (1 - output)); %反向传播 dW=doutput * train_x&#x27; / size(doutput, 2) + reg*face_data&#123;1,1&#125;; db=mean(doutput, 2); face_data&#123;1,1&#125; = face_data&#123;1,1&#125; - l_rate * dW; %更新权重 face_data&#123;1,2&#125; = face_data&#123;1,2&#125; - l_rate * db; %更新偏差end &emsp;&emsp;更新数据文件后我们便能用新的权重来识别了。 测试&emsp;&emsp;整个交互界面如下图所示：&emsp;&emsp;Open Image按钮选择并显示图像；Recognize按钮识别并将识别的身份显示在右侧列表内；Change按钮更改列表中人名；Save按钮训练并保存数据文件；DataClear按钮清除数据文件。测试时我们先清楚数据文件，之后使用一张图片作为训练数据对网络最后一层进行训练。训练图片如下：&emsp;&emsp;训练完毕后使用另一张测试图片识别：&emsp;&emsp;使用摄像头识别： &emsp;&emsp;效果还不错啦。 ### 不足与改进 &emsp;&emsp;可以说这是我深度学习第一个应用的项目，当时并不知道迁移学习的概念，只是单纯发现自己电脑跑不动深度网络，无奈之下采取了这种方式。整个项目还是有很多不足的地方，比如之前在设计识别阈值时候纠结了半天，并且效果还不好。而其实softmax的输出并不能完全当作概率来看，每次增加新身份后它的输出值都会相应下降。正确的做法是在已有身份之外再设置一个\"unknown\"身份，若输出向量中\"unknown\"数值最大则视为人脸未识别。 &emsp;&emsp;另外就是交互功能不完善，纯粹是为了交互而交互。不过当初设计也就是为了好玩，是肯定不能应用到实际中的。 ### 小结 1. 迁移学习是将已经训练好的网络迁移到新任务中使用的方法，可以大大提高开发效率。 2. 迁移学习需要新旧任务有关联性。 3. 迁移学习应用举例。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://haosutopia.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"深度学习","slug":"机器学习/深度学习","permalink":"https://haosutopia.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"迁移学习","slug":"迁移学习","permalink":"https://haosutopia.github.io/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"},{"name":"CNN","slug":"CNN","permalink":"https://haosutopia.github.io/tags/CNN/"},{"name":"Caffe","slug":"Caffe","permalink":"https://haosutopia.github.io/tags/Caffe/"}]},{"title":"分布参数系统控制（二）：建立模型","slug":"RSVP-02","date":"2018-03-01T19:42:16.000Z","updated":"2018-04-11T21:23:14.000Z","comments":true,"path":"2018/03/RSVP-02/","link":"","permalink":"https://haosutopia.github.io/2018/03/RSVP-02/","excerpt":"","text":"&emsp;&emsp;我们在分析一个实际系统前往往需要将它转化为抽象的模型，再通过数学上的方法进行分析。而这其中往往涉及到一些物理化学方面的知识，因此本次学习不是分布参数系统的重点。本次学习将以导热棒为例，使我们对分布参数系统建模有一个初步的印象。 概述&emsp;&emsp;不管是什么系统，对其进行建模的方法都类似：通过系统中各个物理量的相互作用来建立相应的平衡方程，再对这些方程施加一些外部条件便能求得系统的状态变量随时间变化函数。在分布参数系统中，状态变量是函数，而这些函数通常都有着相同的定义域，比如区间$(0,1)$。通过物理平衡方程便可以对区间内的函数值进行描述。然而这时我们发现，区间边界（比如说0和1）的函数值无法通过这个方式确定。我们需要对区间的边界进行限制，来得到边界的函数值，因此就引入了边界条件（Randbedingungen）。同时每个微分方程需要有一个初始值，因此引入了初始条件（Anfangsbedingungen）。通过偏微分方程和边界以及初始条件，我们便能将整个系统确定下来，并能够求解系统的输出。 建立系统方程&emsp;&emsp;导热棒系统是最简单的一阶分布参数系统。我们假设导热棒长度为$L$，质量为$m$，密度为$\\rho$，比热容为$c$。棒上$z$点的温度随时间变化函数为$T(z,t)$，其中$z=(z_1,z_2,z_3)$为棒上任意一点，$z_1,z_2,z_3$分别表示长宽高方向上的坐标，且$t\\geq0$。&emsp;&emsp;取导热棒上其中一个小段$\\Delta\\Omega$，其无穷小体积单元$d\\Omega$上的热量可以用比热容定义得到：$${\\rm d}Q=c\\cdot T\\cdot {\\rm d}m\\tag{1}$$ &emsp;&emsp;其中质量${\\rm d}m$可以通过体积与密度得到：$${\\rm d}m=\\rho \\cdot {\\rm d}\\Omega\\tag{2}$$ &emsp;&emsp;将式(1)与式(2)合并得到热量与温度的微分表达式：$${\\rm d}Q=cT{\\rm d}m=c\\rho T{\\rm d}\\Omega \\tag{3}$$ &emsp;&emsp;将式(3)对体积积分便可以得到该小段$\\Delta \\Omega$的热量为：$$Q(t)=\\int_{\\Delta \\Omega}c\\rho T(z,t){\\rm d}\\Omega\\tag{4}$$ &emsp;&emsp;由热力学知识可知，物质热量的变化量等于物质自身产热加上周围物质传递的热能，而热能传递又可分为物质传递以及能量传递。因此使用公式对热量变化进行描述便可以得到系统的微分方程。&emsp;&emsp;热能的能量传递总是由温度高的物体通过表面接触传递给温度低的，并且温度相差越大，能量传递越快。因此能量传递有方向，是一个矢量。它能通过下式描述：$$\\vec{j_L}=-\\lambda \\cdot {\\rm grad}T(z,t)\\tag{5}$$ &emsp;&emsp;其中$\\vec{j_L}$为热流密度，它代表单位时间单位面积上所传递的热量及方向，$\\lambda$为材料的导热系数，${\\rm grad}T(z,t)$为温度在$z$上的梯度。&emsp;&emsp;物质传递也是矢量，它可以通过下式描述：$$\\vec{j_K}=c\\rho T(z,t)\\cdot \\vec{v}(z,t)\\tag{6}$$ &emsp;&emsp;其中$\\vec{j_K}$与$\\vec{j_L}$类似，它代表单位是间单位面积通过物质传递所传递的能量，$c\\rho T(z,t)$为单位物质能量，$\\vec{v}(z,t)$为棒上$z$点单位面积物质的传输速度及方向。&emsp;&emsp;物质自身产热可以通过$u_\\Omega(z,t)$描述，它表示棒上$z$点单位时间所产生的热量。&emsp;&emsp;将所有表达式合起来便得到了该小段$\\Delta \\Omega$上的平衡方程：$${\\rm d\\over\\rm d\\mathcal t} \\int_{\\Delta \\Omega}c\\rho T(z,t){\\rm d}\\Omega =-\\oint_{\\Delta \\Gamma}(\\vec{j_L}(z,t)+\\vec{j_K}(z,t)){\\rm d}\\vec{\\Gamma}+\\int_{\\Delta \\Omega}u_{\\Omega}(z,t){\\rm d}\\Omega\\tag{7}$$ &emsp;&emsp;其中$\\Delta \\Gamma$表示$\\Delta \\Omega$的包络面。式(7)表示$\\Delta \\Omega$的能量变化等于自身产热减去通过传递流出的热量（即热流密度在包络面的积分）。通过高斯定理可以将$\\vec{j_L}$与$\\vec{j_K}$在包络面上的面积分转化为其散度（${\\rm div}$）在包络面所围体积的体积分，即：$$\\oint_{\\Delta \\Gamma}\\vec{j}{\\rm d}\\vec{\\Gamma}=\\int_{\\Delta \\Omega}{\\rm div}\\vec{j}{\\rm d}\\Omega\\tag{8}$$ &emsp;&emsp;将式(8)代入式(7)得到：$$\\begin{split}{\\rm d\\over \\rm d\\mathcal t}\\int_{\\Delta \\Omega}c\\rho T(z,t){\\rm d}\\Omega&amp;=\\int_{\\Delta \\Omega}c\\rho {\\partial T(z,t)\\over \\partial t}{\\rm d}\\Omega\\&amp;=\\int_{\\Delta \\Omega}-{\\rm div}(\\vec{j_L}(z,t)+\\vec{j_K}(z,t))+u_{\\Omega}(z,t){\\rm d}\\Omega\\end{split}\\tag{9}$$ &emsp;&emsp;将式(9)积分去掉便得到了导热棒系统方程：$$c\\rho {\\partial T(z,t)\\over \\partial t}=-{\\rm div}(\\vec{j_L}(z,t)+\\vec{j_K}(z,t))+u_{\\Omega}(z,t)\\tag{10}$$ &emsp;&emsp;式(10)对导热棒上的点均成立。由于系统方程是对$T(z,t)$的方程，因此我们将式(5)(6)带入式(10)，得到：$$c\\rho {\\partial T(z,t)\\over \\partial t}=\\lambda {\\rm div\\ grad}T(z,t)-c\\rho {\\rm div}(\\vec{v}(z,t)T(z,t))+u_{\\Omega}(z,t)\\tag{11}$$ &emsp;&emsp;在实际应用中，导热棒的宽和高相比它的长度可以忽略不计。因此我们只保留长度方向坐标$z_1$，并将其表示为$z$，其中：$${\\rm div\\ grad}=\\partial_{z_1}^2+\\partial_{z_2}^2+\\partial_{z_3}^2\\Rightarrow\\partial_{z_1}^2\\Rightarrow\\partial_z^2\\tag{12}$$ &emsp;&emsp;以及：$${\\rm div}\\Rightarrow\\partial_{z_1}\\Rightarrow\\partial_z\\tag{13}$$ &emsp;&emsp;具体数学过程感兴趣的可以查阅相关资料。最终得到导热棒系统方程为：$$\\begin{split}c\\rho \\partial_t T(z,t)=&amp;\\lambda \\partial_z^2 T(z,t)-c\\rho \\partial_z(v(z,t)T(z,t))\\&amp;+u_{\\Omega}(z,t),\\ z\\in(0,L),\\ t&gt;0\\end{split}\\tag{14}$$ &emsp;&emsp;一般情况下，导热棒很少存在物质传递，因此通常满足$v(z,t)=0$。并且通过如下坐标变换可以将系统方程标准化：$$z={z\\over L},\\ t={\\lambda \\over c\\rho L^2}t\\tag{15}$$ &emsp;&emsp;标准化后的系统方程为：$$\\partial_t T(z,t)=\\partial_z^2 T(z,t)+u_{\\Omega}(z,t),\\ z\\in(0,1),\\ t&gt;0\\tag{16}$$ &emsp;&emsp;若$u_{\\Omega}(z,t)$是分布式输入（verteilter Eingriff），即输入分布在定义域的某个区间，则它可以表示为$b(z)u(t)$。$b(z)$代表自身产热的作用范围，$u(t)$代表自身产热的热量大小，在之后的学习中我们将控制导热棒的自身产热$u(t)$，使得系统达到想要的状态。式(16)可以写成：$$\\partial_t T(z,t)=\\partial_z^2 T(z,t)+b(z)u(t),\\ z\\in(0,1),\\ t&gt;0\\tag{17}$$ &emsp;&emsp;除了分布式输入外，还有点输入以及边界输入。点输入为在定义域某点$z_0$的输入，此时$b(z)$表示为$k\\cdot\\delta(z_0)$的形式。由于$\\delta$函数不属于希尔伯特空间，操作起来较为麻烦，因此在实际中通常使用脉冲函数来近似，近似后的输入就变成了分布式输入。边界输入将在后面章节介绍。 边界条件&emsp;&emsp;系统方程如式(17)，它是对区间$(0,1)$内所有点的描述，其中每个点的温度$T$都可以通过自身产热以及与周围点的热能传递得到。而对于边界点$T(0,t)$及$T(1,t)$来说，由于我们没有定义区间$(0,1)$以外环境的温度，无法通过系统方程描述它们，从而导致整个系统无法确定。因此需要引入边界条件来限制边界点$T(0,t)$和$T(1,t)$。一般来说，一阶系统的边界条件为：Dirichlet条件、Neumann条件和Robin条件。它们定义分别是：&emsp;&emsp;1. Dirichlet条件：$T(z_0,t)=T_{z_0}(t),\\ t&gt;0$&emsp;&emsp;2. Neumann条件：$\\partial_zT(z_0,t)=T_{z_0}(t),\\ t&gt;0$&emsp;&emsp;3. Robin条件：$\\partial_zT(z_0,t)+\\lambda T(z_0,t)=T_{z_0}(t),\\ t&gt;0$&emsp;&emsp;其中$z_0$为边界点，可取$0$或$1$，$T_{z_0}(t)$为已知函数。这三个条件分别确定了边界点的值（Dirichlet）、导数（Neumann）以及值与导数的关系（Robin）。在应用中它们均通过实际情况确定，并且不同边界点的边界条件可以不同。&emsp;&emsp;若系统输入为边界输入，则边界条件需要包含$u(t)$，它表示通过控制边界点的状态来控制系统。例如：$T(0,t)=u(t)$。 初始条件&emsp;&emsp;求解系统方程除了需要有系统描述，还需要有系统初始的状态。因此引入初始条件：$$T(z,0)=T_{0z}(z),\\ z\\in[0,1]\\tag{18}$$ &emsp;&emsp;在这里需要注意的是初始条件必须与边界条件一致。比方说边界条件为：$T(1,t)=0$，那么初始条件必须满足：$T_{0z}(1)=0$。 系统输出&emsp;&emsp;系统输出是由传感器测得的温度，它可以是棒上某点的温度，称为点测量（punktförmige Messung），也可以是某个区域上的温度，称为分布式测量（verteilte Messung）。它的表达式如下：$$y(t)=\\int_0^1c(z)T(z,t){\\rm d}z\\tag{19}$$ &emsp;&emsp;其中$c(z)$代表传感器作用范围。由于点测量会产生一些问题（点测量时，$c(z)$是冲激函数，不属于希尔伯特空间，无法写入系统方程中。同样的，我们也不使用点输入），接下来我们使用的都是分布式测量。 导热棒系统&emsp;&emsp;将系统方程、边界条件以及初始条件合起来，就可以求解出系统状态以及系统输出，下面是一个具体的例子：&emsp;&emsp;其中$b_1(z)$与$b_2(z)$分别表示$u_1(t)$与$u_2(t)$的作用范围，它们定义如下：$$b_1(z)=\\begin{cases}1:0.5\\leq z\\leq 0.6\\0:其他\\end{cases}\\tag{20}$$ &emsp;&emsp;以及$$b_1(z)=\\begin{cases}0.5:0.15\\leq z\\leq 0.25\\0.5:0.75\\leq z\\leq 0.85\\0:其他\\end{cases}\\tag{21}$$ &emsp;&emsp;将它们向量化后得到：$$b^T(z)=\\begin{bmatrix}b_1(z) &amp; b_2(z)\\end{bmatrix}\\tag{22}$$ &emsp;&emsp;则：$$b^T(z)u(t)=\\begin{bmatrix}b_1(z) &amp; b_2(z)\\end{bmatrix}\\begin{bmatrix}u_1(z) \\ u_2(z)\\end{bmatrix}\\tag{23}$$ &emsp;&emsp;$c_1(z)$与$c_2(z)$分别表示传感器测量范围，它们定义如下：$$c_1(z)=\\begin{cases}1:0.37\\leq z\\leq 0.43\\0:其他\\end{cases}\\tag{24}$$ &emsp;&emsp;以及$$c_2(z)=\\begin{cases}1:0.67\\leq z\\leq 0.73\\0:其他\\end{cases}\\tag{25}$$ &emsp;&emsp;将它们向量化后得到：$$c(z)=\\begin{bmatrix} c_1(z) \\ c_2(z)\\end{bmatrix}\\tag{26}$$ &emsp;&emsp;则：$$y(t)=\\int_0^1c(z)x(z,t){\\rm d}z=\\begin{bmatrix}\\int_0^1c_1(z)x(z,t){\\rm d}z \\ \\int_0^1c_2(z)x(z,t){\\rm d}z\\end{bmatrix}\\tag{27}$$ &emsp;&emsp;假设系统在边界点$z=0$及$z=1$的温度都为常值$0$，且系统状态在$t=0$的初始状态为$x(z,0)=x_0(z)$。则整个系统描述如下：$$\\begin{eqnarray}\\partial_tx(z,t)&amp;=&amp;\\partial_z^2x(z,t)+b^T(z)u(t),\\ t&gt;0,\\ z\\in(0,1)\\tag{28}\\x(0,t)&amp;=&amp;x(1,t)=0,\\ t&gt;0\\tag{29}\\x(z,0)&amp;=&amp;x_0(z),\\ z\\in[0,1]\\tag{30}\\y(t)&amp;=&amp;\\int_0^1c(z)x(z,t){\\rm d}z,\\ t\\geq 0\\tag{31}\\end{eqnarray}$$ &emsp;&emsp;至此我们就建立了一个导热棒的系统模型。若我们把$x(z,t)$看作状态变量，对$x(z,t)$的操作看成算子，那么就可以把系统模型抽象为更一般的状态方程形式。我们设系统算子为$\\mathcal{A}:X\\to X$$$\\mathcal{A}h={\\rm d}_z^2h,\\ h\\in D(\\mathcal{A})\\tag{32}$$ &emsp;&emsp;其中定义域$D(\\mathcal{A})$为：$$D(\\mathcal{A})={h\\in L_2(0,1)|h,{\\rm d}_zh绝对连续,{\\rm d}_z^2h\\in L_2(0,1),且h(0)=h(1)=0}$$ &emsp;&emsp;整个定义域可以分为两个部分。我们设状态变量所在的状态空间$X=L_2(0,1)$。由于$\\mathcal{A}$是空间$X\\to X$的线性算子，因此满足$\\mathcal{A}h\\in L_2(0,1)$，即${\\rm d}_z^2h\\in L_2(0,1)$。而函数二阶导存在且二次可积的条件为：函数及函数一阶导绝对连续。它们构成了定义域的第一部分。同时状态函数还需要满足边界条件，即$h(0)=h(1)=0$。它构成了定义域的第二部分。&emsp;&emsp;设输入算子（Eingangsoperator）为$\\mathcal{B}:\\Bbb{C}^p\\to X$$$\\mathcal{B}u=b^Tu=\\sum_{i=1}^{p}b_iu_i,\\ b_i\\in X,\\ u\\in {\\Bbb C}^p\\tag{33}$$ &emsp;&emsp;设输出算子（Ausgangsoperator）为$\\mathcal{C}:X\\to\\Bbb{C}^m$$$\\mathcal{C}x=\\int_0^1cx{\\rm d}z=\\begin{bmatrix}\\int_0^1c_1x{\\rm d}z \\ \\vdots \\ \\int_0^1c_mx{\\rm d}z \\end{bmatrix},\\ c_i\\in X,\\ x\\in X\\tag{34}$$ &emsp;&emsp;由于通常$c_i$与$x$均为实数，满足：$$\\int_0^1c_ix{\\rm d}z=\\int_0^1x\\overline{c_i}{\\rm d}z=\\langle x,c_i\\rangle\\tag{35}$$ &emsp;&emsp;因此式(34)可以化为：$$\\mathcal{C}x=\\begin{bmatrix} \\langle x,c_1\\rangle \\ \\vdots \\ \\langle x,c_m\\rangle \\end{bmatrix},\\ c_i\\in X,\\ x\\in X\\tag{36}$$ &emsp;&emsp;将以上定义的算子全部带入系统描述，可以得到分布参数系统的状态方程为：$$\\begin{eqnarray}\\dot{x}(t)&amp;=&amp;\\mathcal{A}x(t)+\\mathcal{B}u(t),\\ &amp;t&gt;0,\\ x(0)=x_0\\in X\\tag{37}\\y(t)&amp;=&amp;\\mathcal{C}x(t),\\ &amp;t\\geq0\\tag{38}\\end{eqnarray}$$ &emsp;&emsp;值得注意的是，在这里没有写出边界条件，是因为系统算子$\\mathcal{A}$的定义域$D(\\mathcal{A})$中已经将边界条件包含了。在之后的章节我们都将使用这种状态方程的形式对系统进行分析及求解。 小结 系统模型包含系统方程、边界条件、初始条件以及系统输出四个部分。 系统方程通过物理平衡方程得到。 一阶系统边界条件分为三种：Dirichlet条件、Neumann条件和Robin条件。 引入算子后，系统可以表示为状态方程的形式。","categories":[{"name":"控制理论","slug":"控制理论","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/"},{"name":"分布参数系统控制","slug":"控制理论/分布参数系统控制","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F%E6%8E%A7%E5%88%B6/"}],"tags":[{"name":"分布参数系统","slug":"分布参数系统","permalink":"https://haosutopia.github.io/tags/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F/"},{"name":"数学建模","slug":"数学建模","permalink":"https://haosutopia.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"}]},{"title":"分布参数系统控制（一）：数学基础","slug":"RSVP-01","date":"2018-01-29T00:55:08.000Z","updated":"2020-09-28T01:04:57.651Z","comments":true,"path":"2018/01/RSVP-01/","link":"","permalink":"https://haosutopia.github.io/2018/01/RSVP-01/","excerpt":"","text":"&emsp;&emsp;分布参数系统控制是控制理论的一个重要分支。相对于集总参数系统，分布参数系统使用分布参数的偏微分方程对系统进行描述，并且结果比集总参数系统要精确的多，但问题求解的复杂度也要高得多，可以说是相当难的一个方向。本次学习材料主要是RSVP的上课课件以及我们Deutscher教授编著的德语材料：Zustandsregelung verteilt-parametrischer Systeme（原来可以免费下载，现在好像要收费了，如果需要可以邮件我）。此外，本文将以理解和应用为主，因此一些涉及到数学的复杂证明会被省略，并且会尽量以通俗的语言进行描述。准备好了吗？开始咯。 分布参数系统概念&emsp;&emsp;分布参数是什么？它是一个与集总参数相对的概念。我们之前在自控原理以及现代控制中所学习的系统都是集总参数系统，它们的系统状态都能通过有限个状态参数$x_1,x_2,…,x_n$描述。这有限个状态就是所谓的集总参数，我们通过这些状态的微分方程来分析并且控制整个系统。而现实中大多数系统是无法用有限个状态参数来表示的。例如一个长为$L$的导热棒，棒上每个位置的温度都可以是不同的，因此我们无法使用有限个位置上的温度作为状态来描述整个系统，只有将区间$[0,L]$上所有位置的温度都作为状态参数，而这样的位置由无限多个，于是我们便得到一个无限维的状态参数，它可以写成温度相对于位置的函数：$T(z)$，其中$z\\in[0,L]$，这就是分布参数。若导热棒温度随时间变化，它的状态参数便也是$t$的函数，表示为$T(z,t)$。&emsp;&emsp;分布参数系统就是以分布参数作为状态参数的系统，我们能通过这些状态的偏微分方程对系统进行分析控制。相比于集总参数系统，分布参数系统将有限维参数变成了无限维参数，将状态向量空间变成了函数空间，将对向量进行变换的矩阵变成了对函数进行操作的算子。而函数空间及算子均属于泛函分析范畴，在本文之后的部分会进行讨论。 控制器设计&emsp;&emsp;分布参数系统是无限维的，但在控制实现时无法直接使用无限维进行操作。因此当我们设计控制器时，需要一个近似的步骤：将无限维近似为有限维。在这里我们有两种近似的过程，分别是：&emsp;&emsp;early-lumping：先将系统过程（Strecke）进行近似，转化为有限维的集总参数系统，再通过现代控制理论相关方法设计控制器。该方法相对容易，但精确度不高，会丢失很多原有的系统信息。&emsp;&emsp;late-lumping：先通过分布参数系统的控制方法设计相应的控制器，再通过对控制器进行近似，来实现对系统的控制。该方法精确度高，但设计过程十分麻烦。&emsp;&emsp;之后的文章我们将分别使用这两种方法对过程进行控制。 数学基础&emsp;&emsp;分布参数系统的计算涉及到了很多泛函以及算子方面的知识。因此接下来将对涉及到的部分数学进行介绍。 函数空间&emsp;&emsp;空间这个概念在数学上是指一种具有特殊性质以及一些额外结构的集合。通俗来说，空间包含两个部分：元素和规则。以一般我们所用到的线性空间为例，它的元素就是向量，而它的规则就是空间中定义的距离，范数，内积，线性结构等等。关于各种空间的定义以及关系，将来有机会会再写一篇文章来解释。&emsp;&emsp;对于函数空间来说，它的元素就是满足一定条件的函数，比如说满足定义域为0到1的所有函数。而函数与函数之间也可以有距离，角度等等规则。在分布参数系统中，我们通常使用的是函数空间中的$L_2$空间。 $L_2$空间&emsp;&emsp;$L_2$空间为以$s$为定义域的所有二次可积函数组成的空间，它是一个函数空间。所谓二次可积是指函数绝对值二次方在定义域上为有限值。其数学表达式为：$$L_2={f:s\\to{\\Bbb{C}}|\\int_s|f(z)|^2dz&lt;\\infty}\\tag{1}$$ &emsp;&emsp;其中$s$为函数的定义域，${\\Bbb{C}}为函数域。$$L_2$空间是希尔伯特空间（完备的内积空间），也就是说可以在$L_2$空间中定义内积。其内积的具体形式为：$$\\langle x,y\\rangle=\\int_sx(z)\\overline{y(z)}dz\\tag{2}$$ &emsp;&emsp;通常现实中的应用（如导热棒上的温度）都是二次可积的。因此在分布参数系统中我们通常选取$L_2$空间为系统的状态空间。 算子&emsp;&emsp;算子（Operator）的定义为：从一个空间到另一个空间的映射。我们可以把它当作一种操作或者运算。当空间的某个元素经过算子的操作，就会映射为另一个空间的另一个元素。在线性空间中，算子的表现形式为矩阵，它的本质就是有限维空间的线性变换。而到了函数空间中，算子就是无限维空间的变换，将函数从一个函数空间映射到另一个函数空间。举个例子，我们学控制理论最熟悉的就是拉普拉斯算子，它的作用就是将信号函数从一个空间（时域）转换到了另一个空间（频域）。而通常我们对函数进行的操作如微分、积分等等都可以看作算子（微分算子、积分算子）。在本次分布参数系统学习中使用最多的例子是导热棒，它的状态方程可以写成：$$\\partial_tx(z,t)=\\partial_z^2x(z,t)+b^T(z)u(t),\\ t&gt;0,\\ z\\in[0,1]\\tag{3}$$ &emsp;&emsp;其中$\\partial_z^2$是对$z$的二阶偏微分算子，$b^T(z)$也可以看作一个算子，将它们分别设为$\\cal{A}$和$\\cal{B}$。因此式(3)就可以化成与集总参数系统中的状态方程类似的形式：$$\\dot{x}(t)=\\mathcal{A}x(t)+\\mathcal{B} u(t), t&gt;0\\tag{4}$$ 线性算子&emsp;&emsp;在分布参数系统中我们所使用的算子都是线性算子。顾名思义，线性算子就是满足线性运算条件的算子。它在将函数从一个函数空间转换到另一个函数空间时保持加法运算和数乘运算。 线性算子的谱集&emsp;&emsp;在线性空间中，一个矩阵最重要的性质就是特征值与特征根，同时它们也是系统稳定性的重要判断依据。因此作为矩阵的扩展，特征值在线性算子中的表现形式就是谱集。谱这个说法来自于光学，用来表示光谱的现象，而后在量子力学中使用线性算子来描述这些现象，因此谱的说法也被沿用下来了。&emsp;&emsp;设$\\mathcal{A}$为空间$\\mathcal{H}\\to\\mathcal{H}$的线性算子。同矩阵一样，算子谱集的计算方式为：$$\\mathcal{A}x=\\lambda x\\tag{5}$$ &emsp;&emsp;由上式可以得到计算方程：$$(\\lambda I-\\mathcal{A})x=0\\tag{6}$$ &emsp;&emsp;虽然求解的表达式与矩阵一致，但使用算子求解得到的$\\lambda$值并不像矩阵那样是有限个数值。由于函数空间是无限维的，因此算子谱集中的$\\lambda$值会有无穷多个。我们可以通过$(\\lambda I-\\mathcal{A})$的性质对$\\lambda$在复平面上进行分类：&emsp;&emsp;预解集：也叫正则集，表示为$\\rho(\\mathcal{A})$。定义：满足以下条件的$\\lambda$称为正则点：&emsp;&emsp;1. $(\\lambda I-\\mathcal{A})$的值域在$\\mathcal{H}$中稠密。&emsp;&emsp;2. $(\\lambda I-\\mathcal{A})$有连续的逆算子。&emsp;&emsp;这样的$\\lambda$的全体称为$\\mathcal{A}$的预解集。&emsp;&emsp;谱集：也叫近似点谱，表示为$\\sigma(\\mathcal{A})$。定义：预解集$\\rho(\\mathcal{A})$的补集称为$\\mathcal{A}$的谱集。即：$$\\sigma(\\mathcal{A})=\\Bbb{C}\\backslash\\rho(\\mathcal{A})\\tag{7}$$ &emsp;&emsp;若$\\lambda\\in\\sigma(\\mathcal{A})$，则称$\\lambda$为$\\mathcal{A}$的谱点。&emsp;&emsp;由此可以看出，通过$(\\lambda I-\\mathcal{A})$的性质我们将$\\lambda$的值域分成了预解集和谱集。同时我们还可以对谱集进行进一步的分类：&emsp;&emsp;点谱：定义：满足以下条件的$\\lambda$称为$\\mathcal{A}$的点谱：&emsp;&emsp;1. $\\lambda I-\\mathcal{A}$的映射不是一一对应的&emsp;&emsp;这些$\\lambda$在复平面上是一个个离散的点，因此称它们为点谱。我们将点谱的全体记为$\\sigma_p(A)$。由于$\\lambda I-\\mathcal{A}$的映射不是一一对应的，因此式(6)在空间$\\mathcal{H}$中存在非0解，此时的$\\lambda$就是算子的特征值，而式(6)求得的非0解也就是特征元素（相当于矩阵中的特征向量）。&emsp;&emsp;连续谱：定义：满足以下条件的$\\lambda$称为$\\mathcal{A}$的连续谱：&emsp;&emsp;1. $\\lambda I-\\mathcal{A}$的映射是一一对应的，且$\\lambda I-\\mathcal{A}$的值域在$\\mathcal{H}$中稠密。&emsp;&emsp;2. $\\lambda I-\\mathcal{A}$的逆算子是不连续的。&emsp;&emsp;这些$\\lambda$在复平面上常常是连续的，因此称它们为连续谱。我们将连续谱的全体记为$\\sigma_c(\\mathcal{A})$。&emsp;&emsp;剩余谱：定义：满足以下条件的$\\lambda$称为$\\mathcal{A}$的剩余谱：&emsp;&emsp;1. $\\lambda I-\\mathcal{A}$的映射是一一对应的，但$\\lambda I-\\mathcal{A}$的值域在$\\mathcal{H}$中不稠密。&emsp;&emsp;由于其值域不稠密，在$\\mathcal{H}$空间中将存在一部分无法被值域覆盖，相当于存在剩余的空间，因此称这些$\\lambda$为剩余谱。我们将剩余谱的全体记为$\\sigma_r(\\mathcal{A})$。&emsp;&emsp;由点谱、连续谱以及剩余谱的定义我们可以发现，$\\sigma_p(\\mathcal{A})$、$\\sigma_c(\\mathcal{A})$和$\\sigma_r(\\mathcal{A})$是互不相交的集合，并且它们的并集为$\\sigma(\\mathcal{A})$。在有限维空间中，矩阵的谱集全部都是点谱。而在无限维空间中，算子的谱集就可能会有连续谱以及剩余谱。下面以右移算子为例：&emsp;&emsp;在希尔伯特空间$\\mathcal{H}=l^2[1,\\infty)$上，我们定义右移算子$\\mathcal{T}:l^2\\to l^2$,$$\\mathcal{T}:(x_1,x_2,…)\\to (0,x_1,x_2,…)\\tag{8}$$ &emsp;&emsp;当$\\lambda=0$时：$$\\lambda I-\\mathcal{T}=-\\mathcal{T}\\tag{9}$$ &emsp;&emsp;我们可以发现$\\lambda I-\\mathcal{T}$的映射是一一对应的，但由于映射后的第一个数始终为0，因此$\\lambda I-\\mathcal{T}$的值域在$\\mathcal{H}$中不稠密，故$\\lambda=0$为剩余谱。&emsp;&emsp;在本次学习中我们仅讨论一些特殊结构的算子：Riesz-Spektral算子和Sturm-Liouville算子。它们的谱集只有点谱以及点谱的聚点（属于连续谱）。因此通过它们求得的特征元素${\\phi_i,i\\geq1}$能够作为整个函数空间$\\mathcal{H}$的基，从而方便我们对系统进行分析。 伴随算子&emsp;&emsp;伴随算子（Adjungiert Operator）是算子理论中的一个重要概念，它的定义由空间的内积得到：&emsp;&emsp;设$x,y\\in \\mathcal{H}$，线性算子$\\mathcal{A},\\mathcal{B}$满足：$$\\langle\\mathcal{A}x,y\\rangle=\\langle x,\\mathcal{B}y\\rangle\\tag{10}$$ &emsp;&emsp;则称$\\mathcal{B}$为$\\mathcal{A}$的伴随算子，记作$\\mathcal{A}^\\ast$。若$\\mathcal{A}=\\mathcal{A}^\\ast$，则称$\\mathcal{A}$为自伴随算子（Selbstadjungiert Operator）。&emsp;&emsp;在有限维中，设$x,y\\in\\Bbb{C}^n$，则矩阵$A$的伴随矩阵可由下列等式得出：$$\\langle Ax,y\\rangle=(Ax)^T\\bar{y}=x^TA^T\\bar{y}=x^T\\overline{\\overline{A^T}y}=\\langle x,\\overline{A^T}y\\rangle\\tag{11}$$ &emsp;&emsp;因此$A$的伴随矩阵为$A^\\ast=\\overline{A^T}$。&emsp;&emsp;若$\\mathcal{A}$的特征元素${\\phi_i,i\\geq1}$为$\\mathcal{H}$的一组基，则其伴随矩阵$\\mathcal{A}^\\ast$的特征元素${\\psi_i,i\\geq1}$也为$\\mathcal{H}$的基，并且将它们标准化后满足： $$\\langle\\phi_i,\\psi_i\\rangle=\\delta_{ij}\\begin{cases}0,&amp;i\\not=j\\1,&amp;i=j\\end{cases}\\tag{12}$$ &emsp;&emsp;由于$\\mathcal{A}$的特征元素${\\phi_i,i\\geq1}$是$\\mathcal{H}$的基，因此空间$\\mathcal{H}$中任意元素$\\xi$均可以通过$\\phi_i$的无穷线性组合表示，其系数就可以利用式(12)进行计算： $$\\xi=\\sum_{i=1}^{\\infty}\\langle\\xi,\\psi_i\\rangle\\phi_i\\tag{13}$$ Riesz-Spektral算子&emsp;&emsp;Riesz-Spektral算子（Riesz-Spektraloperator）是一种特殊的算子。设$\\mathcal{A}$是希尔伯特空间$\\mathcal{H}$上的一个线性算子，并且满足：&emsp;&emsp;1. $\\mathcal{A}$是一个闭算子。&emsp;&emsp;2. $\\mathcal{A}$的特征值${\\lambda_i,i\\geq1}$只有单根。&emsp;&emsp;3. $\\mathcal{A}$的特征元素${\\phi_i,i\\geq1}$为空间$\\mathcal{H}$的一组基，称为Riesz基（Riesz-Basis）。&emsp;&emsp;4. $\\mathcal{A}$特征值的闭包$\\overline{{\\lambda_i,i\\geq1}}$是完全不相关的（vollständig unzusammenhängend），也就是说$\\overline{{\\lambda_i,i\\geq1}}$中没有连续的部分。&emsp;&emsp;则称$\\mathcal{A}$为一个Riesz-Spektral算子。&emsp;&emsp;通过上述定义可以看出：条件一表示算子定义域所在的空间是完备的，也就是说空间中所有柯西数列的极限仍在空间内。条件四限制了谱的连续性，虽然特征值的聚点属于连续谱，但实际上它们并不连续，因此空间中任意一点可以使用特征元素的累加得到（若存在连续的连续谱，那么在累加之后还需要增加积分才能表示空间中的元素。因此条件四的作用是将积分项剔除了）。&emsp;&emsp;以上仅给出了Riesz-Spektral算子满足的条件，但当我们得到一个算子并不能马上判断它是不是Riesz-Spektral算子。因此下面我们介绍相对更实用的算子：Sturm-Liouville算子。 Sturm-Liouville算子&emsp;&emsp;Sturm-Liouville算子（Sturm-Liouville-Operator）是Riesz-Spektral算子的一个子集。当算子$\\mathcal{L}$满足：$$\\begin{eqnarray}&amp;&amp;\\mathcal{L}h= {1\\over w}\\left(-{d\\over dz}\\left(p{d\\over dz}h\\right)+qh\\right)\\tag{14}\\&amp;&amp;h\\in D(\\mathcal{L})={h\\in L_2(0,1)|h,{d\\over dz}h\\text{绝对连续},\\tag{15}\\&amp;&amp;{d^2\\over dz^2}h\\in L_2(0,1)\\text{且}\\beta_1h(0)+\\gamma_1{dh\\over dz}(0)=0,\\beta_2h(1)+\\gamma_2{dh\\over dz}(1)=0}\\end{eqnarray}$$ &emsp;&emsp;且在希尔伯特空间$L_2(0,1)$存在内积：$$\\langle h_1,h_2 \\rangle=\\int_0^1h_1(z)\\overline{h_2(z)}w(z)dz\\tag{16}$$ &emsp;&emsp;并且参数满足：&emsp;&emsp;1. $w(z),p(z),{dp\\over dz}(z)$与$q(z)$都是在区间$[0,1]$上的实值连续函数。&emsp;&emsp;2. $p(z)&gt;0$且$w(z)&gt;0$。&emsp;&emsp;3. $\\beta_1,\\beta_2,\\gamma_1,\\gamma_2$均为实常数，且$|\\beta_1|+|\\gamma_1|&gt;0$，$|\\beta_2|+|\\gamma_2|&gt;0$成立。&emsp;&emsp;此时我们称$\\mathcal{L}$为一个Sturm-Liouville算子。&emsp;&emsp;这个定义可以说是相当复杂了，但实际运用中我们可能只会用到一些简单的情况。我们可以将$w(z),p(z),q(z)$都设为常数，并且使得$w=1$，$p&gt;0$，$q$任意取值。此时式(14)就可以化为如下的简单形式： $$\\mathcal{L}h=-p{d^2\\over dz^2}h+qh\\tag{17}$$ &emsp;&emsp;并且内积也变成了我们熟悉的内积形式：$$\\langle h_1,h_2\\rangle = \\int_0^1h_1(z)\\overline{h_2(z)}dz\\tag{18}$$ &emsp;&emsp;Sturm-Liouville算子作为Riesz-Spektral算子的特殊情况，拥有着更好的性质。若$\\mathcal{L}$在空间$\\mathcal{H}$是一个Sturm-Liouville算子，那么它就满足如下性质：&emsp;&emsp;1. $\\mathcal{L}$在式(16)所定义的内积下是自伴随算子。&emsp;&emsp;2. $\\mathcal{L}$的谱集由一系列孤立单根的实特征值组成，并且只有有限个特征值在复平面的左半平面。&emsp;&emsp;3. $\\mathcal{L}$的特征元素在经过标准化后是一组空间$\\mathcal{H}$的正交基。&emsp;&emsp;可以发现满足这些性质的Sturm-Liouville算子就和线性空间的矩阵差不多了，我们可以像矩阵一样非常方便的求得特征根与特征元素，并且以此为基来分析整个系统。关于如何计算特征值与特征元素，之后的文章将会举例说明。同时它的第二条性质对我们将来的稳定性判断非常有用。根据性质二，$-\\mathcal{L}$只有有限个特征值在复平面的右半平面，那么我们就能通过反馈将这有限个右半平面的特征值移动到左半平面，从而使得系统稳定。&emsp;&emsp;当我们在实际运用时，只需要将遇到的算子与式(14)或式(17)进行比对，若满足就可以说该算子是一个Sturm-Liouville算子，并且满足其所有性质。 小结 分布参数将集总参数有限维的向量空间扩展为无限维的函数空间，将矩阵扩展为算子。但分布参数系统的状态方程仍可以使用集总参数系统类似的形式表示。 算子是一种空间到空间的映射。 矩阵的特征值扩展到线性算子就是谱集，通常用它来表示线性算子的特性。 可以通过伴随算子$\\mathcal{A}^\\ast$的基$\\psi_i$来计算空间元素分解后在$\\mathcal{A}$的基$\\phi_i$前的系数。 Riesz-Spektral算子与Sturm-Liouville算子为特殊算子，它们拥有良好的性质。","categories":[{"name":"控制理论","slug":"控制理论","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/"},{"name":"分布参数系统控制","slug":"控制理论/分布参数系统控制","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F%E6%8E%A7%E5%88%B6/"}],"tags":[{"name":"分布参数系统","slug":"分布参数系统","permalink":"https://haosutopia.github.io/tags/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F/"},{"name":"算子理论","slug":"算子理论","permalink":"https://haosutopia.github.io/tags/%E7%AE%97%E5%AD%90%E7%90%86%E8%AE%BA/"}]},{"title":"Backstepping（反步控制）","slug":"Backstepping-01","date":"2018-01-14T01:20:16.000Z","updated":"2018-03-01T13:46:40.000Z","comments":true,"path":"2018/01/Backstepping-01/","link":"","permalink":"https://haosutopia.github.io/2018/01/Backstepping-01/","excerpt":"","text":"&emsp;&emsp;Backstepping（反步控制）是一种设计非线性系统控制器的方法。它通过对子系统的递归方便地求得能使系统稳定的输入。但大多数相关文献中并未考虑系统本身的可控性，因此首先我们假设需要控制的系统是可控的。 基本思想&emsp;&emsp;Backstepping针对的是能写成如下形式的系统：$$\\begin{cases}\\dot{x}_1=x_2+f_1(x_1)\\\\dot{x}_2=x_3+f_2(x_1,x_2)\\\\quad\\vdots\\\\dot{x}i=x{i+1}+f_i(x_1,x_2,…,x_i)\\\\quad\\vdots\\\\dot{x}_n=u+f_n(x_1,x_2,…,x_n)\\end{cases}\\tag{1}$$ &emsp;&emsp;可以发现该形式的系统有如下特征：状态$x_1$只由$x_1,x_2$决定；状态$x_2$由$x_1,x_2,x_3$决定…以此类推，状态$x_i$由$x_1,x_2…x_{i+1}$决定；而状态$x_n$则由系统的所有状态来决定。&emsp;&emsp;这对我们就有了一定的启发。对于状态$x_1$来说，它只由本身与$x_2$决定，因此我们可以把它看作整个非线性系统的一个子系统。在这个系统中只需要控制$x_2$便可以使得$x_1$渐近稳定。再对于状态$x_2$来说，它由$x_1,x_2,x_3$决定，我们再将方程组的前两个方程看作一个子系统。此时$x_1$已经渐近稳定，因此我们只需要控制$x_3$便可以使得$x_2$渐近稳定。以此类推，最终我们就可以得到一个输入$u$，使得之前所有的状态$x_1,x_2…x_n$都渐近稳定，那么整个系统就稳定了。&emsp;&emsp;为了使各个子系统都稳定，我们需要为每个系统设置一个虚拟的控制量，并以此定义一个Lyapunov函数，使得所有子系统的Lyapunov函数都满足渐近稳定条件。&emsp;&emsp;概括来说，Backstepping的基本思想就是：将复杂的非线性系统分解成不超过系统阶数的子系统，然后为每个子系统设计部分Lyapunov函数和中间虚拟控制量，一直“后退”到整个系统，将它们集成起来完成整个控制律的设计。 基于Backstepping控制器设计&emsp;&emsp;在这里我们不直接给出控制器设计的公式，而是希望以探索的方式来得到并理解它。&emsp;&emsp;对于任意一个子系统$\\dot{x}i=x{i+1}+f_i(x_1,x_2,…,x_i)$来说，我们把$x_{i+1}$看作是这个系统的输入量，也就是模拟的控制量（实际上它只是整个系统的一个状态而已），再通过确定适当的虚拟反馈$\\alpha_i(i=1,2,…,n-1)$来控制$x_{i+1}$，使得这个子系统达到渐近稳定。而正由于系统是渐近稳定，因此状态与反馈之间会存在着误差。在这里我们引入误差变量$z$，期望通过控制的作用，使得$x_{i+1}$与虚拟反馈$a_i$之间具有某种渐进特性，从而实现整个系统的渐近稳定。&emsp;&emsp;误差变量$z$的设定如下：$$\\begin{cases}z_1=x_1\\z_2=x_2-\\alpha_1(x_1)\\\\quad\\vdots\\z_i=x_i-\\alpha_{i-1}(x_1,x_2,…,x_{i-1})\\\\quad\\vdots\\z_n=x_n-\\alpha_{n-1}(x_1,x_2,…,x_{n-1})\\end{cases}\\tag{2}$$ &emsp;&emsp;其中$\\alpha_i$为虚拟反馈量，也是我们要求解的待定量。对于每一个子系统，我们都会构造一个Lyapunov函数，使得每一个状态分量具有适当的渐近稳定特性。在此我们注意到：对于任意一个误差变量$z_i=x_i-\\alpha_{i-1}(x_1,x_2,…,x_{i-1})$，$x_i$与$z_i$的渐近稳定特性是相同的（因为我们在设定这个子系统前已经通过前边的子系统将$x_1$到$x_{i-1}$都控制为渐近稳定了）。因此为了渐近稳定原系统，我们只需要将误差变量$z$渐近稳定即可。而$x_1$由于是第一个子系统的状态量，并没有反馈，因此我们直接将其设为$z_1$。至此，我们设计目标就由$x$的渐近稳定转变为$z$的渐近稳定了。 分部推导&emsp;&emsp;首先看第一个子系统，将其以$z$为状态表示，得到：$$\\begin{split}\\dot{z}_1&amp;=\\dot{x}_1\\&amp;=x_2+f_1(x_1)\\&amp;=z_2+\\alpha_1(x_1)+f_1(x_1)\\end{split}\\tag{3}$$ &emsp;&emsp;定义该系统的Lyapunov方程：$$V_1=\\frac{1}{2}z_1^2\\tag{4}$$ &emsp;&emsp;$V_1$可以算是最简单直接，也是最常见的Lyapunov方程了，我们能够轻易看出它是正定的。但为了使该系统稳定，我们还需要使它的导数为一个负定的函数。将$V_1$求导：$$\\dot{V}_1=z_1\\dot{z}_1=z_1z_2+z_1(\\alpha_1(x_1)+f_1(x_1))\\tag{5}$$ &emsp;&emsp;为了使$\\dot{V}_1$负定，我们取：$$\\alpha_1(x_1)=-x_1-f_1(x_1)\\tag{6}$$ &emsp;&emsp;由于$z_1=x_1$，因此虚拟反馈$\\alpha_1(x_1)$可以使用$z_1$的函数$\\tilde{\\alpha}_1(z_1)$表示：$$\\alpha_1(x_1)=-z_1-f_1(z_1)\\tag{7}=\\tilde{\\alpha}_1(z_1)$$ &emsp;&emsp;此时：$$\\begin{split}\\dot{z}_1&amp;=z_2+\\tilde{\\alpha}_1(z_1)+f_1(x_1)\\&amp;=z_2-z_1\\end{split}\\tag{8}$$ &emsp;&emsp;将式(7)带入式(5)得到$\\dot{V}_1$：$$\\dot{V}_1=-z_1^2+z_1z_2\\tag{9}$$ &emsp;&emsp;可以发现，$\\dot{V}_1$在$z_2=0$，即$x_2=a_1(z_1)$的条件下为负定，此时该子系统渐近稳定。但一般情况下$z_2\\not=0$，因此我们需要对下一个子系统进行设计，使得$z_2$具有期望的渐近特性。&emsp;&emsp;因此将第二个子系统以$z$为状态表示，由式(2)与式(1)得到：$$\\begin{split}\\dot{z}_2&amp;=\\dot{x}_2-\\frac{\\partial\\tilde{\\alpha}_1}{\\partial z_1}\\dot{z}_1\\&amp;=x_3+f_2(x_1,x_2)-\\frac{\\partial\\tilde{\\alpha}_1}{\\partial z_1}\\dot{z}_1\\&amp;=z_3+\\alpha_2(x_1,x_2)+f_2(x_1,x_2)-\\frac{\\partial\\tilde{\\alpha}_1}{\\partial z_1}\\dot{z}_1\\end{split}\\tag{10}$$ &emsp;&emsp;由于：$$\\begin{eqnarray}x_1&amp;=&amp;z_1\\tag{11}\\x_2&amp;=&amp;\\tilde{\\alpha}_1(z_1)+z_2\\tag{12}\\\\end{eqnarray}$$ &emsp;&emsp;因此$\\alpha_2(x_1,x_2)$与$f_2(x_1,x_2)$可以完全用z1、z2表示。设：$$\\begin{eqnarray}\\tilde{\\alpha}_2(z_1,z_2)&amp;=&amp;\\alpha_2(x_1,x_2)\\tag{13}\\\\tilde{f}_2(z_1,z_2)&amp;=&amp;f_2(x_1,x_2)-\\frac{\\partial\\tilde{\\alpha}_1}{\\partial z_1}\\dot{z}_1\\tag{14}\\end{eqnarray}$$ &emsp;&emsp;带入式(10)得到：$$\\dot{z}_2=z_3+\\tilde{\\alpha}_2(z_1,z_2)+\\tilde{f}_2(z_1,z_2)\\tag{15}$$ &emsp;&emsp;和第一个子系统一样，我们需要设置它的Lyapunov方程。该是子系统基于第一个子系统的，它包含了$z_1,z_2$两个状态，因此我们将该系统Lyapunov方程设为：$$V_2=V_1+\\frac{1}{2}z_2^2=\\frac{1}{2}z_1^2+\\frac{1}{2}z_2^2\\tag{16}$$ &emsp;&emsp;同样它也是一个正定函数。对其求导得到：$$\\begin{split}\\dot{V}_2&amp;=\\dot{V}_1+z_2\\dot{z}_2\\&amp;=-z_1^2+z_1z_2+z_2z_3+z_2(\\tilde{\\alpha}_2(z_1,z_2)+\\tilde{f}_2(z_1,z_2))\\end{split}\\tag{17}$$ &emsp;&emsp;在这个子系统中我们使用$z_3$来控制$z_1,z_2$，因此需要将$z_1z_2$这项消去，并且增加一项$-z_2^2$。取：$$\\tilde{\\alpha}_2(z_1,z_2)=-z_1-z_2-\\tilde{f}_2(z_1,z_2)\\tag{18}$$ &emsp;&emsp;此时：$$\\begin{split}\\dot{z}_2&amp;=z_3+\\tilde{\\alpha}_2(z_1,z_2)+\\tilde{f}_2(z_1,z_2)\\&amp;=z_3-z_1-z_2\\end{split}\\tag{19}$$ &emsp;&emsp;带入式(17)得到$\\dot{V}_2$：$$\\dot{V}_2=-z_1^2-z_2^2+z_2z_3\\tag{20}$$ &emsp;&emsp;同样，它在$z_3=0$，即$x_3=\\tilde{\\alpha}_2(z_1,z_2)$的条件下为负定，此时该子系统渐近稳定。但一般情况下$z_3\\not=0$，因此我们需要对下一个子系统进行设计，使得$z_3$具有期望的渐近特性。如此下去我们便得可以得到Backstepping控制器设计的一般公式。 一般形式&emsp;&emsp;对于第$i$个子系统，我们可以将它表示为：$$\\begin{cases}\\tilde{\\alpha}i(z_1,z_2,…,z_i)=-z_{i-1}-z_i-\\tilde{f}_i(z_1,z_2,…z_i)\\\\\\dot{z}_i=-z{i-1}-z_i+z_{i+1}\\\\\\dot{V}i=-(z_1^2+z_2^2+…+z_i^2)+z_iz{i+1}\\end{cases}\\tag{21}$$ &emsp;&emsp;此时我们只需要控制$z_{i+1}$渐进稳定于0就可以了。&emsp;&emsp;一直重复这个过程，直到最终我们将系统“后退”到整个系统时：$$\\begin{split}\\dot{z}n&amp;=\\dot{x}n-\\sum{i=1}^{n-1}\\frac{\\partial\\tilde{\\alpha}{n-1}}{\\partial z_i}\\dot{z}i\\&amp;=u+f_n(x_1,x_2,…,x_n)-\\sum{i=1}^{n-1}\\frac{\\partial\\tilde{\\alpha}_{n-1}}{\\partial z_i}\\dot{z}_i\\&amp;=u+\\tilde{f}_n(z_1,z_2,…z_n)\\end{split}\\tag{22}$$ &emsp;&emsp;系统的Lyapunov方程为：$$V_n=V_{n-1}+\\frac{1}{2}z_n^2\\tag{23}$$ &emsp;&emsp;$V_{n-1}$为正定函数，因此$V_n$也为正定。将其求导得：$$\\begin{split}\\dot{V}n&amp;=\\dot{V}{n-1}+z_n\\dot{z}n\\&amp;=-(z_1^2+z_2^2+…+z{n-1}^2)+z_{n-1}z_n+z_n[u+\\tilde{f}_n(z_1,z_2,…z_n)]\\end{split}\\tag{24}$$ &emsp;&emsp;选取整个系统的输入为：$$u=\\tilde{\\alpha}_n(z_1,z_2,…,z_n)=-z_{n-1}-z_n-\\tilde{f}_n(z_1,z_2,…z_n)\\tag{25}$$ &emsp;&emsp;则由上述关系式(22)、(24)及(25)得：$$\\begin{cases}\\dot{z}n=-z{n-1}-z_n\\\\\\dot{V}_n=-(z_1^2+z_2^2+…+z_n^2)\\end{cases}\\tag{26}$$ &emsp;&emsp;可以得到$\\dot{V}n$在$u=-z{n-1}-z_n-\\tilde{f}_n(z_1,z_2,…z_n)$的条件下是负定的，因此误差变量$z$是渐进稳定的，从而得出整个系统是渐近稳定的。至此我们就成功地使用Backstepping设计了一个非线性系统的控制器！ 小结 Backstepping用来控制结构为(1)的非线性系统。 Backstepping是一种由前往后递推的设计方法，通过设置虚拟反馈控制子系统，并且前边的子系统必须通过后边子系统的虚拟控制才能达到稳定，以此层层递推。 Backstepping使得控制器的设计过程系统化、结构化。","categories":[{"name":"控制理论","slug":"控制理论","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/"},{"name":"非线性控制","slug":"控制理论/非线性控制","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%8E%A7%E5%88%B6/"}],"tags":[{"name":"Backstepping","slug":"Backstepping","permalink":"https://haosutopia.github.io/tags/Backstepping/"}]},{"title":"Tensorflow学习（二）","slug":"Tensorflow-02","date":"2018-01-12T01:46:15.000Z","updated":"2018-01-23T23:10:14.000Z","comments":true,"path":"2018/01/Tensorflow-02/","link":"","permalink":"https://haosutopia.github.io/2018/01/Tensorflow-02/","excerpt":"","text":"&emsp;&emsp;当我们学习一个新的编程语言时，输出的第一句话一定是“Hallo World”；当我们学习一个新的深度学习框架时，跑的第一个数据集一定是MNIST。这是Tensorflow官方教程的原话。MNIST是一个简单的计算机图像数据集，它由一系列大小为28×28的手写数字图像以及它们的标记组成（如下图所示），可以算是最简单的数据集之一啦。&emsp;&emsp;本次学习会使用两种不同的方法对MNIST进行训练：Softmax回归和卷积神经网络。它们分别会得到不同精确度的手写数字预测模型。 导入MNIST数据&emsp;&emsp;MNIST数据集可以直接在它的官网上下载，也可以通过Tensorflow的教程文件直接导入。教程文件是我们在安装Tensorflow时自动添加的。这里我们直接将数据从中导入。python中的实现如下： 12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot = True) &emsp;&emsp;这个代码在运行时会在当前目录下生成一个名为“MNIST_data”的文件夹，并在此文件夹中导入MNIST数据。一共得到四个文件，分别是训练数据、训练标签、测试数据以及测试标签。在接下来的代码中它们可以分别通过mnist.train.images、minst.train.labels、mnist.test.images、mnist.test.labels来调用，非常方便。&emsp;&emsp;函数中的one-hot是针对数据标签的操作。它可以将表示分类的标签y转换为一个只有第y项为1，其余项都为0，且项数为总分类数的向量。这样操作之后，向量的每一项代表了一个分类，当数据属于一个分类时，该分类所属项为1，其余项为0。以此可以更加方便地计算代价函数。举个例子：MNIST数据集一共有10个分类（数字0~9），那么数字3的标签就会转换为[0,0,0,1,0,0,0,0,0,0]。 softmax回归&emsp;&emsp;softmax回归比较简单。它相当于在线性模型后增加了一个softmax函数，将线性模型的输出转换到[0,1]区间中，并且使各个类别的输出值相加为1，因此它可以被当成是某个分类的概率。softmax回归模型最终输出为一个向量，其项数为总分类数，并且每项代表一个分类的概率。这与one-hot输出的形式是一样的。 定义张量&emsp;&emsp;MNIST的数据x与标签y_为模型的输入，因此使用placeholder类别；而模型参数W与b需要通过数据集训练得到，因此使用Variable类别。定义它们并将其初始化为0。实现代码如下： 12345#定义张量x = tf.placeholder(tf.float32, [None, 784])y_ = tf.placeholder(tf.float32, [None, 10])W = tf.Variable(tf.zeros([784, 10]))b = tf.Variable(tf.zeros([10])) 构建computational graph&emsp;&emsp;softmax回归模型由一个线性模型及softmax函数组成，其输出表达式为：$$y=softmax(W*x+b)$$&emsp;&emsp;其中W与b相乘为矩阵相乘。在Tensorflow中矩阵相乘不能使用普通的乘号，也不能用numpy的函数，必须使用Tensorflow内置的矩阵乘法函数：tf.matmul(mat1, mat2)。softmax函数可以直接使用Tensorflow函数tf.nn.softmax()实现。tf.nn模块包含了几乎所有神经网络要用到的函数，因此之后会经常用到它（官方文档）。输出y的实现代码如下： 12#构建computational graphy = tf.nn.softmax(tf.matmul(x, W) + b) &emsp;&emsp;我们选择交叉熵函数（cross-entropy）作为代价函数，它是在信息学中最先被提出的。其定义为：$$H_{y’}(y)=-\\sum_{i} {y’_ilog(y_i)}$$&emsp;&emsp;求出每个样本的交叉熵后，再对它们求平均值，便得到了损失值。将其表示为代码： 12#代价函数cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices = [1])) &emsp;&emsp;其中tf.reduce_sum(input_tensor, axis, reduction_indices)为Tensorflow的求和函数（官方文档），reduction_indices = [1]代表将输入的第二个维度相加，与axis作用一致。tf.reduce_mean(input_tensor, axis)为Tensorflow的均值函数（官方文档）。&emsp;&emsp;而实际上Tensorflow在tf.nn中已经提供了直接求交叉熵代价函数的方程，由于该函数包含了softmax步骤，因此可以将之前的代码改为： 1234#构建computational graphy = tf.matmul(x, W) + b#代价函数cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = y, labels = y_)) &emsp;&emsp;其中logits为模型输出值，labels为实际标签值。 训练参数&emsp;&emsp;设置训练目标，这里选用梯度下降法，学习率为0.5，训练目标为降低损失值cross_entropy： 12#设置训练目标train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) &emsp;&emsp;设置Session，初始化Variable，之后使用分批训练的方式，每100个数据为一个batch，通过for循环运行1000次，得到预测模型： 123456789#设置Sessionsess = tf.Session()#Variable初始化init = tf.global_variables_initializer()sess.run(init)#分批训练for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) #mnist.train.next_batch()函数得到下一batch的数据 sess.run(train_step, feed_dict = &#123;x: batch_xs, y_: batch_ys&#125;) #使用该批次训练 评估模型&emsp;&emsp;训练完模型后就可以使用测试数据集来测试模型的精确度了。首先定义精确度，它等于识别正确的数量除以测试总数。精确度可以采用以下代码实现： 123#评估模型correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) #判断是否识别正确accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) #计算精确度 &emsp;&emsp;其中tf.argmax(input, axis)函数（官方文档）为返回一个向量最大值的坐标，第二个参数1表示在向量第二维寻找最大值并返回其坐标（correct_prediction[n][10]第一维n为样本数量，第二维为每个样本的输出）。使用tf.argmax()分别得到模型测试的类别以及真实标签的类别。再使用tf.equal(x, y)函数（官方文档）比较它们，得到一系列布尔值的矩阵。它代表预测的分类与实际的分类相同与否，并以此来判断识别是否正确。&emsp;&emsp;tf.cast(x, dtype)函数（官方文档）在这里的作用是将布尔值转换为浮点数。最后用均值函数求取均值就得到了模型的测试精确度。&emsp;&emsp;通过Session运行并输出： 12#输出print(&quot;accuracy:&quot;, sess.run(accuracy, feed_dict = &#123;x: mnist.test.images, y_:mnist.test.labels&#125;)) &emsp;&emsp;最终得到模型的精确度为：0.9212 1accuracy: 0.9212 &emsp;&emsp;0.92的精确度显然不能令我们满意，毕竟这只是一个十个数字符号的识别任务。softmax回归模型过于简单，没有考虑图像像素的位置关系。接下来我们将使用一个简单的卷积神经网络来大幅提升精确度。 卷积神经网络&emsp;&emsp;卷积神经网络（Convolutional Neural Network: CNN）常被用于图像的识别，它考虑了输入数据的二维或三维（可能更高）结构，因此对于数字字符图像这样的识别对象非常有效。 网络结构&emsp;&emsp;在这里我们要建立一个包含两个卷积层的卷积神经网络，并在训练时使用dropout。网络的前向传播过程为：输入数据经过reshape变形为二维图形数据，输入一个filter为5×5，步长为1，padding为same的卷积层，经过relu激活函数后输入一个步长为2，padding为same的max池化层。再经过一个相同的卷积层+relu+池化层组合后，将输出变形为一个列向量，输入一个全连接层。该全连接层输出为一个项数为1024的向量。之后经过dropout后再将其输入至另一个1024×10的全连接层，最终得到一个1×10的向量，即输出向量。整个卷积神经网络结构如下： 定义张量&emsp;&emsp;卷积神经网络由于有多个层，每个层都会有权重W和偏差b。在这里我们不像之前那样将它们全部初始化为0，而是初始化为一些很小的随机数,因为在深度网络中全部初始化为0会造成梯度上的一些问题。为了方便起见，我们将W及b的定义与初始化放入函数中，这样可以使代码更加美观直接： 123456789#定义W初始化函数def weight_variable(shape): initial = tf.truncated_normal(shape, stddev = 0.1) return tf.Variable(initial)#定义b初始化函数def bias_variable(shape): initial = tf.constant(0.1, shape = shape) return tf.Variable(initial) &emsp;&emsp;初始化权重时，我们使用了tf.truncated_normal(shape, mean, stddev)函数（官方文档）,其中shape代表生成张量的维度，mean代表均值，stddev代表标准差。与tf.random_normal()不同，它是从截断的正太分布中输出维度为shape的随机数，且该正太分布服从均值为mean，标准差为stddev。截断的意思指生成的随机数不能离均值太远，只能在[μ-2σ，μ+2σ]之间取值。若生成的随机数超过了这个区间，则函数会自动舍弃并重新生成，这保证了参数的初始值不会太大。因此它经常被用来初始化训练参数。参数初始化后使用tf.Variable()定义张量。&emsp;&emsp;初始化偏差时，我们将它们统一初始化为0.1，再定义Variable张量即可。&emsp;&emsp;定义完函数后我们将使用它们对各个模型参数进行定义和初始化，同时定义x与y_。每个卷积层和全连接层都有一个W和一个b，而池化层没有参数，定义时要注意它们的维度计算。除此之外我们还需要定义dropout的概率参数，因为在训练模型和评估模型时它们的值是不同的。代码实现如下： 12345678910111213141516#定义张量x = tf.placeholder(tf.float32, [None, 784])y_ = tf.placeholder(tf.float32, [None, 10])#初始化W,bW_conv1 = weight_variable([5, 5, 1, 32]) #第一个卷积层权重b_conv1 = bias_variable([32]) #第一个卷积层偏差W_conv2 = weight_variable([5, 5, 32, 64]) #第二个卷积层权重b_conv2 = bias_variable([64]) #第二个卷积层偏差W_fc1 = weight_variable([7 * 7 * 64, 1024]) #第一个全连接层权重b_fc1 = bias_variable([1024]) #第一个全连接层偏差W_fc2 = weight_variable([1024, 10]) #第二个全连接层权重b_fc2 = bias_variable([10]) #第二个全连接层偏差#定义dropout参数keep_prob = tf.placeholder(tf.float32) 构建computational graph&emsp;&emsp;根据之前规划的网络结构来构建computational graph。卷积神经网络的各个层都可以使用tf.nn模块中定义的函数实现。这里我们使用的函数如下：&emsp;&emsp;**tf.nn.conv2d(input, filter, strides, padding)**（官方文档）：使用filter对输入input进行卷积，其卷积的步长为strides，padding为padding。&emsp;&emsp;**tf.nn.relu(feature)**（官方文档）：对输入feature进行relu运算。&emsp;&emsp;**tf.nn.max_pool(value, ksize, strides, padding)**（官方文档）：使用ksize设定的窗对输入value进行池化运算，其步长为strides，padding为padding。&emsp;&emsp;全连接层没有专门的函数，直接使用Tensorflow的矩阵运算就可以了。构建网络的代码实现如下： 12345678910111213#构建computational graphx_image = tf.reshape(x, [-1, 28, 28, 1])Z1 = tf.nn.conv2d(x_image, W_conv1, strides = [1, 1, 1, 1], padding = &#x27;SAME&#x27;) + b_conv1A1 = tf.nn.relu(Z1)P1 = tf.nn.max_pool(A1, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = &#x27;SAME&#x27;)Z2 = tf.nn.conv2d(P1, W_conv2, strides = [1, 1, 1, 1], padding = &#x27;SAME&#x27;) + b_conv2A2 = tf.nn.relu(Z2)P2 = tf.nn.max_pool(A2, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = &#x27;SAME&#x27;)P2_flat = tf.reshape(P2, [-1, 7 * 7 * 64])Z3 = tf.matmul(P2_flat, W_fc1) + b_fc1A3 = tf.nn.relu(Z3)D1 = tf.nn.dropout(A3, keep_prob)y = tf.matmul(D1, W_fc2) + b_fc2 &emsp;&emsp;最后定义代价函数，其方法与softmax回归模型一致： 12#定义代价函数cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y_, logits = y)) 设置训练目标与评价参数&emsp;&emsp;在这里我们使用Adam的优化方法来降低代价函数的损失值。在这里要注意的是，程序中对Variable的初始化要在Adam定义之后，不然会报错，这可能是因为Adam的函数中本身也定义了一些Variable需要初始化。因此在写Tensorflow时我都习惯将初始化的代码放在训练之前。代码如下： 12#设置训练目标train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) &emsp;&emsp;若我们希望训练过程中输出模型的精确度，则需要在训练之前设置评估参数。其设置方法与softmax回归模型一致： 123#设置评估参数correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) 训练参数&emsp;&emsp;在初始化Variable之后使用for循环对参数进行训练，其实现过程与softmax类似。由于网络参数较多，我们每次使用数据量为50的batch迭代20000次。并且每迭代1000次输出模型对训练集的精确度。其实现代码如下： 1234567891011#Variable初始化init = tf.global_variables_initializer()sess.run(init)#循环训练for i in range(20000): batch_xs, batch_ys = mnist.train.next_batch(50) sess.run(train_step, feed_dict = &#123;x: batch_xs, y_: batch_ys, keep_prob: 0.5&#125;) if i % 1000 == 0: train_accuracy = sess.run(accuracy, feed_dict = &#123;x: batch_xs, y_: batch_ys, keep_prob: 1&#125;) print(&quot;step&quot;, i, &quot;training accuracy:&quot;,train_accuracy) 评估模型&emsp;&emsp;最后使用测试集对模型进行评估并输出最终精确度： 1234#输出test_accuracy = sess.run(accuracy, feed_dict = &#123;x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1&#125;)print(&quot;test accuracy:&quot;, test_accuracy)sess.close() &emsp;&emsp;这样就已经大功告成啦！打开终端运行代码，整个训练过程可能要持续半小时或者更久（由你CPU或GPU水平决定）。休息下喝杯茶吧！&emsp;&emsp;最终的输出为： 123456789101112131415161718192021step 0 training accuracy: 0.04step 1000 training accuracy: 0.96step 2000 training accuracy: 1.0step 3000 training accuracy: 1.0step 4000 training accuracy: 0.98step 5000 training accuracy: 1.0step 6000 training accuracy: 1.0step 7000 training accuracy: 1.0step 8000 training accuracy: 1.0step 9000 training accuracy: 1.0step 10000 training accuracy: 1.0step 11000 training accuracy: 1.0step 12000 training accuracy: 1.0step 13000 training accuracy: 1.0step 14000 training accuracy: 1.0step 15000 training accuracy: 1.0step 16000 training accuracy: 1.0step 17000 training accuracy: 1.0step 18000 training accuracy: 1.0step 19000 training accuracy: 1.0test accuracy: 0.9918 &emsp;&emsp;每个人得到的结果会略有不同，因为参数初始化时是随机的，但大致都在99.2%左右，比softmax回归模型提高了很多！ 小结 认识MNIST数据集 定义一个softmax回归模型，训练后得到最终精确度为92% 使用tf.nn中的函数能方便地定义一个卷积神经网络，训练后得到最终精确度为99.2%，相比softmax回归模型有了大幅的提升。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://haosutopia.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"深度学习","slug":"机器学习/深度学习","permalink":"https://haosutopia.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://haosutopia.github.io/tags/Tensorflow/"}]},{"title":"Tensorflow学习（一）","slug":"Tensorflow-01","date":"2018-01-05T21:23:35.000Z","updated":"2018-01-23T23:09:44.000Z","comments":true,"path":"2018/01/Tensorflow-01/","link":"","permalink":"https://haosutopia.github.io/2018/01/Tensorflow-01/","excerpt":"","text":"&emsp;&emsp;Tensorflow是一个强大的深度学习框架，通过它可以方便地配置、训练以及评估模型。本次学习材料主要是Tensorflow官方教程、Deeplearning.ai课程以及一些网上的资料，并会加上自己的一些理解及想法。此外，本次学习针对的是机器学习在Tensorflow中的应用，不会对其理论做过多的阐述。 理解Tensorflow&emsp;&emsp;Tensorflow这个词由Tensor与Flow组成。Tensor为张量，不用管维基百科上多么复杂的解释，在这里张量就相当于一个容器，数据的容器。它可以是一维、二维、三维乃至n维的向量，也可以是文字、图像或者视频。任何想要在框架中使用的数据都需要定义成Tensorflow的张量，之后框架才能对它们进行操作。&emsp;&emsp;而flow为流动。以CNN模型为例，我们将图像看作一个n×n×3的矩阵，输入模型，通过前向传播，最终得出它的损失值。每一次运算就相当于一个节点（node），前向传播就好像数据流过各个节点的过程，而数据最终以损失值的形式输出。&emsp;&emsp;Tensorflow的框架思想就是如此，在使用时有下列几个步骤： &emsp;&emsp;1. 创建张量（Tensor）&emsp;&emsp;2. 写出张量的运算过程，构成一个计算网络（computational graph）&emsp;&emsp;3. 初始化张量&emsp;&emsp;4. 创建Session&emsp;&emsp;5. 运行Session &emsp;&emsp;Tensorflow的计算依赖于一个高效的C++后台，而Session的作用就是Tensorflow与后台的连接，可以把目标事物通过Session传递到后台，再运行就可以进行计算了，这个之后会通过例子说明。从这些步骤可以看出，我们只需将前向传播的张量和网络设置好，其他事情，像什么反向传播，参数更新啥的就全都可以交给Tensorflow做了。 Tensorflow中的数据&emsp;&emsp;Tensorflow的数据储存在张量之中，并且它们有三种形式：constant、placeholder和variable constant&emsp;&emsp;constant为常量，它的值在整个运算过程中不会变化，若我们不想在训练中改变某些参数的值，就可将它们设置为constant。比如在Neural Style Transfer中，我们希望训练时保持模型不变而只改变输入图像，就需要将模型设置为常量。&emsp;&emsp;在Tensorflow中设置常量要使用constant函数（官方文档），我们会发现官方文档中constant函数的参数有很多，但一般情况下我们只需要使用它最基本的几个参数就够了： 12import tensorflow as tf #导入类const = tf.constant(3.0, dtype = tf.float32) #创建constant &emsp;&emsp;程序第一行用import将tensorflow类导入程序中，以便调用。这是所有Tensorflow程序都拥有的语句。第二行使用constant函数将3.0赋值给const，并设置数据类型为float32。&emsp;&emsp;若用print函数直接将const输出，得到的结果为： 1Tensor(&quot;Const:0&quot;, shape=(), dtype=float32) &emsp;&emsp;它并非我们想的那样输出const的值，因为const此时是框架中的一个张量，无法用print直接输出。若要输出const的实际值，则需要创建一个Session，将const带入并运行，最后关闭Session： 123sess = tf.Session() #创建一个Sessionprint(sess.run(const)) #sess.run(const)得到const运行后的值，再通过print输出sess.close() &emsp;&emsp;run是Session的一个method，通过它可以运行computational graph并输出目标事物的值。运行后得到输出为： 13.0 placeholder&emsp;&emsp;placeholder有点像函数中的形参，它的作用是在computational graph中为数据预留一个位置，在每次运行Session时再给它赋值。因此它常常用于分批训练，每次运行Session时带入一个batch的训练集。由于不用赋值，因此定义placeholder只需要指定数据类型（官方文档）： 12import tensorflow as tfa = tf.placeholder(tf.int32) #创建placeholder &emsp;&emsp;当运行Session时，需要在run函数中对placeholder进行赋值，赋值方法为增加参数feed_dict = {placeholder1: value1, placeholder2: value2 …}： 123sess = tf.Session()print(sess.run(a, feed_dict = &#123;a: [3, 4]&#125;)) #用feed_dict带入数值sess.close() &emsp;&emsp;得到输出为： 1[3 4] variable&emsp;&emsp;variable是Tensorflow中的变量，可以随Session运行而改变，因此在训练模型中常用于模型训练参数（W、b等）的定义。Variable在定义时需要指定初始值，并且在运行Session前需要使用initializer进行初始化（官方文档）： 1234567import tensorflow as tfW = tf.Variable(3.0, dtype = tf.float32) #创建variableinit = tf.global_variables_initializer() #创建一个全局变量的initializersess = tf.Session()sess.run(init) #运行initializer，使得变量初始化print(sess.run(W))sess.close() &emsp;&emsp;这里需要注意的是创建variable时的函数首字母是大写的，而constant和placeholder的函数都是小写的，很容易就搞错了（吐糟一句：统一一下函数命名这么难吗-_-||）。这里的initializer用的是global_variables_initializer()，用Session运行它可以初始化所有全局变量。程序运行结果为： 13.0 &emsp;&emsp;我们还可以使用assign给变量重新分配一个值，当然也需要通过Session来实现： 1234fixW = tf.assign(W, 2.0) #将2.0分配给Wsess.run(fixW)print(sess.run(W))sess.close() &emsp;&emsp;得到输出为： 12.0 构建computational graph&emsp;&emsp;创建张量后的第二步就是构建一个computational graph，我们可以通过Tensorflow中的各种运算建立节点以形成网络。常用的运算符有：tf.add、tf.subtract、tf.multiply、tf.divide、tf.square等。&emsp;&emsp;以线性模型为例。设：$$y=W*x+b$$&emsp;&emsp;则它的损失值可通过如下函数求出：$$loss=\\sum(Wx_i+b-y_i)^2$$&emsp;&emsp;由于W、b是我们要训练的参数，而x、y为模型的输入，因此设W、b为variable，x、y为placeholder。通过张量间的各种运算求出loss并输出： 1234567891011121314151617181920212223import tensorflow as tf#创建TensorW = tf.Variable(0.3, dtype = tf.float32)b = tf.Variable(-0.3, dtype = tf.float32)x = tf.placeholder(tf.float32)y = tf.placeholder(tf.float32)#构建computational graphlinear_model = tf.add(tf.multiply(W, x), b) #linear_model = Wx + bloss = tf.square(tf.subtract(y, linear_model)) #loss = (y - linear_model) ^ 2loss = tf.reduce_sum(loss) #求和#初始化sess = tf.Session()init = tf.global_variables_initializer()sess.run(init)#输出print(sess.run(loss, feed_dict = &#123;x: [1, 2, 3, 4], y: [0, -1, -2, -3]&#125;))#关闭Sessionsess.close() &emsp;&emsp;输出结果为： 123.66 &emsp;&emsp;而实际上Tensorflow已经将最常用的算数运算符重载了，因此在运算时也可以直接使用，其结果是一样的： 12345#构建computational graphlinear_model = W * x + b #linear_model = Wx + bloss = (y - linear_model)**2 #loss = (y - linear_model) ^ 2loss = tf.reduce_sum(loss) #求和sess.close() &emsp;&emsp;其构建的computational graph为：&emsp;&emsp;可以看到张量通过各个运算节点流过整张computational graph，并最终形成了输出。&emsp;&emsp;这张图是通过Tensorflow内置的Tensorboard自动生成的。图中各张量的名称是在定义它们时加上参数name得到的。若想使用Tensorboard可视化整个训练过程，只需将输出改为： 1234#输出writer = tf.summary.FileWriter(&quot;output&quot;, sess.graph)print(sess.run(loss, feed_dict = &#123;x: [1, 2, 3, 4], y: [0, -1, -2, -3]&#125;))writer.close() &emsp;&emsp;运行后便会在代码所在目录新建一个名为output的文件夹。再打开一个终端中输入如下代码： 1tensorboard --logdir&#x3D;path&#x2F;to&#x2F;output #output文件夹所在位置 &emsp;&emsp;回车后便会自动生成一个Tensorboard界面，还是很方便的。 使用optimizer训练参数&emsp;&emsp;Tensorflow最核心的功能便是训练参数了，在构建好computational graph之后，我们可以使用它内置的各种优化方式（optimizer）来方便快捷地实现参数训练。&emsp;&emsp;接之前线性模型的例子，我们希望通过训练，改变W和b的值，使得loss减小，即模型输入x所得到的linear_model值越来越接近y。Tensorflow提供了很多优化方法，这里选择最简单的梯度下降法（gradient descent）。我们只需设置优化方法（optimizer）及优化目标，Tensorflow便会自动计算梯度并且更新参数。其实现代码如下： 123456789#接之前代码#设置优化方式optimizer = tf.train.GradientDescentOptimizer(0.01) #设置优化方式为梯度下降，学习率为0.01train = optimizer.minimize(loss) #设置优化目标为降低lossfor i in range(1000): #迭代1000次 sess.run(train, feed_dict = &#123;x: [1, 2, 3, 4], y: [0, -1, -2, -3]&#125;)print(sess.run([W, b, loss], feed_dict = &#123;x: [1, 2, 3, 4], y: [0, -1, -2, -3]&#125;))sess.close() &emsp;&emsp;我们对模型迭代了1000次，每次迭代时，W与b的值都会更新，使loss减小。最终得到结果为： 1[-0.9999969, 0.99999082, 5.6999738e-11] &emsp;&emsp;可以看到，经过1000次迭代后，损失值已经非常小了。至此我们已经成功的使用Tensorflow训练了一个线性模型！ 小结 Tensorflow有constant、placeholder、variable三种数据形式。其中需要训练的参数设为variable，训练集设为placeholder，常量设为constant。 通过运算建立张量流动的节点，构成computational graph，形成前向传播通道 设置optimizer与优化目标来训练模型。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://haosutopia.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"深度学习","slug":"机器学习/深度学习","permalink":"https://haosutopia.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://haosutopia.github.io/tags/Tensorflow/"}]}],"categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://haosutopia.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"},{"name":"机器学习","slug":"机器学习","permalink":"https://haosutopia.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"控制理论","slug":"控制理论","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/"},{"name":"分布参数系统控制","slug":"控制理论/分布参数系统控制","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F%E6%8E%A7%E5%88%B6/"},{"name":"深度学习","slug":"机器学习/深度学习","permalink":"https://haosutopia.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"非线性控制","slug":"控制理论/非线性控制","permalink":"https://haosutopia.github.io/categories/%E6%8E%A7%E5%88%B6%E7%90%86%E8%AE%BA/%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%8E%A7%E5%88%B6/"}],"tags":[{"name":"无监督学习","slug":"无监督学习","permalink":"https://haosutopia.github.io/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"},{"name":"Backstepping","slug":"Backstepping","permalink":"https://haosutopia.github.io/tags/Backstepping/"},{"name":"分布参数系统","slug":"分布参数系统","permalink":"https://haosutopia.github.io/tags/%E5%88%86%E5%B8%83%E5%8F%82%E6%95%B0%E7%B3%BB%E7%BB%9F/"},{"name":"SVM","slug":"SVM","permalink":"https://haosutopia.github.io/tags/SVM/"},{"name":"Python","slug":"Python","permalink":"https://haosutopia.github.io/tags/Python/"},{"name":"迁移学习","slug":"迁移学习","permalink":"https://haosutopia.github.io/tags/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"},{"name":"CNN","slug":"CNN","permalink":"https://haosutopia.github.io/tags/CNN/"},{"name":"Caffe","slug":"Caffe","permalink":"https://haosutopia.github.io/tags/Caffe/"},{"name":"数学建模","slug":"数学建模","permalink":"https://haosutopia.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"},{"name":"算子理论","slug":"算子理论","permalink":"https://haosutopia.github.io/tags/%E7%AE%97%E5%AD%90%E7%90%86%E8%AE%BA/"},{"name":"Tensorflow","slug":"Tensorflow","permalink":"https://haosutopia.github.io/tags/Tensorflow/"}]}